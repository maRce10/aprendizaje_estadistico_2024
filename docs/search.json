[
  {
    "objectID": "fundamentos_de_r.html",
    "href": "fundamentos_de_r.html",
    "title": "Elementos básicos del lenguaje R",
    "section": "",
    "text": "Familiarizarse con los bloques básicos usados en la programación en R\nConocer las fuentes principales de documentación estandarizada en R"
  },
  {
    "objectID": "fundamentos_de_r.html#section",
    "href": "fundamentos_de_r.html#section",
    "title": "Elementos básicos del lenguaje R",
    "section": "2.1  ",
    "text": "2.1"
  },
  {
    "objectID": "fundamentos_de_r.html#objetos-que-contienen-datos",
    "href": "fundamentos_de_r.html#objetos-que-contienen-datos",
    "title": "Elementos básicos del lenguaje R",
    "section": "3.1 Objetos que contienen datos",
    "text": "3.1 Objetos que contienen datos\nLa estructura de datos básica en R es el vector. Con este se contruyen todas las otras clases de objetos. Para entender las clases es útil pensar en el número de dimensiones (1, 2 o mas) y tipos de datos que pueden contener: homogéneos (un único tipo de elemento) o homogéneos (o múltiples tipos de elementos).\n\n\n\n\n\nflowchart LR\n    classDef largeText font-size:18px, padding:15px;\n\n    D(Objetos de Datos) --&gt; D1(1 dimension)\n    D --&gt; D2(2 dimensiones)\n    D1 --&gt; V(Vector)\n    D1 --&gt; L(Lista)\n    D2 --&gt; M(Matriz)\n    D2 --&gt;  DF(\"Cuadro de Datos &lt;br&gt;(Data Frame)\")\n\n    style D fill:#40498E66, stroke:#000, stroke-width:2px, color:#FFF, width:180px\n    style D1 fill:#348AA666, stroke:#000, stroke-width:2px, color:#FFF, width:140px\n    style D2 fill:#348AA666, stroke:#000, stroke-width:2px, color:#FFF, width:140px\n    style DF fill:#49C1AD66, stroke:#000, stroke-width:2px, color:#000\n    style V fill:#49C1AD66, stroke:#000, stroke-width:2px, color:#000\n    style M fill:#49C1AD66, stroke:#000, stroke-width:2px, color:#000\n    style L fill:#49C1AD66, stroke:#000, stroke-width:2px, color:#000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomogéneo\nHeterogéneo\n\n\n\n\n1d\nVector atómico\nLista\n\n\n2d\nMatriz\nCuadro de datos (data frame)\n\n\nnd\narreglo (Array)\n\n\n\n\n\n\n\n\n3.1.1 Objetos de 1 dimensión\nHay dos tipos básicos de vectores: vectores atómicos y listas.\nTienen tres propiedades comunes:\n\nTipo, typeof() (clase/modo ~)\nLongitud, length() (número de elementos)\nAttributes, attributes() (metadatos)\n\nSe diferencian en los tipos de sus elementos: todos los elementos de un vector atómico deben ser del mismo tipo, mientras que los elementos de una lista pueden tener diferentes tipos.\n\n3.1.1.1 Vectores atomicos\nTipos de vectores atómicos:\n\nLógico (booleano)\nEntero\nNumérico (doble)\nCaracteres\nFactores\n\nLos vectores se construyen con la función c(), Pueden ser numérico:\n\n\nCódigo\nx &lt;- 1\nx1 &lt;- c(1)\n\nall.equal(x, x1)\n\n\n[1] TRUE\n\n\nCódigo\nclass(x)\n\n\n[1] \"numeric\"\n\n\nDe caracteres:\n\n\nCódigo\ny &lt;- \"algo\"\n\nclass(y)\n\n\n[1] \"character\"\n\n\nLógico:\n\n\nCódigo\nz &lt;- TRUE\n\nclass(z)\n\n\n[1] \"logical\"\n\n\nO factor:\n\n\nCódigo\nq &lt;- factor(1)\n\nclass(q)\n\n\n[1] \"factor\"\n\n\n \nPor tanto, los números o cadenas individuales son en realidad vectores de longitud uno:\n\n\nCódigo\nclass(1)\n\n\n[1] \"numeric\"\n\n\nCódigo\nclass(\"a\")\n\n\n[1] \"character\"\n\n\nLos vectores sólo pueden contener elementos del mismo tipo. Los tipos diferentes de elementos serán forzados al tipo más flexible:\n\n\nCódigo\nx &lt;- c(1, 2, \"a\")\n\nx\n\n\n[1] \"1\" \"2\" \"a\"\n\n\nCódigo\nclass(x)\n\n\n[1] \"character\"\n\n\n \nLos valores que faltan se especifican con NA, que es un vector lógico de longitud 1. NA siempre será interpretado al tipo correcto si se utiliza dentro de c():\n\n\nCódigo\nv &lt;- c(10, 11, NA)\n\nclass(v)\n\n\n[1] \"numeric\"\n\n\nCódigo\nv &lt;- c(\"a\", \"b\", NA)\n\nclass(v)\n\n\n[1] \"character\"\n\n\n\n\n3.1.1.2 Factores\nLos vectores con factores son muy simulares a los de caracteres. Sin embargo, un factor sólo puede contener valores predefinidos, conocidos como niveles. Los atributos se utilizan para definir los niveles del factor.\nLos factores se construyen sobre vectores enteros utilizando dos atributos:\n\nclase “factor”: hace que se comporten de forma diferente a los vectores de caracteres normales\nniveles: define el conjunto de valores permitidos\n\n\n\nCódigo\nx &lt;- factor(c(\"a\", \"b\", \"b\", \"a\"))\nx\n\n\n[1] a b b a\nLevels: a b\n\n\nCódigo\nlevels(x)\n\n\n[1] \"a\" \"b\"\n\n\nCódigo\nstr(x)\n\n\n Factor w/ 2 levels \"a\",\"b\": 1 2 2 1\n\n\n \nLos factores parecen vectores de caracteres, pero en realidad son números enteros:\n\n\nCódigo\nx &lt;- factor(c(\"a\", \"b\", \"b\", \"a\"))\n\nc(x)\n\n\n[1] a b b a\nLevels: a b\n\n\n\n\n3.1.1.3 Listas\nPuede contener objetos de diferentes clases y tamaños. Las listas se construyen con list():\n\n\nCódigo\nl &lt;- list(\"a\", 1, FALSE)\n\nl\n\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] 1\n\n[[3]]\n[1] FALSE\n\n\nCódigo\nclass(l)\n\n\n[1] \"list\"\n\n\nCódigo\nstr(l)\n\n\nList of 3\n $ : chr \"a\"\n $ : num 1\n $ : logi FALSE\n\n\nEn realidad pueden ser vistas como cajones donde se pueden poner cualquier otro tipo de objeto:\n\n\nCódigo\nl &lt;- list(c(\"a\", \"b\"), c(1, 2, 3, 4), c(FALSE, TRUE, FALSE))\n\nl\n\n\n[[1]]\n[1] \"a\" \"b\"\n\n[[2]]\n[1] 1 2 3 4\n\n[[3]]\n[1] FALSE  TRUE FALSE\n\n\nCódigo\nstr(l)\n\n\nList of 3\n $ : chr [1:2] \"a\" \"b\"\n $ : num [1:4] 1 2 3 4\n $ : logi [1:3] FALSE TRUE FALSE\n\n\n\n\n\n3.1.2 Objetos de 2 dimensiones\n\n\n3.1.3 Matrices\nTodas los elementos son del mismo tipo:\n\n\nCódigo\nm &lt;- matrix(c(1, 2, 3, 11, 12, 13), nrow = 2)\n\ndim(m)\n\n\n[1] 2 3\n\n\nCódigo\nm\n\n\n     [,1] [,2] [,3]\n[1,]    1    3   12\n[2,]    2   11   13\n\n\nCódigo\nclass(m)\n\n\n[1] \"matrix\" \"array\" \n\n\nCódigo\nm &lt;- matrix(c(1, 2, 3, 11, 12, \"13\"), nrow = 2)\nm\n\n\n     [,1] [,2] [,3]\n[1,] \"1\"  \"3\"  \"12\"\n[2,] \"2\"  \"11\" \"13\"\n\n\n \n\n\n3.1.4 Cuadros de datos (data frames)\nCaso especial de las listas. Puede contener elementos de diferentes tipos:\n\n\nCódigo\nm &lt;-\n  data.frame(\n    ID = c(\"a\", \"b\", \"c\", \"d\", \"e\"),\n    size = c(1, 2, 3, 4, 5),\n    observed = c(FALSE, TRUE, FALSE, FALSE, FALSE)\n  )\n\ndim(m)\n\n\n[1] 5 3\n\n\nCódigo\nm\n\n\n\n\n\n\nID\nsize\nobserved\n\n\n\n\na\n1\nFALSE\n\n\nb\n2\nTRUE\n\n\nc\n3\nFALSE\n\n\nd\n4\nFALSE\n\n\ne\n5\nFALSE\n\n\n\n\n\n\nCódigo\nclass(m)\n\n\n[1] \"data.frame\"\n\n\nCódigo\nis.data.frame(m)\n\n\n[1] TRUE\n\n\nCódigo\nis.list(m)\n\n\n[1] TRUE\n\n\nCódigo\nstr(m)\n\n\n'data.frame':   5 obs. of  3 variables:\n $ ID      : chr  \"a\" \"b\" \"c\" \"d\" ...\n $ size    : num  1 2 3 4 5\n $ observed: logi  FALSE TRUE FALSE FALSE FALSE\n\n\n \nPero los vectores deben tener la misma longitud:\n\n\nCódigo\nm &lt;-\n  data.frame(\n    ID = c(\"a\", \"b\", \"c\", \"d\", \"e\"),\n    size = c(1, 2, 3, 4, 5, 6),\n    observed = c(FALSE, TRUE, FALSE, FALSE, FALSE)\n  )\n\n\nError in data.frame(ID = c(\"a\", \"b\", \"c\", \"d\", \"e\"), size = c(1, 2, 3, : arguments imply differing number of rows: 5, 6\n\n\n \n\n\n3.1.5 Ejercicio 1\n \n\nCree un vector numérico con 8 elementos que contenga números positivos y negativos\nCree un vector de caracteres con los nombres de las provincias de Costa Rica\nAñada al vector de punto anterior un NA\nCree una matriz numérica con 3 columnas y 3 filas\nCree una matriz de caracteres con 4 columnas y 3 filas\n¿Qué tipo de objeto es ‘iris’ y cuales son sus dimensiones? (pista: iris es un objeto disponible por omisión en su ambiente)\nCree un cuadro de datos (data frame) con una columna numérica, una columna de caracteres y una columna con factores"
  },
  {
    "objectID": "fundamentos_de_r.html#funciones-objetos-que-hacen-tareas",
    "href": "fundamentos_de_r.html#funciones-objetos-que-hacen-tareas",
    "title": "Elementos básicos del lenguaje R",
    "section": "3.2 Funciones: objetos que hacen tareas",
    "text": "3.2 Funciones: objetos que hacen tareas\nTodas las funciones se crean con la función function() y siguen la misma estructura:\n\n* Modified from Grolemund 2014  \n\n3.2.1 Funciones integradas\n\n3.2.1.1 Funciones básicas\nR viene con muchas funciones que puedes usar para hacer tareas sofisticadas:\n\n\nCódigo\n# built in functions\nbi &lt;- builtins(internal = FALSE)\n\nlength(bi)\n\n\n[1] 1402\n\n\n \nAlgunas funciones vienen de forma predeterminada con R básico. Nuevas funciones pueden ser cargadas como parte de paquetes adicionales o incluso creadas por el usuario.\n\n\n\n\n\nflowchart LR\n    classDef largeText font-size:18px, padding:15px;\n\n    F(Funciones) --&gt; BF(Funciones Integradas)\n    BF --&gt; OP(Operadores)\n    BF --&gt; BA(Funciones Básicas)\n    F --&gt; PF(Paquetes)\n    F --&gt; UF(Funciones Definidas por el Usuario)\n\n    class R,D,D1,D2,F largeText;\n\n    style F fill:#357BA266, stroke:#000, stroke-width:2px, color:#FFF, width:120px\n    style BF fill:#A0DFB966, stroke:#000, stroke-width:2px, color:#000\n    style BA fill:#DEF5E566, stroke:#000, stroke-width:2px, color:#000\n    style OP fill:#DEF5E566, stroke:#000, stroke-width:2px, color:#000    \n    style PF fill:#A0DFB966, stroke:#000, stroke-width:2px, color:#000000\n    style UF fill:#A0DFB966, stroke:#000, stroke-width:2px, color:#000\n\n\n\n\n\n\n\n\n\n3.2.1.2 Operadores\nLos operadores son funciones:\n\n\nCódigo\n1 + 1\n\n\n[1] 2\n\n\nCódigo\n'+'(1, 1)\n\n\n[1] 2\n\n\nCódigo\n2 * 3\n\n\n[1] 6\n\n\nCódigo\n'*'(2, 3)\n\n\n[1] 6\n\n\n \n\n3.2.1.2.1 Operadores mas utilizados\nOperadores aritméticos:\n\n\n\n\n\n\n\nOperador\nDescrición\n\n\n\n\n+\nsuma\n\n\n-\nresta\n\n\n*\nmultiplicación\n\n\n/\ndivisión\n\n\n^ or **\nexponente\n\n\n\n\n\n\n\n\n\n\n\nCódigo\n1 - 2\n\n\n[1] -1\n\n\nCódigo\n1 + 2\n\n\n[1] 3\n\n\nCódigo\n2 ^ 2\n\n\n[1] 4\n\n\nCódigo\n2 ** 2\n\n\n[1] 4\n\n\nCódigo\n2:3 %in% 2:4\n\n\n[1] TRUE TRUE\n\n\n \nOperadores lógicos:\n\n\n\nOperador\nDescrición\n\n\n\n\n&lt;\nmenor que\n\n\n&lt;=\nmenor o igual que\n\n\n&gt;\nmayor que\n\n\n&gt;=\nmayor o igual que\n\n\n==\nexactamente igual que\n\n\n!=\ndiferente que\n\n\n!x\nNo es x\n\n\nx | y\nx O y\n\n\nx & y\nx Y y\n\n\nx %in% y\ncorrespondencia\n\n\n\n\n\nCódigo\n1 &lt; 2 \n\n\n[1] TRUE\n\n\nCódigo\n1 &gt; 2 \n\n\n[1] FALSE\n\n\nCódigo\n1 &lt;= 2 \n\n\n[1] TRUE\n\n\nCódigo\n1 == 2\n\n\n[1] FALSE\n\n\nCódigo\n1 != 2\n\n\n[1] TRUE\n\n\nCódigo\n1 &gt; 2 \n\n\n[1] FALSE\n\n\nCódigo\n5 %in% 1:6\n\n\n[1] TRUE\n\n\nCódigo\n5 %in% 1:4\n\n\n[1] FALSE\n\n\n \n\n\n\n\n3.2.2 Vectorización\nLa mayoría de las funciones están vectorizadas:\n\n\nCódigo\n1:6 * 1:6\n\n\n\n* Modified from Grolemund & Wickham 2017\n \n\n\n[1]  1  4  9 16 25 36\n\n\n\n\nCódigo\n1:6 - 1:6\n\n\n[1] 0 0 0 0 0 0\n\n\nR recicla vectores de longitud desigual:\n\n\nCódigo\n1:6 * 1:5\n\n\n\n* Modified from Grolemund & Wickham 2017\n\n \n\n3.2.3 Funciones de paquetes adicionales\nEstas son funciones que son incluidas en paquetes adicionales que se pueden instalar y cargar en R. Para ser utilizadas el paquete debe ser instalado y cargado.Por ejemplo para usar la función corTest del paquete “psych” primero debemos instalar. Los paquetes son instalados del servidor de CRAN (Comprehensive R Archive Network) con la función install.packages():\n\n\nCódigo\ninstall.packages(\"psych\")\n\n\n.. y cargar el paquete:\n\n\nCódigo\nlibrary(psych)\n\n\nUna vez instalado y cargado ‘psych’, podemos llamar a la función corTest:\n\n\nCódigo\ncorTest(iris$Sepal.Length, iris$Sepal.Width)\n\n\nCall:corTest(x = iris$Sepal.Length, y = iris$Sepal.Width)\nCorrelation matrix \n[1] -0.12\nSample Size \n[1] 150\nThese are the unadjusted probability values.\n  The probability values  adjusted for multiple tests are in the p.adj object. \n[1] 0.15\n\n To see confidence intervals of the correlations, print with the short=FALSE option\n\n\nEl uso de paquetes externos es la caracteristica mas util de R ya que permite hacer uso de un número casi infinito de funciones especializadas en diferentes tareas así como de campos muy diversos de la ciencia y la industria.\nPodemos explorar los paquetes disponibles para R en la página de CRAN (hacer click en el enlace “packages”).\n\n\n3.2.4 Ejercicio 2\n \n\nBusque un paquete que le interese en CRAN\nInstale el paquete y carguelo\nCorra el codigo de ejemplo de una de sus funciones"
  },
  {
    "objectID": "fundamentos_de_r.html#extraer-subconjuntos-usando-indexación-indexing",
    "href": "fundamentos_de_r.html#extraer-subconjuntos-usando-indexación-indexing",
    "title": "Elementos básicos del lenguaje R",
    "section": "4.1 Extraer subconjuntos usando indexación (indexing)",
    "text": "4.1 Extraer subconjuntos usando indexación (indexing)\nLos elementos dentro de los objetos pueden ser llamados por medio de la indexación. Para sub-conjuntar un vector simplemente llame a la posición del objeto usando corchetes:\n\n\nCódigo\nx &lt;- c(1, 3, 4, 10, 15, 20, 50, 1, 6)\n\nx[1]\n\n\n[1] 1\n\n\nCódigo\nx[2]\n\n\n[1] 3\n\n\nCódigo\nx[2:3]\n\n\n[1] 3 4\n\n\nCódigo\nx[c(1,3)]\n\n\n[1] 1 4\n\n\n \nLos elementos se pueden eliminar de la misma manera:\n\n\nCódigo\nx[-1]\n\n\n[1]  3  4 10 15 20 50  1  6\n\n\nCódigo\nx[-c(1,3)]\n\n\n[1]  3 10 15 20 50  1  6\n\n\n \nLas matrices y los marcos de datos requieren 2 índices [fila, columna]:\n\n\nCódigo\nm &lt;- matrix(c(1, 2, 3, 11, 12, 13), nrow = 2)\n\nm[1, ]\n\n\n[1]  1  3 12\n\n\nCódigo\nm[, 1]\n\n\n[1] 1 2\n\n\nCódigo\nm[1, 1]\n\n\n[1] 1\n\n\nCódigo\nm[-1, ]\n\n\n[1]  2 11 13\n\n\nCódigo\nm[, -1]\n\n\n     [,1] [,2]\n[1,]    3   12\n[2,]   11   13\n\n\nCódigo\nm[-1, -1]\n\n\n[1] 11 13\n\n\nCódigo\ndf &lt;- data.frame(\n  provincia = c(\"San José\", \"Guanacaste\", \"Guanacaste\"), \n  canton = c(\"Montes de Oca\", \"Nicoya\", \"Liberia\"), \n    distrito = c(\"San Rafael\", \"Nosara\", \"Nacascolo\")\n  )\n\ndf\n\n\n\n\n\n\nprovincia\ncanton\ndistrito\n\n\n\n\nSan José\nMontes de Oca\nSan Rafael\n\n\nGuanacaste\nNicoya\nNosara\n\n\nGuanacaste\nLiberia\nNacascolo\n\n\n\n\n\n\nCódigo\ndf[1, ]\n\n\n\n\n\n\nprovincia\ncanton\ndistrito\n\n\n\n\nSan José\nMontes de Oca\nSan Rafael\n\n\n\n\n\n\nCódigo\ndf[, 1]\n\n\n[1] \"San José\"   \"Guanacaste\" \"Guanacaste\"\n\n\nCódigo\ndf[1, 1]\n\n\n[1] \"San José\"\n\n\nCódigo\ndf[-1, ]\n\n\n\n\n\n\n\nprovincia\ncanton\ndistrito\n\n\n\n\n2\nGuanacaste\nNicoya\nNosara\n\n\n3\nGuanacaste\nLiberia\nNacascolo\n\n\n\n\n\n\nCódigo\ndf[, -1]\n\n\n\n\n\n\ncanton\ndistrito\n\n\n\n\nMontes de Oca\nSan Rafael\n\n\nNicoya\nNosara\n\n\nLiberia\nNacascolo\n\n\n\n\n\n\nCódigo\ndf[-1, -1]\n\n\n\n\n\n\n\ncanton\ndistrito\n\n\n\n\n2\nNicoya\nNosara\n\n\n3\nLiberia\nNacascolo\n\n\n\n\n\n\nCódigo\ndf[,\"provincia\"]\n\n\n[1] \"San José\"   \"Guanacaste\" \"Guanacaste\"\n\n\nCódigo\ndf[,c(\"provincia\", \"canton\")]\n\n\n\n\n\n\nprovincia\ncanton\n\n\n\n\nSan José\nMontes de Oca\n\n\nGuanacaste\nNicoya\n\n\nGuanacaste\nLiberia\n\n\n\n\n\n\n \nLas listas requieren 1 índice entre dobles corchetes [[índice]]:\n\n\nCódigo\nl &lt;- list(c(\"a\", \"b\"),\n          c(1, 2, 3),\n          c(FALSE, TRUE, FALSE, FALSE))\n\nl[[1]]\n\n\n[1] \"a\" \"b\"\n\n\nCódigo\nl[[3]]\n\n\n[1] FALSE  TRUE FALSE FALSE\n\n\n \nLos elementos dentro de las listas también pueden ser subconjuntos en la misma cadena de código:\n\n\nCódigo\nl[[1]][1:2]\n\n\n[1] \"a\" \"b\"\n\n\nCódigo\nl[[3]][2]\n\n\n[1] TRUE"
  },
  {
    "objectID": "fundamentos_de_r.html#explorar-objectos",
    "href": "fundamentos_de_r.html#explorar-objectos",
    "title": "Elementos básicos del lenguaje R",
    "section": "4.2 Explorar objectos",
    "text": "4.2 Explorar objectos\nLas siguientes funciones de R básico (predeterminadas) nos pueden ayudar a explorar la estructura de los objetos:\n\n\nCódigo\nstr(df)\n\n\n'data.frame':   3 obs. of  3 variables:\n $ provincia: chr  \"San José\" \"Guanacaste\" \"Guanacaste\"\n $ canton   : chr  \"Montes de Oca\" \"Nicoya\" \"Liberia\"\n $ distrito : chr  \"San Rafael\" \"Nosara\" \"Nacascolo\"\n\n\nCódigo\nnames(df)\n\n\n[1] \"provincia\" \"canton\"    \"distrito\" \n\n\nCódigo\ndim(df)\n\n\n[1] 3 3\n\n\nCódigo\nnrow(df)\n\n\n[1] 3\n\n\nCódigo\nncol(df)\n\n\n[1] 3\n\n\nCódigo\nhead(df)\n\n\n\n\n\n\nprovincia\ncanton\ndistrito\n\n\n\n\nSan José\nMontes de Oca\nSan Rafael\n\n\nGuanacaste\nNicoya\nNosara\n\n\nGuanacaste\nLiberia\nNacascolo\n\n\n\n\n\n\nCódigo\ntail(df)\n\n\n\n\n\n\nprovincia\ncanton\ndistrito\n\n\n\n\nSan José\nMontes de Oca\nSan Rafael\n\n\nGuanacaste\nNicoya\nNosara\n\n\nGuanacaste\nLiberia\nNacascolo\n\n\n\n\n\n\nCódigo\ntable(df$provincia)\n\n\n\nGuanacaste   San José \n         2          1 \n\n\nCódigo\nclass(df)\n\n\n[1] \"data.frame\"\n\n\n\n\nCódigo\nView(df)"
  },
  {
    "objectID": "fundamentos_de_r.html#ejercicio-3",
    "href": "fundamentos_de_r.html#ejercicio-3",
    "title": "Elementos básicos del lenguaje R",
    "section": "4.3 Ejercicio 3",
    "text": "4.3 Ejercicio 3\n \n\nUtilice los datos de ejemplo iris para crear un subconjunto de datos con sólo las observaciones de la especie setosa\nAhora cree un subconjunto de datos que contenga las observaciones tanto de “setosa” como de “versicolor”\nTambién con iris cree un subconjunto de datos con las observaciones para las que iris$Sepal.length es mayor que 6\n¿Cuántas observaciones tienen una longitud de sépalo superior a 6?"
  },
  {
    "objectID": "fundamentos_de_r.html#section-1",
    "href": "fundamentos_de_r.html#section-1",
    "title": "Elementos básicos del lenguaje R",
    "section": "4.4  ",
    "text": "4.4"
  },
  {
    "objectID": "fundamentos_de_r.html#nombres-de-archivos",
    "href": "fundamentos_de_r.html#nombres-de-archivos",
    "title": "Elementos básicos del lenguaje R",
    "section": "5.1 Nombres de archivos",
    "text": "5.1 Nombres de archivos\nLos nombres de los archivos deben terminar en .R y, por supuesto, ser auto-explicatorios:\n\nBien: graficar_probabilidad_posterior.R\nMal: graf.R"
  },
  {
    "objectID": "fundamentos_de_r.html#nombres-de-objetos",
    "href": "fundamentos_de_r.html#nombres-de-objetos",
    "title": "Elementos básicos del lenguaje R",
    "section": "5.2 Nombres de objetos",
    "text": "5.2 Nombres de objetos\nVariables y funciones:\n\nMinúsculas\nUtilice un guión bajo\nEn general, nombres para las variables y verbos para las funciones\nProcure que los nombres sean concisos y significativos (no siempre es fácil)\nAvoid using names of existing functions of variables\n\n\n\nCódigo\n  - Bien: dia_uno: dia_1, peso_promedio(),\n  \n  - Mal: diauno, dia1, primer.dia_delmes"
  },
  {
    "objectID": "fundamentos_de_r.html#syntaxis",
    "href": "fundamentos_de_r.html#syntaxis",
    "title": "Elementos básicos del lenguaje R",
    "section": "5.3 Syntaxis",
    "text": "5.3 Syntaxis\n\n5.3.1 Espacios\n\nUtilice espacios alrededor de los operadores y para los argumentos dentro de una función\nPonga siempre un espacio después de una coma, y nunca antes (como en el inglés normal)\nColoque un espacio antes del paréntesis izquierdo, excepto en una llamada a una función\n\n\n\nCódigo\n  - Bien: \n          a &lt;- rnorm(n = 10, sd = 10, mean = 1)\n          total &lt;- sum(x[1, ])\n\n  - Mal: \n         a&lt;-rnorm(n=10,sd=10,mean=1) \n         total &lt;- sum(x[,1])  \n\n\n \n\n5.3.1.1 Corchetes\n\nLa llave de apertura nunca debe ir en su propia línea\nLa llave de cierre debe ir siempre en su propia línea\nPuede omitir las llaves cuando un bloque consiste en una sola declaración\n\n\n\nCódigo\n  - Bien:\n              if (is.null(ylim)) {\n              ylim &lt;- c(0, 0.06)\n            }\n                      \n            if (is.null(ylim))\n              ylim &lt;- c(0, 0.06)\n          \n  - Mal:\n            \n         if (is.null(ylim)) ylim &lt;- c(0, 0.06)\n                    \n         if (is.null(ylim)) {ylim &lt;- c(0, 0.06)} \n\n         if (is.null(ylim)) {\n           ylim &lt;- c(0, 0.06)\n           } \n\n\n \n\n\n5.3.1.2 Crear objetos\n\nUse &lt;-, no =\n\n\n\nCódigo\n  - GOOD:\n         x &lt;- 5 \n          \n  - BAD:\n         x = 5\n\n\n \n\n\n5.3.1.3 Sugerencias para añadir comentarios\n\nComente su código\nLas líneas enteras comentadas deben comenzar con # y un espacio\nLos comentarios cortos pueden colocarse después del código precedido por dos espacios, #, y luego un espacio\n\n\n\nCódigo\n# Create histogram of frequency of campaigns by pct budget spent.\nhist(df$pct.spent,\n     breaks = \"scott\",  # method for choosing number of buckets\n     main   = \"Histograma: individuos por unidad de tiempo\",\n     xlab   = \"Número de individuos\",\n     ylab   = \"Frecuencia\")\n\n\n \n\n\n5.3.1.4 Disposición general y ordenación (estilo de google)\n\nComentario de la declaración de derechos de autor (?)\nComentario del autor\nComentario de la descripción del archivo, incluyendo el propósito del programa, las entradas y las salidas\ndeclaraciones source() y library()\nDefiniciones de funciones\nSentencias ejecutadas, si procede (por ejemplo, print, plot)"
  },
  {
    "objectID": "fundamentos_de_r.html#package-documentation",
    "href": "fundamentos_de_r.html#package-documentation",
    "title": "Elementos básicos del lenguaje R",
    "section": "6.1 Package documentation",
    "text": "6.1 Package documentation\n\n \nReference manuals\nLos manuales de referencia son colecciones de la documentación de todas las funciones de un paquete (sólo 1 por paquete):\n\nmanual de dynaSpec\nmanual de baRulho"
  },
  {
    "objectID": "fundamentos_de_r.html#documentación-de-las-funciones",
    "href": "fundamentos_de_r.html#documentación-de-las-funciones",
    "title": "Elementos básicos del lenguaje R",
    "section": "6.2 Documentación de las funciones",
    "text": "6.2 Documentación de las funciones\nTodas las funciones (por defecto o de paquetes cargados) deben tener una documentación que siga un formato estándar:\n\n\nCódigo\n?mean\n\nhelp(\"mean\")\n\n\n\nEsta documentación también puede mostrarse en Rstudio pulsando F1 cuando el cursor está en el nombre de la función\n \nSi no recuerda el nombre de la función pruebe con apropos():\n\n\nCódigo\napropos(\"mean\")\n\n\n [1] \".colMeans\"      \".rowMeans\"      \"circadian.mean\" \"circular.mean\" \n [5] \"colMeans\"       \"geometric.mean\" \"harmonic.mean\"  \"kmeans\"        \n [9] \"mean\"           \"mean.Date\"      \"mean.default\"   \"mean.difftime\" \n[13] \"mean.POSIXct\"   \"mean.POSIXlt\"   \"rowMeans\"       \"weighted.mean\" \n[17] \"winsor.mean\"    \"winsor.means\""
  },
  {
    "objectID": "fundamentos_de_r.html#viñetas-vignettes",
    "href": "fundamentos_de_r.html#viñetas-vignettes",
    "title": "Elementos básicos del lenguaje R",
    "section": "6.3 Viñetas (vignettes)",
    "text": "6.3 Viñetas (vignettes)\nLas viñetas son documentos ilustrativos o casos de estudio que detallan el uso de un paquete (opcional, pueden ser varios por paquete).\nLas viñetas se pueden llamar directamente desde R:\n\n\nCódigo\nvgn &lt;- browseVignettes() \n\n\n\n\nCódigo\nvignette()\n\n\nTambién deberían aparecer en la página del paquete en CRAN."
  },
  {
    "objectID": "fundamentos_de_r.html#demonstraciones",
    "href": "fundamentos_de_r.html#demonstraciones",
    "title": "Elementos básicos del lenguaje R",
    "section": "6.4 Demonstraciones",
    "text": "6.4 Demonstraciones\nLos paquetes también pueden incluir demostraciones de código extendidas (“demos”). Para listar las demos de un paquete ejecute demo(\"nombre del paquete\"):\n\n\nCódigo\ndemo(package=\"stats\")\n\n# call demo directly\ndemo(\"nlm\")"
  },
  {
    "objectID": "fundamentos_de_r.html#ejercicio-4",
    "href": "fundamentos_de_r.html#ejercicio-4",
    "title": "Elementos básicos del lenguaje R",
    "section": "6.5 Ejercicio 4",
    "text": "6.5 Ejercicio 4\n \n\n¿Qué hace la función cut()?\n¿Para qué se utiliza el argumento breaks en cut()?\nEjecuta las 4 primeras líneas de código de los ejemplos proporcionados en la documentación de cut().\n¿Cuántas viñetas tiene el paquete warbleR?"
  },
  {
    "objectID": "fundamentos_de_r.html#referencias",
    "href": "fundamentos_de_r.html#referencias",
    "title": "Elementos básicos del lenguaje R",
    "section": "6.6 Referencias",
    "text": "6.6 Referencias\n\nAdvanced R, H Wickham\nGoogle’s R Style Guide\n\nHands-On Programming with R (Grolemund, 2014)\n\n\nInformación de la sesión\n\n\nR version 4.4.1 (2024-06-14)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 22.04.4 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0 \nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=es_CR.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=es_CR.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=es_CR.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=es_CR.UTF-8 LC_IDENTIFICATION=C       \n\ntime zone: America/Costa_Rica\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] psych_2.4.6.26\n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.5       nlme_3.1-165      svglite_2.1.3     cli_3.6.3        \n [5] knitr_1.48        rlang_1.1.4       xfun_0.48         stringi_1.8.4    \n [9] highr_0.11        jsonlite_1.8.9    glue_1.8.0        colorspace_2.1-1 \n[13] htmltools_0.5.8.1 scales_1.3.0      rmarkdown_2.28    grid_4.4.1       \n[17] evaluate_1.0.1    munsell_0.5.1     kableExtra_1.4.0  fastmap_1.2.0    \n[21] yaml_2.3.10       lifecycle_1.0.4   stringr_1.5.1     compiler_4.4.1   \n[25] htmlwidgets_1.6.4 rstudioapi_0.16.0 lattice_0.22-6    systemfonts_1.1.0\n[29] digest_0.6.37     viridisLite_0.4.2 R6_2.5.1          parallel_4.4.1   \n[33] mnormt_2.1.1      magrittr_2.0.3    tools_4.4.1       xml2_1.3.6"
  },
  {
    "objectID": "intro_curso.html#qué-es-el-aprendizaje-estadístico",
    "href": "intro_curso.html#qué-es-el-aprendizaje-estadístico",
    "title": "Aprendizaje Estadístico, SEP-UCR 2024",
    "section": "¿Qué es el Aprendizaje Estadístico?",
    "text": "¿Qué es el Aprendizaje Estadístico?\n\n\nUtilización de datos para modelar fenómenos, hacer predicciones y tomar decisiones basadas en la información extraída\nCombina teorías estadísticas con algoritmos computacionales"
  },
  {
    "objectID": "intro_curso.html#qué-es-el-aprendizaje-estadístico-1",
    "href": "intro_curso.html#qué-es-el-aprendizaje-estadístico-1",
    "title": "Aprendizaje Estadístico, SEP-UCR 2024",
    "section": "¿Qué es el Aprendizaje Estadístico?",
    "text": "¿Qué es el Aprendizaje Estadístico?\n\n\n\n\n\nInteligencia Artificial: hacer tareas que requieren inteligencia humana\nAprendizaje estadístico: uso de algoritmos para analizar datos, aprender de ellos, y hacer predicciones"
  },
  {
    "objectID": "intro_curso.html#qué-es-el-aprendizaje-estadístico-2",
    "href": "intro_curso.html#qué-es-el-aprendizaje-estadístico-2",
    "title": "Aprendizaje Estadístico, SEP-UCR 2024",
    "section": "¿Qué es el Aprendizaje Estadístico?",
    "text": "¿Qué es el Aprendizaje Estadístico?\n\n\n\n\n\nInteligencia Artificial: hacer tareas que requieren inteligencia humana\nAprendizaje estadístico: uso de algoritmos para analizar datos, aprender de ellos, y hacer predicciones\nAprendizaje Profundo: uso de redes neuronales profundas para analizar diversos niveles de abstracción en los datos\nCiencia de Datos: técnicas de análisis, procesamiento, y visualización de datos para extraer información/conocimiento de grandes volúmenes de datos"
  },
  {
    "objectID": "intro_curso.html#qué-es-el-aprendizaje-estadístico-3",
    "href": "intro_curso.html#qué-es-el-aprendizaje-estadístico-3",
    "title": "Aprendizaje Estadístico, SEP-UCR 2024",
    "section": "¿Qué es el Aprendizaje Estadístico?",
    "text": "¿Qué es el Aprendizaje Estadístico?\n\n\\[ Y = f(X) + E \\]\n\n\n\nY: variable dependiente (se quiere predecir o explicar)\n\n\n\n\nX: variable(s) independiente(s) con información relevante para predecir Y\n\n\n\n\nE: error no reducible"
  },
  {
    "objectID": "intro_curso.html#qué-es-el-aprendizaje-estadístico-4",
    "href": "intro_curso.html#qué-es-el-aprendizaje-estadístico-4",
    "title": "Aprendizaje Estadístico, SEP-UCR 2024",
    "section": "¿Qué es el Aprendizaje Estadístico?",
    "text": "¿Qué es el Aprendizaje Estadístico?\n\n\\[ Y = f(X) + E \\]\n\n\n\nsinónimos de Y: variable dependiente, resultado (outcome), etiqueta, objetivo, etc.\n\n\n\n\nsinónimos de X: variable independiente, atributo (feature), dimensión, predictor, entrada (input), etc."
  },
  {
    "objectID": "intro_curso.html#para-que-estimar-fx",
    "href": "intro_curso.html#para-que-estimar-fx",
    "title": "Aprendizaje Estadístico, SEP-UCR 2024",
    "section": "¿Para que Estimar f(X)?",
    "text": "¿Para que Estimar f(X)?\n\n\\[ Y = f(X) + E \\]\n\n\nPredicción: predecir valores futuros de Y\nInferencia: establecer una relación causal (i.e. estimar el efecto de un cambio en X sobre el cambio en Y)\n\n\n\n\nAprendizaje estadístico = estimar f(X)"
  },
  {
    "objectID": "intro_curso.html#aplicaciones",
    "href": "intro_curso.html#aplicaciones",
    "title": "Aprendizaje Estadístico, SEP-UCR 2024",
    "section": "Aplicaciones",
    "text": "Aplicaciones\nOmnipresente en herramientas electrónicas:\n\nDetección de correo spam: busca patrones en el contenido del mensaje, como palabras clave sospechosas y comportamientos de envío\nReconocimeinto facial: analiza las características faciales desde diferentes ángulos y aprenden a reconocer variaciones sutiles en la apariencia de un individuo\nOptimización de Rutas con GPS: usa los patrones de tráfico históricos y en tiempo real para predecir y sugerir las rutas más rápidas o eficientes"
  },
  {
    "objectID": "intro_curso.html#aplicaciones-en-ciencias-cognoscitivas",
    "href": "intro_curso.html#aplicaciones-en-ciencias-cognoscitivas",
    "title": "Aprendizaje Estadístico, SEP-UCR 2024",
    "section": "Aplicaciones en Ciencias Cognoscitivas",
    "text": "Aplicaciones en Ciencias Cognoscitivas\n\nCrucial para entender cómo procesamos la información, tomamos decisiones y aprendemos. Algunos sub-campos que utilizan estas herramientas:\n\n\n\nNeurociencia Cognitiva: Identificación de patrones en la actividad cerebral relacionados con diferentes funciones cognitivas\nPsicología Experimental: Modelado de comportamiento humano y predicción de respuestas psicológicas\nNeurociencia Computacional: Modelado de la actividad neuronal para simular procesos cerebrales"
  },
  {
    "objectID": "intro_curso.html#section",
    "href": "intro_curso.html#section",
    "title": "Aprendizaje Estadístico, SEP-UCR 2024",
    "section": "",
    "text": "Aplicaciones: Memoria\n\n\n\n\nRoediger et al 2001. Factors that determine false recall: A multiple regression analysis\nRegresión multiple para identificar factores que influyen en el recuerdo falso\n\n\n\nRecuerdo verídico (r = -0.43) y la fuerza de la asociación entre items (r = 0.73) influyen\n\n\n\n\n\n\n\n\n\n\n\n\\(Y: recuerdo\\ falso \\sim X: asociacion\\ entre\\ items + recuerdo\\ verídico + ...\\)"
  },
  {
    "objectID": "intro_curso.html#section-1",
    "href": "intro_curso.html#section-1",
    "title": "Aprendizaje Estadístico, SEP-UCR 2024",
    "section": "",
    "text": "Aplicaciones: Reconocimiento de Emociones\n\n\n\n\nHema et al 2023. Emotional speech Recognition using CNN and Deep learning techniques\nAprendizaje profundo en el reconocimiento y análisis del tono emocional en la voz\n78% de precision\n\n\n\n\n\n\n\n\n\\(Y: emoción \\sim X: estructura\\ acústica\\)"
  },
  {
    "objectID": "intro_curso.html#section-2",
    "href": "intro_curso.html#section-2",
    "title": "Aprendizaje Estadístico, SEP-UCR 2024",
    "section": "",
    "text": "Aplicaciones: Resilencia al Estrés\n\n\n\n\nChen et al. 2023. Identifying the top determinants of psychological resilience among community older adults during COVID-19 in Taiwan: A random forest approach\nFactores que determinan la resiliencia al estrés\n80% de probabilidad de una predición correcta\n\n\n\n\n\n\n\n\n\n\\(Y:resilencia\\ (binaria) \\sim X: género + edad + educación + ...\\)"
  },
  {
    "objectID": "intro_curso.html#tipos-de-modelos",
    "href": "intro_curso.html#tipos-de-modelos",
    "title": "Aprendizaje Estadístico, SEP-UCR 2024",
    "section": "Tipos de Modelos",
    "text": "Tipos de Modelos\nSupervisado: Se predice una Y conocida a priori\n\\[ Y = f(X) \\]\n\n\n\n\n\n\nY\nX1\nX2\nX3\n\n\n\n\n-0.56\n1.72\n1.22\n1.79\n\n\n-0.23\n0.46\n0.36\n0.50\n\n\n1.56\n-1.27\n0.40\n-1.97\n\n\n0.07\n-0.69\n0.11\n0.70\n\n\n0.13\n-0.45\n-0.56\n-0.47"
  },
  {
    "objectID": "intro_curso.html#tipos-de-modelos-1",
    "href": "intro_curso.html#tipos-de-modelos-1",
    "title": "Aprendizaje Estadístico, SEP-UCR 2024",
    "section": "Tipos de Modelos",
    "text": "Tipos de Modelos\nSupervisado: Se predice una Y conocida a priori\n\\[ Y = f(X) \\]\n\n\n\n\n\n\nY\nX1\nX2\nX3\n\n\n\n\nA\n-0.55\n-1.60\n-0.83\n\n\nC\n-0.02\n0.66\n-0.62\n\n\nB\n-0.58\n-1.87\n-0.29\n\n\nC\n1.07\n0.26\n-0.50\n\n\nA\n1.04\n-0.22\n1.10"
  },
  {
    "objectID": "intro_curso.html#tipos-de-modelos-2",
    "href": "intro_curso.html#tipos-de-modelos-2",
    "title": "Aprendizaje Estadístico, SEP-UCR 2024",
    "section": "Tipos de Modelos",
    "text": "Tipos de Modelos\nNo supervisado: genera una Y con base en la estructura de X\n\\[ f(X) \\]\n\n\n\n\n\n\nY\nX1\nX2\nX3\n\n\n\n\n?\n-0.56\n1.72\n1.22\n\n\n?\n-0.23\n0.46\n0.36\n\n\n?\n1.56\n-1.27\n0.40\n\n\n?\n0.07\n-0.69\n0.11\n\n\n?\n0.13\n-0.45\n-0.56"
  },
  {
    "objectID": "intro_curso.html#tipos-de-modelos-3",
    "href": "intro_curso.html#tipos-de-modelos-3",
    "title": "Aprendizaje Estadístico, SEP-UCR 2024",
    "section": "Tipos de Modelos",
    "text": "Tipos de Modelos\nNo supervisado: ó explora la estructura de X\n\\[ f(X) \\]\n\n\n\n\n\n\nX1\nX2\nX3\n\n\n\n\n-0.56\n1.72\n1.22\n\n\n-0.23\n0.46\n0.36\n\n\n1.56\n-1.27\n0.40\n\n\n0.07\n-0.69\n0.11\n\n\n0.13\n-0.45\n-0.56"
  },
  {
    "objectID": "intro_curso.html#escogiendo-el-modelo-adecuado",
    "href": "intro_curso.html#escogiendo-el-modelo-adecuado",
    "title": "Aprendizaje Estadístico, SEP-UCR 2024",
    "section": "Escogiendo el Modelo Adecuado",
    "text": "Escogiendo el Modelo Adecuado\n\n\n¿Existe Y?"
  },
  {
    "objectID": "intro_curso.html#escogiendo-el-modelo-adecuado-1",
    "href": "intro_curso.html#escogiendo-el-modelo-adecuado-1",
    "title": "Aprendizaje Estadístico, SEP-UCR 2024",
    "section": "Escogiendo el Modelo Adecuado",
    "text": "Escogiendo el Modelo Adecuado\n\n\nSi Y existe ¿Es continua o categórica?"
  },
  {
    "objectID": "intro_curso.html#escogiendo-el-modelo-adecuado-2",
    "href": "intro_curso.html#escogiendo-el-modelo-adecuado-2",
    "title": "Aprendizaje Estadístico, SEP-UCR 2024",
    "section": "Escogiendo el Modelo Adecuado",
    "text": "Escogiendo el Modelo Adecuado\n\n\nSi Y no existe ¿Queremos generarla o solo explorar X?"
  },
  {
    "objectID": "intro_curso.html#escogiendo-el-modelo-adecuado-3",
    "href": "intro_curso.html#escogiendo-el-modelo-adecuado-3",
    "title": "Aprendizaje Estadístico, SEP-UCR 2024",
    "section": "Escogiendo el Modelo Adecuado",
    "text": "Escogiendo el Modelo Adecuado"
  },
  {
    "objectID": "intro_curso.html#escogiendo-el-modelo-adecuado-4",
    "href": "intro_curso.html#escogiendo-el-modelo-adecuado-4",
    "title": "Aprendizaje Estadístico, SEP-UCR 2024",
    "section": "Escogiendo el Modelo Adecuado",
    "text": "Escogiendo el Modelo Adecuado"
  },
  {
    "objectID": "intro_curso.html#escogiendo-el-modelo-adecuado-5",
    "href": "intro_curso.html#escogiendo-el-modelo-adecuado-5",
    "title": "Aprendizaje Estadístico, SEP-UCR 2024",
    "section": "Escogiendo el Modelo Adecuado",
    "text": "Escogiendo el Modelo Adecuado\n\nInterpretabilidad\n\n\nTomado de Badillo et al 2020"
  },
  {
    "objectID": "programa.html#recursos-adicionales",
    "href": "programa.html#recursos-adicionales",
    "title": "Antes de empezar el curso",
    "section": "Recursos adicionales",
    "text": "Recursos adicionales\n\nLectura: Badillo et al (2020). An introduction to machine learning"
  },
  {
    "objectID": "programa.html#recursos-adicionales-1",
    "href": "programa.html#recursos-adicionales-1",
    "title": "Antes de empezar el curso",
    "section": "Recursos adicionales",
    "text": "Recursos adicionales\n\nLectura: Roediger et al (2001). Factors that determine false recall: A multiple regression analysis"
  },
  {
    "objectID": "programa.html#recursos-adicionales-2",
    "href": "programa.html#recursos-adicionales-2",
    "title": "Antes de empezar el curso",
    "section": "Recursos adicionales",
    "text": "Recursos adicionales\n\nLectura: Chen et al (2023). Identifying the top determinants of psychological resilience among community older adults during COVID-19 in Taiwan: A random forest approach"
  },
  {
    "objectID": "programa.html#recursos-adicionales-3",
    "href": "programa.html#recursos-adicionales-3",
    "title": "Antes de empezar el curso",
    "section": "Recursos adicionales",
    "text": "Recursos adicionales\n\nLectura: Choi et al. (2020). Introduction to machine learning, neural networks, and deep learning\nVideo: Random Forest Algorithm Clearly Explained"
  },
  {
    "objectID": "programa.html#referencia",
    "href": "programa.html#referencia",
    "title": "Antes de empezar el curso",
    "section": "Referencia",
    "text": "Referencia\nCortez, P., & Silva, A.M. (2008). Using data mining to predict secondary school student performance. (enlace)"
  },
  {
    "objectID": "programa.html#recursos-adicionales-4",
    "href": "programa.html#recursos-adicionales-4",
    "title": "Antes de empezar el curso",
    "section": "Recursos adicionales",
    "text": "Recursos adicionales\n\nVideo: Convolutional Neural Networks (CNNs) explained\nVideo: Neural Network In 5 Minutes"
  },
  {
    "objectID": "programa.html#recursos-adicionales-5",
    "href": "programa.html#recursos-adicionales-5",
    "title": "Antes de empezar el curso",
    "section": "Recursos adicionales",
    "text": "Recursos adicionales\n\nLectura: Yarkoni & Westfall. 2017. Choosing prediction over explanation in psychology: Lessons from machine learning\nVideo: ¿Que es una red neuronal?"
  },
  {
    "objectID": "programa.html#recursos-adicionales-6",
    "href": "programa.html#recursos-adicionales-6",
    "title": "Antes de empezar el curso",
    "section": "Recursos adicionales",
    "text": "Recursos adicionales\n\nLectura: Yarkoni & Westfall. 2017. Choosing prediction over explanation in psychology: Lessons from machine learning\nVideo: Clustering in Machine Learning"
  },
  {
    "objectID": "sobreajuste_y_entrenamiento.html",
    "href": "sobreajuste_y_entrenamiento.html",
    "title": "Sobreajuste y entrenamiento de modelos",
    "section": "",
    "text": "Entender el concepto de sobreajuste en modelos de aprendizaje estadístico\nAprender a detectar sobreajuste en modelos de aprendizaje estadístico\nAplicar métodos estadisticos para evitar el sobreajuste\nPaquetes a utilizar en este manual:\nCódigo\n# instalar/cargar paquetes\nsketchy::load_packages(\n  c(\"ggplot2\", \n    \"viridis\", \n    \"caret\",\n    \"MuMIn\",\n    \"nnet\"\n   )\n  )\nFuncion personalizada a utilizar en este manual:\nCódigo\nr2_lm &lt;- function(model, data, response = \"y\"){\n  pred &lt;- predict(model, newdata = data)\n  caret::R2(pred, data[, response])\n  }"
  },
  {
    "objectID": "sobreajuste_y_entrenamiento.html#sobreajuste",
    "href": "sobreajuste_y_entrenamiento.html#sobreajuste",
    "title": "Sobreajuste y entrenamiento de modelos",
    "section": "1.1 Sobreajuste",
    "text": "1.1 Sobreajuste\nEl sobreajuste ocurre cuando un modelo se ajusta demasiado bien a los datos de entrenamiento, capturando tanto los patrones verdaderos como el ruido o variaciones aleatorias de los datos. Como resultado, el modelo funciona bien en el conjunto de entrenamiento, pero tiene un rendimiento deficiente en nuevos datos (pobre capacidad de generalización). El sobreajuste se refiere a cuando modelo está tan ajustado a los datos de entrenamiento que afecta su capacidad de generalización. El sobreajuste se produce cuando un sistema de aprendizaje automático se entrena demasiado o con datos (levemente) sesgados, que hace que el algoritmo aprenda patrones que no son generales. Aprende características especificas pero no los patrones generales, el concepto.\nUna forma de evaluar la capacidad de generalización de un modelo es mediante la división de los datos en dos conjuntos: entrenamiento y prueba. El modelo se ajusta a los datos de entrenamiento y se evalúa en los datos de prueba. El sobreajuste se puede detectar cuando el error en los datos de prueba es mucho mayor que el error en los datos de entrenamiento.\nLos modelos más complejos tienden a sobreajustar más que lo modelos más simples. Además, ante un mismo modelo, a menor cantidad de datos es más posible que ese modelo se sobreajuste. Existen varios métodos para evaluar cuándo un modelo está sobreajustando. En la simulación que se muestra a continuación, se ajusta un modelo de regresión lineal con diferentes cantidades de predictores (p). Se calcula el error cuadrático medio en los datos de entrenamiento y en los datos de prueba.\n\n\nCódigo\nrepeticiones &lt;- 100 # Número de repeticiones\nn &lt;- 100  # Número de observaciones\np &lt;- 20  # Número de predictores\n\nexpr &lt;- expression({\n# Generar datos sintéticos\n\n\n# Crear variables predictoras aleatorias\ndatos &lt;- as.data.frame(matrix(rnorm(n * p), n, p))\ncolnames(datos) &lt;- paste0(\"x\", 1:p)\n\n# Crear variable de respuesta con una combinación de algunas variables\ndatos$y &lt;-\n  3 * datos$x1 - 2 * datos$x2 + 1 * datos$x3 + rnorm(n, 0, 2)\n\n# Dividir en conjunto de entrenamiento y prueba\nentren_indice &lt;- createDataPartition(datos$y, p = 0.9, list = FALSE)\ndatos_entren &lt;- datos[entren_indice, ]\ndatos_prueba &lt;- datos[-entren_indice, ]\n\n\nresultados_lista &lt;- lapply(1:(ncol(datos) - 1), function(z) {\n  # Ajustar modelo de regresión lineal\n  modelo &lt;-\n    lm(y ~ ., data = datos_entren[, c(\"y\", paste0(\"x\", 1:z))])\n  \n  r2_entren &lt;- r2_lm(modelo, datos_entren)\n  r2_prueba &lt;- r2_lm(modelo, datos_prueba)\n  \n  resultados &lt;-\n    data.frame(\n       r2 = c(r2_prueba, r2_entren)\n    )\n  \n  return(resultados)\n  \n})\n\nresultados_df &lt;- do.call(rbind, resultados_lista)\n})\n\n# repetir x veces\nreps &lt;- replicate(repeticiones, eval(expr), simplify = TRUE)\n\n# promediar resultados\nr2 &lt;- apply(as.data.frame(reps), 1, mean)\n\n\nresultados_promedio &lt;-\n    data.frame(\n      n_predictores = rep(1:p, each = 2),\n      Tipo = c(\"Prueba\", \"Entrenamiendo\"),\n      r2 = r2\n    )\n\nsaveRDS(resultados_promedio, \"resultados_promedio.rds\")\n\nggplot(resultados_promedio, aes(x = n_predictores, y = r2, color = Tipo)) +\n  geom_line(lwd = 2) +\n  scale_x_continuous(breaks = seq(0, p, 1)) +\n  scale_color_viridis_d(end = 0.9) +\n  labs(x = \"Número de predictores\",\n       y = bquote('Coeficiente de determinación' ~ R ^ 2)) +\n  theme(\n    legend.background = element_rect(fill = \"#fff3cd\"),\n    legend.position = c(0.5, 0.2),\n    panel.background = element_rect(fill = \"#fff3cd\"),\n    plot.background = element_rect(fill = \"#fff3cd\", colour = NA)\n  )    \n\n\n\n\n\n\n\n\n\n\n\nPodemos ver como en ambos casos el coeficiente de determinación (R2) aumenta en los primeros 3 predictores. Esto es de esperar ya que estos son los predictores asociados a la respuesta. Sin embargo, luego de este punto el R2 aumenta para los datos de entrenamiento, pero no para los datos de prueba. Esto es un claro indicio de sobreajuste.\nEn la siguiente simulación podemos ver mas claramente como el R2 calculado sobre datos de entrenamiento aumenta con la cantidad de predictores, a pesar de no haber un solo predictor asociado a la variable respuesta:\n\n\nCódigo\nrepeticiones &lt;- 100 # Número de repeticiones\nn &lt;- 100  # Número de observaciones\np &lt;- 20  # Número de predictores\n\nexpr &lt;- expression({\n# Generar datos sintéticos\n\n\n# Crear variables predictoras aleatorias\ndatos &lt;- as.data.frame(matrix(rnorm(n * p), n, p))\ncolnames(datos) &lt;- paste0(\"x\", 1:p)\n\n# Crear variable de respuesta con una combinación de algunas variables\ndatos$y &lt;- rnorm(n, 0, 2)\n\n# Dividir en conjunto de entrenamiento y prueba\nentren_indice &lt;- createDataPartition(datos$y, p = 0.9, list = FALSE)\ndatos_entren &lt;- datos[entren_indice, ]\ndatos_prueba &lt;- datos[-entren_indice, ]\n\n\nresultados_lista &lt;- lapply(1:(ncol(datos) - 1), function(z) {\n  # Ajustar modelo de regresión lineal\n  modelo &lt;-\n    lm(y ~ ., data = datos_entren[, c(\"y\", paste0(\"x\", 1:z))])\n  \n  r2_entren &lt;- r2_lm(modelo, datos_entren)\n  r2_prueba &lt;- r2_lm(modelo, datos_prueba)\n  \n  resultados &lt;-\n    data.frame(\n       r2 = c(r2_prueba, r2_entren)\n    )\n  \n  return(resultados)\n  \n})\n\nresultados_df &lt;- do.call(rbind, resultados_lista)\n})\n\n# repetir x veces\nreps &lt;- replicate(repeticiones, eval(expr), simplify = TRUE)\n\n# promediar resultados\nr2 &lt;- apply(as.data.frame(reps), 1, mean)\n\n\nresultados_promedio_aleatorio &lt;-\n    data.frame(\n      n_predictores = rep(1:p, each = 2),\n      Tipo = c(\"Prueba\", \"Entrenamiendo\"),\n      r2 = r2\n    )\n\nsaveRDS(resultados_promedio_aleatorio, \"resultados_promedio_aleatorio.rds\")\n\nggplot(resultados_promedio_aleatorio, aes(x = n_predictores, y = r2, color = Tipo)) +\n  geom_line(lwd = 2) +\n  scale_x_continuous(breaks = seq(0, p, 1)) +\n  scale_color_viridis_d(end = 0.9) +\n  labs(x = \"Número de predictores\",\n       y = bquote('Coeficiente de determinación' ~ R ^ 2)) +\n  theme(\n    legend.background = element_rect(fill = \"#fff3cd\"),\n    legend.position = c(0.7, 0.2),\n    panel.background = element_rect(fill = \"#fff3cd\"),\n    plot.background = element_rect(fill = \"#fff3cd\", colour = NA)\n  )    \n\n\n\n\n\n\n\n\n\n\n\nSin embargo, el R2 calculado sobre datos de prueba se mantiene constante."
  },
  {
    "objectID": "sobreajuste_y_entrenamiento.html#selección-de-modelos-con-aic",
    "href": "sobreajuste_y_entrenamiento.html#selección-de-modelos-con-aic",
    "title": "Sobreajuste y entrenamiento de modelos",
    "section": "2.1 Selección de modelos con AIC",
    "text": "2.1 Selección de modelos con AIC\nEl criterio de información de Akaike (AIC) es una medida de la calidad relativa de un modelo estadístico para un conjunto dado de datos. EL AIC penaliza la complejidad del modelo, lo que ayuda a evitar el sobreajuste. El AIC se calcula como:\n\\[AIC = -2 * log(L) + 2 * k\\]\nL es la función de verosimilitud del modelo y k es el número de parámetros del modelo.\nEl AIC proporciona una medida relativa de la calidad de cada modelo, en relación con los otros modelos. Por lo tanto, el AIC se puede utilizar para seleccionar el mejor modelo de un conjunto de modelos candidatos. Un AIC más bajo indica un mejor modelo. En el contexto de selección de modelos el modelo preferido sería el que tiene el menor AIC. Una diferencia de AIC de 2 o más unidades se considera evidencia fuerte a favor del modelo con el AIC más bajo.\nPodemos ver como el AIC aumenta con la cantidad de predictores en el modelo. En el siguiente gráfico se muestra el AIC para un modelo de regresión lineal con diferentes cantidades de predictores:\n\n\nCódigo\nrepeticiones &lt;- 100 # Número de repeticiones\nn &lt;- 100  # Número de observaciones\np &lt;- 20  # Número de predictores\n\nexpr &lt;- expression({\n# Generar datos sintéticos\n\n\n# Crear variables predictoras aleatorias\ndatos &lt;- as.data.frame(matrix(rnorm(n * p), n, p))\ncolnames(datos) &lt;- paste0(\"x\", 1:p)\n\n\n# Crear variable de respuesta con una combinación de algunas variables\ndatos$y &lt;-\n  3 * datos$x1 - 2 * datos$x2 + 1 * datos$x3 + rnorm(n, 0, 2)\n\n# Dividir en conjunto de entrenamiento y prueba\nentren_indice &lt;- createDataPartition(datos$y, p = 0.9, list = FALSE)\ndatos_entren &lt;- datos[entren_indice, ]\ndatos_prueba &lt;- datos[-entren_indice, ]\n\n\nresultados_lista &lt;- lapply(1:(ncol(datos) - 1), function(z) {\n  # Ajustar modelo de regresión lineal\n   modelo &lt;-\n    lm(y ~ ., data = datos[, c(\"y\", paste0(\"x\", 1:z))])\n  \n  # modelos nulos\n  modelo_nulo &lt;- lm(y ~ 1, data = datos[, c(\"y\", paste0(\"x\", 1:z))])\n  \n  # Calcular AIC\n  aics &lt;- AIC(modelo, modelo_nulo)\n  aics$delta.aic &lt;- aics$AIC - min(aics$AIC)\n  \n  resultados &lt;- aics[\"modelo_nulo\", \"delta.aic\"]\n  \n  return(resultados)\n  \n})\n\nresultados_df &lt;- do.call(rbind, resultados_lista)\n})\n\n# repetir x veces\nreps &lt;- replicate(repeticiones, eval(expr), simplify = TRUE)\n\n# promediar resultados\ndelta_aic &lt;- apply(as.data.frame(reps), 1, mean)\n\n\nresultados_promedio_aic &lt;-\n    data.frame(\n      n_predictores = rep(1:p),\n      delta_aic = delta_aic\n    )\n\nggplot(resultados_promedio_aic, aes(x = n_predictores, y = delta_aic)) +\n  geom_line(lwd = 2, color = viridis(1)) +\n  scale_x_continuous(breaks = seq(0, p, 1)) +\n  labs(x = \"Número de predictores\", y = \"Delta AIC\") +\n  theme(legend.position = c(0.5, 0.7)) +\n  scale_y_reverse()\n\n\n\n\n\n\n\n\n\nDatos donde ningun predictor está asociado:\n\n\nCódigo\nrepeticiones &lt;- 100 # Número de repeticiones\nn &lt;- 100  # Número de observaciones\np &lt;- 20  # Número de predictores\n\nexpr &lt;- expression({\n# Generar datos sintéticos\n\n# Crear variables predictoras aleatorias\ndatos &lt;- as.data.frame(matrix(rnorm(n * p), n, p))\ncolnames(datos) &lt;- paste0(\"x\", 1:p)\n\n\n# Crear variable de respuesta no asociada a los predictores\ndatos$y &lt;- rnorm(n, 0, 2)\n\nresultados_lista &lt;- lapply(1:(ncol(datos) - 1), function(z) {\n  # Ajustar modelo de regresión lineal\n   modelo &lt;-\n    lm(y ~ ., data = datos[, c(\"y\", paste0(\"x\", 1:z))])\n  \n  # modelos nulos\n  modelo_nulo &lt;- lm(y ~ 1, data = datos[, c(\"y\", paste0(\"x\", 1:z))])\n  \n  # Calcular AIC\n  aics &lt;- AIC(modelo, modelo_nulo)\n  aics$delta.aic &lt;- aics$AIC - min(aics$AIC)\n  \n  resultados &lt;- aics[\"modelo_nulo\", \"delta.aic\"]\n  \n  return(resultados)\n  \n})\n\nresultados_df &lt;- do.call(rbind, resultados_lista)\n})\n\n# repetir x veces\nreps &lt;- replicate(repeticiones, eval(expr), simplify = TRUE)\n\n# promediar resultados\ndelta_aic &lt;- apply(as.data.frame(reps), 1, mean)\n\n\nresultados_promedio_aic &lt;-\n    data.frame(\n      n_predictores = rep(1:p),\n      delta_aic = delta_aic\n    )\n\nggplot(resultados_promedio_aic, aes(x = n_predictores, y = delta_aic)) +\n  geom_line(lwd = 2, color = viridis(1)) + \n  scale_x_continuous(breaks = seq(0, p, 1)) +\n  labs(x = \"Número de predictores\", y = \"Delta AIC\") +\n  theme(legend.position = c(0.5, 0.7)) +\n  scale_y_reverse(limits = c(1, -2))"
  },
  {
    "objectID": "sobreajuste_y_entrenamiento.html#aplicación-del-aic",
    "href": "sobreajuste_y_entrenamiento.html#aplicación-del-aic",
    "title": "Sobreajuste y entrenamiento de modelos",
    "section": "2.2 Aplicación del AIC",
    "text": "2.2 Aplicación del AIC\nComo se mencionó anteriormente, el AIC se puede utilizar para seleccionar el mejor modelo de un conjunto de modelos candidatos. El modelo preferido es el que tiene el menor AIC. Por ejemplo, podemos construir un conjunto de modelos de regresión lineal con diferentes predictores y seleccionar el modelo con el menor AIC. En el siguiente ejemplo, se ajustan modelos de regresión lineal a los datos Titanic con diferentes predictores y comparemos el ajuste relativo de los modelos con AIC:\n\n\nCódigo\n# cargar datos\ndata(\"Titanic\")\n\n# dar formato con una observacion por fila\ndatos_titanic &lt;- as.data.frame(Titanic)\ndatos_titanic &lt;- datos_titanic[datos_titanic$Freq &gt; 0, ]\n\ndatos_tab_titanic &lt;- do.call(rbind, lapply(1:nrow(datos_titanic), function(x) datos_titanic[rep(x, datos_titanic$Freq[x]),]))\ndatos_tab_titanic$Freq &lt;- NULL\n\n# explorar datos\nhead(datos_tab_titanic, 10)\n\n\n\n\n\n\n\nClass\nSex\nAge\nSurvived\n\n\n\n\n3\n3rd\nMale\nChild\nNo\n\n\n3.1\n3rd\nMale\nChild\nNo\n\n\n3.2\n3rd\nMale\nChild\nNo\n\n\n3.3\n3rd\nMale\nChild\nNo\n\n\n3.4\n3rd\nMale\nChild\nNo\n\n\n3.5\n3rd\nMale\nChild\nNo\n\n\n3.6\n3rd\nMale\nChild\nNo\n\n\n3.7\n3rd\nMale\nChild\nNo\n\n\n3.8\n3rd\nMale\nChild\nNo\n\n\n3.9\n3rd\nMale\nChild\nNo\n\n\n\n\n\n\nCódigo\n# correr un modelo con clase como predictor\nmodelo_clase &lt;- glm(Survived ~ Class, data = datos_tab_titanic, family = binomial)\n\n# correr un modelo con edad como predictor\nmodelo_edad &lt;- glm(Survived ~ Age, data = datos_tab_titanic, family = binomial)\n\ntabla_aic &lt;- AIC(modelo_clase, modelo_edad)\n\ntabla_aic\n\n\n\n\n\n\n\ndf\nAIC\n\n\n\n\nmodelo_clase\n4\n2596.6\n\n\nmodelo_edad\n2\n2753.9\n\n\n\n\n\n\nEn este caso, el modelo con la clase como predictor tiene un AIC más bajo que el modelo con la edad como predictor, lo que sugiere que el modelo con la clase como predictor es preferido. Resulta mas intuitivo comparar los modelos utilizando la diferencia en AIC (comúnmente llamada “delta AIC”), ya que esta diferencia nos da una idea de cuan mejor es un modelo con respecto a otro:\n\n\nCódigo\n# calcular delta AIC\ntabla_aic$delta.aic &lt;- tabla_aic$AIC - min(tabla_aic$AIC)\n\ntabla_aic\n\n\n\n\n\n\n\ndf\nAIC\ndelta.aic\n\n\n\n\nmodelo_clase\n4\n2596.6\n0.00\n\n\nmodelo_edad\n2\n2753.9\n157.34\n\n\n\n\n\n\nEl modelo con la clase como predictor tiene un delta AIC de 0. Por lo tanto este es el mejor modelo. También podemos ver que la diferencia con el modelo de edad es de 157.34, lo que indica que el modelo con la clase como predictor es significativamente mejor que el modelo con la edad como predictor.\nEn algunos casos donde el número de posibles modelos es alto resulta útil utilizar la funcion dredge del paquete MuMIn para calcular el AIC de todos los modelos posibles y seleccionar el mejor modelo.\n\n\nCódigo\ndatos_tab_titanic$aleat &lt;- rnorm(n = nrow(datos_tab_titanic))\n\n# crear modelo global\nmodelo_global &lt;- glm(Survived ~ ., data = datos_tab_titanic, family = binomial)\n\n# cambiar comportamiento en presencia de NAs (para evitar problemas con la funcion dredge)\noptions(na.action = \"na.fail\")\n\n# usar la funcion dredge\ndredge(modelo_global)\n\n\nFixed term is \"(Intercept)\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Intercept)\nAge\naleat\nClass\nSex\ndf\nlogLik\nAICc\ndelta\nweight\n\n\n\n\n14\n0.68532\n+\nNA\n+\n+\n6\n-1105.0\n2222.1\n0.0000\n7.2280e-01\n\n\n16\n0.68505\n+\n0.01649\n+\n+\n7\n-1105.0\n2224.0\n1.9184\n2.7697e-01\n\n\n13\n-0.35312\nNA\nNA\n+\n+\n5\n-1114.5\n2238.9\n16.8408\n1.5926e-04\n\n\n15\n-0.35249\nNA\n0.01848\n+\n+\n6\n-1114.4\n2240.8\n18.7318\n6.1866e-05\n\n\n10\n-0.78004\n+\nNA\nNA\n+\n3\n-1164.5\n2335.1\n113.0065\n2.0892e-25\n\n\n12\n-0.78093\n+\n0.01885\nNA\n+\n4\n-1164.5\n2337.0\n114.8819\n8.1797e-26\n\n\n9\n-1.31281\nNA\nNA\nNA\n+\n2\n-1167.5\n2339.0\n116.8940\n2.9910e-26\n\n\n11\n-1.31297\nNA\n0.01980\nNA\n+\n3\n-1167.4\n2340.9\n118.7532\n1.1806e-26\n\n\n6\n1.56519\n+\nNA\n+\nNA\n5\n-1281.5\n2573.0\n350.9003\n4.5919e-77\n\n\n8\n1.56518\n+\n0.00979\n+\nNA\n6\n-1281.5\n2575.0\n352.8706\n1.7145e-77\n\n\n5\n0.50918\nNA\nNA\n+\nNA\n4\n-1294.3\n2596.6\n374.4742\n3.4913e-82\n\n\n7\n0.50968\nNA\n0.01216\n+\nNA\n5\n-1294.2\n2598.5\n376.4201\n1.3196e-82\n\n\n2\n0.09181\n+\nNA\nNA\nNA\n2\n-1374.9\n2753.9\n531.8022\n2.3969e-116\n\n\n4\n0.09177\n+\n0.00183\nNA\nNA\n3\n-1374.9\n2755.9\n533.8061\n8.8003e-117\n\n\n1\n-0.73986\nNA\nNA\nNA\nNA\n1\n-1384.7\n2771.5\n549.3592\n3.6914e-120\n\n\n3\n-0.73985\nNA\n0.00312\nNA\nNA\n2\n-1384.7\n2773.5\n551.3582\n1.3586e-120\n\n\n\n\n\n\nEsta función usa el AICc para estimar el ajuste del modelo. EL AICc es una versión corregida del AIC para muestras pequeñas. La función calcula el AICc de todos los modelos posibles y selecciona el mejor modelo. Las cruces en las columnas con los nombres de los predictores indican si el predictor está presente en el modelo. En este ejemplo, el modelo con los 3 predictores es el mejor modelo.\nEs importante destacar que el AIC no está definido para ciertos tipos de modelos, lo que imposibilita su cálculo en esas circunstancias. En tales casos, se pueden emplear otros criterios de información, como el BIC o el DIC (ver más adelante). Sin embargo, para modelos más complejos en el ámbito del aprendizaje estadístico, la estructura intrincada de estos modelos a menudo impide la aplicación de criterios de información tradicionales. En estas situaciones, los métodos de selección de modelos basados en la validación cruzada se convierten en una herramienta fundamental, ya que permiten evaluar el desempeño del modelo utilizando particiones repetidas del conjunto de datos."
  },
  {
    "objectID": "sobreajuste_y_entrenamiento.html#criterios-de-información-adicionales",
    "href": "sobreajuste_y_entrenamiento.html#criterios-de-información-adicionales",
    "title": "Sobreajuste y entrenamiento de modelos",
    "section": "2.3 Criterios de información adicionales",
    "text": "2.3 Criterios de información adicionales\nEl AIC es solo uno de varios criterios de información utilizados para evaluar modelos estadísticos. Otros criterios comunes incluyen el Criterio de Información Bayesiano (BIC), el Criterio de Información de Deviance (DIC) y el DIC (una versión corregida del AIC para muestras pequeñas). Aunque cada criterio tiene su propia formulación y énfasis (por ejemplo, el BIC penaliza más fuertemente los modelos con mayor complejidad), todos comparten el objetivo de balancear la calidad de ajuste del modelo con su complejidad para evitar el sobreajuste. Estos criterios pueden ser aplicados de manera similar al AIC y seleccionados según el contexto y los objetivos del análisis. Por ejemplo, el BIC es más conservador en términos de selección de predictores, lo que puede ser útil cuando se desea un modelo más parsimonioso.\n\nEl uso de ‘dredge’ para la selección de variables debería ser evitado cuando hay prueba de hipótesis. En lugar de esto, se recomienda el uso de modelos causales por medio de grafos dirigidos acíclicos (DAGs: Directed Acyclic Graphs) para seleccionar las variables, ya que proporcionan un enfoque más sólido y coherente con la inferencia causal."
  },
  {
    "objectID": "sobreajuste_y_entrenamiento.html#ejercicio-1",
    "href": "sobreajuste_y_entrenamiento.html#ejercicio-1",
    "title": "Sobreajuste y entrenamiento de modelos",
    "section": "2.4 Ejercicio 1",
    "text": "2.4 Ejercicio 1\n\nAjusta un modelo de regresión lineal global con los datos de mtcars donde la variable respuesta sea mpg.\nUtilice la función dredge para seleccionar el mejor modelo."
  },
  {
    "objectID": "sobreajuste_y_entrenamiento.html#validación-simple",
    "href": "sobreajuste_y_entrenamiento.html#validación-simple",
    "title": "Sobreajuste y entrenamiento de modelos",
    "section": "3.1 Validación simple",
    "text": "3.1 Validación simple\nEn su forma mas simple la división de los datos produce un único conjunto de entrenamiento y un único conjunto de prueba. Se conoce en inglés como “hold-out validation”. Aunque no es realmente una forma de validación cruzada en el sentido técnico, es una estrategia ampliamente utilizada para evaluar el desempeño de modelos en situaciones simples o preliminares y es útil para introducir ejemplificar el uso de datos de entrenamiento y datos de ejemplo. Esta estrategia consiste en dividir el conjunto de datos en dos partes:\n\nConjunto de entrenamiento: Se utiliza para ajustar (entrenar) el modelo.\nConjunto de prueba: Se utiliza para evaluar el modelo ajustado.\n\n\n\nCódigo\n# Configurar el conjunto de datos\nset.seed(123)\ndata(mtcars)\nmtcars$am &lt;- as.factor(mtcars$am) # Convertir la variable am a factor para clasificación\n\n# Dividir los datos en entrenamiento (80%) y prueba (20%)\nindice_entrenamiento &lt;- caret::createDataPartition(mtcars$am, p = 0.8, list = FALSE)\ndatos_entrenamiento &lt;- mtcars[indice_entrenamiento, ]\ndatos_prueba &lt;- mtcars[-indice_entrenamiento, ]\n\n# Ajustar un modelo de regresión logística usando los datos de entrenamiento\nmodel &lt;- caret::train(\n  am ~ hp + wt,\n  data = datos_entrenamiento,\n  method = \"glm\",\n  family = \"binomial\"\n)\n\n# Resumen del modelo ajustado\nprint(model)\n\n\nGeneralized Linear Model \n\n27 samples\n 2 predictor\n 2 classes: '0', '1' \n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 27, 27, 27, 27, 27, 27, ... \nResampling results:\n\n  Accuracy  Kappa  \n  0.90663   0.79597\n\n\nCódigo\n# Hacer predicciones en el conjunto de prueba\npredictions &lt;- predict(model, newdata = datos_prueba)\n\n# Evaluar el desempeño del modelo\nconfusion_matrix &lt;- caret::confusionMatrix(predictions, datos_prueba$am)\n\n# Mostrar los resultados\nprint(confusion_matrix)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction 0 1\n         0 3 0\n         1 0 2\n                                    \n               Accuracy : 1         \n                 95% CI : (0.478, 1)\n    No Information Rate : 0.6       \n    P-Value [Acc &gt; NIR] : 0.0778    \n                                    \n                  Kappa : 1         \n                                    \n Mcnemar's Test P-Value : NA        \n                                    \n            Sensitivity : 1.0       \n            Specificity : 1.0       \n         Pos Pred Value : 1.0       \n         Neg Pred Value : 1.0       \n             Prevalence : 0.6       \n         Detection Rate : 0.6       \n   Detection Prevalence : 0.6       \n      Balanced Accuracy : 1.0       \n                                    \n       'Positive' Class : 0"
  },
  {
    "objectID": "sobreajuste_y_entrenamiento.html#validación-cruzada-simple-k-fold",
    "href": "sobreajuste_y_entrenamiento.html#validación-cruzada-simple-k-fold",
    "title": "Sobreajuste y entrenamiento de modelos",
    "section": "3.2 Validación cruzada simple (k-fold)",
    "text": "3.2 Validación cruzada simple (k-fold)\nLa validación cruzada simple divide el conjunto de datos en k subconjuntos y se entrena el modelo k veces, usando cada uno de los subconjuntos como conjunto de prueba una vez. Esta técnica es comúnmente utilizada para evaluar modelos de aprendizaje estadístico. La validación cruzada simple proporciona una estimación más precisa del rendimiento del modelo en nuevos datos que la validación simple, ya que utiliza todos los datos para ajustar el modelo y evaluar su rendimiento. La validación cruzada simple es particularmente útil cuando se dispone de un número limitado de datos, ya que permite utilizar todos los datos para ajustar el modelo y evaluar su rendimiento. Podemos ver un ejemplo de validación cruzada simple en el siguiente código. La función train del paquete caret ajusta el modelo y evalúa su rendimiento utilizando validación cruzada simple:\n\n\nCódigo\n# Configurar validación cruzada simple con 5 folds\ncontrol &lt;- trainControl(method = \"cv\", number = 5)\n\n# Ajustar modelo usando validación cruzada\nset.seed(123)\nmodelo &lt;- train(\n  mpg ~ wt + hp,\n  data = mtcars,\n  method = \"lm\",  # Modelo de regresión lineal\n  trControl = control\n)\n\n# Resultados del modelo\nprint(modelo)\n\n\nLinear Regression \n\n32 samples\n 2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 26, 25, 25, 26, 26 \nResampling results:\n\n  RMSE    Rsquared  MAE  \n  2.7007  0.84844   2.144\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\nEl argumento “number” controla el numero de subconjuntos (k) en los que se divide el conjunto de datos. En este caso, se utiliza un valor de 5. El modelo se ajusta 5 veces, utilizando cada uno de los subconjuntos como conjunto de prueba una vez.\n\n\nCódigo\nmodelo$resample\n\n\n\n\n\n\nRMSE\nRsquared\nMAE\nResample\n\n\n\n\n1.8112\n0.93248\n1.4489\nFold1\n\n\n2.9085\n0.88885\n2.2319\nFold2\n\n\n2.4782\n0.72856\n2.0412\nFold3\n\n\n3.2604\n0.84493\n2.8800\nFold4\n\n\n3.0452\n0.84737\n2.1181\nFold5\n\n\n\n\n\n\n\n\nCódigo\nmodelo$results\n\n\n\n\n\n\nintercept\nRMSE\nRsquared\nMAE\nRMSESD\nRsquaredSD\nMAESD\n\n\n\n\nTRUE\n2.7007\n0.84844\n2.144\n0.57356\n0.07596\n0.51082\n\n\n\n\n\n\nNote que el resultado del código resume los datos calculando un promedio de las métricas de desempeño en los diferentes subconjuntos:\n\n\nCódigo\nmean(modelo$resample$RMSE)\n\n\n[1] 2.7007\n\n\nCódigo\nmean(modelo$resample$Rsquared)\n\n\n[1] 0.84844\n\n\nTambién podemos extraer un modelo final. En este caso el modelo final es un modelo ajustado en la totalidad de los datos:\n\n\nCódigo\nsummary(modelo$finalModel)\n\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879   23.28  &lt; 2e-16 ***\nwt          -3.87783    0.63273   -6.13  1.1e-06 ***\nhp          -0.03177    0.00903   -3.52   0.0015 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.59 on 29 degrees of freedom\nMultiple R-squared:  0.827, Adjusted R-squared:  0.815 \nF-statistic: 69.2 on 2 and 29 DF,  p-value: 9.11e-12"
  },
  {
    "objectID": "sobreajuste_y_entrenamiento.html#dejar-uno-fuera-leave-one-out-cross-validation-loocv",
    "href": "sobreajuste_y_entrenamiento.html#dejar-uno-fuera-leave-one-out-cross-validation-loocv",
    "title": "Sobreajuste y entrenamiento de modelos",
    "section": "3.3 Dejar uno fuera (Leave-One-Out Cross-Validation, LOOCV)",
    "text": "3.3 Dejar uno fuera (Leave-One-Out Cross-Validation, LOOCV)\nCada observación es utilizada como conjunto de prueba una vez. Este enfoque es adecuado para conjuntos de datos pequeños, pero computacionalmente costoso para grandes conjuntos de datos.\n\n\nCódigo\n# Configurar LOOCV\ncontrol &lt;- trainControl(method = \"LOOCV\")\n\n# Ajustar modelo\nset.seed(123)\nmodelo &lt;- train(\n  mpg ~ wt + hp,\n  data = mtcars,\n  method = \"lm\",  # Modelo de regresión lineal\n  trControl = control\n)\n\n# Resultados del modelo\nprint(modelo)\n\n\nLinear Regression \n\n32 samples\n 2 predictor\n\nNo pre-processing\nResampling: Leave-One-Out Cross-Validation \nSummary of sample sizes: 31, 31, 31, 31, 31, 31, ... \nResampling results:\n\n  RMSE    Rsquared  MAE   \n  2.7755  0.78303   2.1234\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\nEste método no devuelve los valores individuales por iteración, pero podemos obtener el modelo final ajustado:\n\n\nCódigo\nsummary(modelo$finalModel)\n\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879   23.28  &lt; 2e-16 ***\nwt          -3.87783    0.63273   -6.13  1.1e-06 ***\nhp          -0.03177    0.00903   -3.52   0.0015 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.59 on 29 degrees of freedom\nMultiple R-squared:  0.827, Adjusted R-squared:  0.815 \nF-statistic: 69.2 on 2 and 29 DF,  p-value: 9.11e-12"
  },
  {
    "objectID": "sobreajuste_y_entrenamiento.html#validación-cruzada-repetida",
    "href": "sobreajuste_y_entrenamiento.html#validación-cruzada-repetida",
    "title": "Sobreajuste y entrenamiento de modelos",
    "section": "3.4 Validación cruzada repetida",
    "text": "3.4 Validación cruzada repetida\nEn este enfoque, la validación cruzada simple se repite varias veces para obtener una evaluación más robusta del modelo.\n\n\nCódigo\n# Configurar validación cruzada repetida (5 folds, 3 repeticiones)\ncontrol &lt;- trainControl(method = \"repeatedcv\", number = 5, repeats = 3)\n\n# Ajustar modelo\nset.seed(123)\nmodelo &lt;- train(\n  mpg ~ wt + hp,\n  data = mtcars,\n  method = \"lm\",  # Modelo de regresión lineal\n  trControl = control\n)\n\n# Resultados del modelo\nprint(modelo)\n\n\nLinear Regression \n\n32 samples\n 2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold, repeated 3 times) \nSummary of sample sizes: 26, 25, 25, 26, 26, 25, ... \nResampling results:\n\n  RMSE    Rsquared  MAE   \n  2.6093  0.85947   2.1656\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\nCódigo\nmodelo$resample\n\n\n\n\n\n\nRMSE\nRsquared\nMAE\nResample\n\n\n\n\n1.8112\n0.93248\n1.44890\nFold1.Rep1\n\n\n2.9085\n0.88885\n2.23185\nFold2.Rep1\n\n\n2.4782\n0.72856\n2.04122\nFold3.Rep1\n\n\n3.2604\n0.84493\n2.87996\nFold4.Rep1\n\n\n3.0452\n0.84737\n2.11810\nFold5.Rep1\n\n\n3.7273\n0.70065\n2.91640\nFold1.Rep2\n\n\n1.4267\n0.94729\n1.31007\nFold2.Rep2\n\n\n0.8549\n0.98663\n0.66527\nFold3.Rep2\n\n\n3.8493\n0.90974\n3.49789\nFold4.Rep2\n\n\n2.7973\n0.75968\n2.42463\nFold5.Rep2\n\n\n4.0172\n0.81159\n3.39130\nFold1.Rep3\n\n\n1.9829\n0.95206\n1.63557\nFold2.Rep3\n\n\n1.8503\n0.91850\n1.54283\nFold3.Rep3\n\n\n3.2477\n0.74280\n2.63592\nFold4.Rep3\n\n\n1.8819\n0.92096\n1.74432\nFold5.Rep3"
  },
  {
    "objectID": "sobreajuste_y_entrenamiento.html#validación-de-remuestreo-con-reemplazo-bootstrap",
    "href": "sobreajuste_y_entrenamiento.html#validación-de-remuestreo-con-reemplazo-bootstrap",
    "title": "Sobreajuste y entrenamiento de modelos",
    "section": "3.5 Validación de remuestreo con reemplazo (“bootstrap”)",
    "text": "3.5 Validación de remuestreo con reemplazo (“bootstrap”)\nEl remuestreo bootstrap genera múltiples muestras con reemplazo para entrenar el modelo y estimar su desempeño.\n\n\nCódigo\n# Configurar remuestreo bootstrap\ncontrol &lt;- trainControl(method = \"boot\", number = 10)\n\n# Ajustar modelo\nset.seed(123)\nmodelo &lt;- train(\n  mpg ~ wt + hp,\n  data = mtcars,\n  method = \"lm\",  # Modelo de regresión lineal\n  trControl = control\n)\n\n# Resultados del modelo\nprint(modelo)\n\n\nLinear Regression \n\n32 samples\n 2 predictor\n\nNo pre-processing\nResampling: Bootstrapped (10 reps) \nSummary of sample sizes: 32, 32, 32, 32, 32, 32, ... \nResampling results:\n\n  RMSE    Rsquared  MAE   \n  2.9646  0.74983   2.3132\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\nCódigo\nmodelo$resample\n\n\n\n\n\n\nRMSE\nRsquared\nMAE\nResample\n\n\n\n\n2.7283\n0.61356\n2.2136\nResample01\n\n\n2.8014\n0.57724\n2.2220\nResample02\n\n\n2.7525\n0.86627\n2.0515\nResample03\n\n\n2.9532\n0.72869\n2.2321\nResample04\n\n\n2.7605\n0.89815\n2.0242\nResample05\n\n\n2.7978\n0.75217\n2.3469\nResample06\n\n\n3.8496\n0.77098\n3.0208\nResample07\n\n\n3.0133\n0.79124\n2.4657\nResample08\n\n\n2.7202\n0.72953\n1.9825\nResample09\n\n\n3.2696\n0.77051\n2.5720\nResample10"
  },
  {
    "objectID": "sobreajuste_y_entrenamiento.html#validación-cruzada-estratificada",
    "href": "sobreajuste_y_entrenamiento.html#validación-cruzada-estratificada",
    "title": "Sobreajuste y entrenamiento de modelos",
    "section": "3.6 Validación cruzada estratificada",
    "text": "3.6 Validación cruzada estratificada\nEn problemas de clasificación, la distribución de las clases puede ser desbalanceada. La validación cruzada estratificada garantiza que cada subconjunto tenga una proporción similar de clases, mejorando la estabilidad de las métricas.\nHay dos formas basicas en las que se puede balancear los datos durante las iteraciones de la validación cruzada. La primera (muestrear hacia arriba o “up-sampling”) es balancear forzando que todas las clases tengan el mismo numero de observaciones que la clase mas frecuente. tenga el mismo numero de observaciones. La segunda es balancear “hacia abajo” (down-sampling) forzando todas las clases a tener el mismo numero de observaciones que la clase menos frecuente.\nPara muestrear hacia abajo, simplemente definimos el argumento sampling = \"up\" en la función trainControl:\n\n\nCódigo\n# Configurar validación cruzada con 10 folds\ncontrol &lt;- caret::trainControl(\n  method = \"cv\",\n  number = 10,\n  classProbs = TRUE,\n  sampling = \"up\"\n)\n\n# Ajustar modelo de clasificación\nmtcars$am_f &lt;- ifelse(mtcars$am == 1, \"manual\", \"automatico\")\n\n\nset.seed(123)\nmodelo &lt;- caret::train(\n  am_f ~ wt + hp,\n  data = mtcars,\n  method = \"glm\",\n  family = binomial,\n  trControl = control\n)\n\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nCódigo\n# Resultados del modelo\nprint(modelo)\n\n\nGeneralized Linear Model \n\n32 samples\n 2 predictor\n 2 classes: 'automatico', 'manual' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 29, 29, 28, 28, 28, 29, ... \nAddtional sampling using up-sampling\n\nResampling results:\n\n  Accuracy  Kappa\n  0.95      0.9  \n\n\nPodemos ver como la proporción de clases se mantiene constante en cada partición:\n\n\nCódigo\ntable(modelo$finalModel$data$.outcome)\n\n\n\nautomatico     manual \n        19         19 \n\n\nA pesar de que no estaba perfectamente balanceada en los datos originales:\n\n\nCódigo\ntable(mtcars$am_f)\n\n\n\nautomatico     manual \n        19         13 \n\n\nPara muestrear hacia abajo, definimos el argumento sampling = \"down\":\n\n\nCódigo\n# Configurar validación cruzada con 10 folds\ncontrol &lt;- caret::trainControl(\n  method = \"cv\",\n  number = 10,\n  classProbs = TRUE,\n  sampling = \"down\"\n)\n\n\nset.seed(123)\nmodelo &lt;- caret::train(\n  am_f ~ wt + hp,\n  data = mtcars,\n  method = \"glm\",\n  family = binomial,\n  trControl = control\n)\n\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nCódigo\n# Resultados del modelo\nprint(modelo)\n\n\nGeneralized Linear Model \n\n32 samples\n 2 predictor\n 2 classes: 'automatico', 'manual' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 29, 29, 28, 28, 28, 29, ... \nAddtional sampling using down-sampling\n\nResampling results:\n\n  Accuracy  Kappa\n  0.925     0.85 \n\n\nPodemos ver nuevamente como la proporción de clases se mantiene constante en cada partición, pero en este caso con el número de observaciones en la categoría menos numerosa:\n\n\nCódigo\ntable(modelo$finalModel$data$.outcome)\n\n\n\nautomatico     manual \n        13         13 \n\n\nTambien se han desarrollado metodos hibridos que combinan “up-sampling” y “down-sampling”. El método híbrido “smote” puede ser utilizado de esta forma:\nPara muestrear hacia abajo, definimos el argumento sampling = \"down\":\n\n\nCódigo\n# Configurar validación cruzada con 10 folds\ncontrol &lt;- caret::trainControl(\n  method = \"cv\",\n  number = 10,\n  classProbs = TRUE,\n  sampling = \"smote\"\n)\n\n\nset.seed(123)\nmodelo &lt;- caret::train(\n  am_f ~ wt + hp,\n  data = mtcars,\n  method = \"glm\",\n  family = binomial,\n  trControl = control\n)\n\n\nLoading required package: recipes\n\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nAttaching package: 'recipes'\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nCódigo\n# Resultados del modelo\nprint(modelo)\n\n\nGeneralized Linear Model \n\n32 samples\n 2 predictor\n 2 classes: 'automatico', 'manual' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 29, 29, 28, 28, 28, 29, ... \nAddtional sampling using SMOTE\n\nResampling results:\n\n  Accuracy  Kappa\n  0.95      0.9"
  },
  {
    "objectID": "sobreajuste_y_entrenamiento.html#resumen-de-las-técnicas-de-validación-cruzada",
    "href": "sobreajuste_y_entrenamiento.html#resumen-de-las-técnicas-de-validación-cruzada",
    "title": "Sobreajuste y entrenamiento de modelos",
    "section": "3.7 Resumen de las técnicas de validación cruzada",
    "text": "3.7 Resumen de las técnicas de validación cruzada\n\n\n\n\n\n\n\n\n\n\n\nMétodo\nDescripcion\nCodigo.de.caret\n\n\n\n\nValidación cruzada (k-fold)\nDivide el conjunto de datos en k partes y usa cada una de ellas como conjunto de prueba una vez.\ntrainControl(method = 'cv', number = 5)\n\n\nBalanceada\nSimilar a k-fold, pero mantiene las proporciones de clases en cada fold.\ntrainControl(method = 'cv', number = 10, classProbs = TRUE, sampling = 'smote')\n\n\nLOOCV (Leave-One-Out)\nUsa una observación como conjunto de prueba y las restantes como conjunto de entrenamiento.\ntrainControl(method = 'LOOCV')\n\n\nRepetida (repeatedcv)\nRepite k-fold varias veces para obtener una estimación más robusta del rendimiento del modelo.\ntrainControl(method = 'repeatedcv', number = 5, repeats = 3)\n\n\nBootstrap\nGenera múltiples muestras con reemplazo para entrenar el modelo y evaluar su desempeño.\ntrainControl(method = 'boot', number = 100)\n\n\n\n\n\n\nCada uno de estos métodos tiene sus ventajas y desventajas, y la elección del método dependerá del tamaño de los datos y de las características del problema (por ejemplo, si es un problema de clasificación o regresión).\n\n\n\n\n\n\n\n\n\n\n\nMétodo\nVentajas\nDesventajas\n\n\n\n\nValidación cruzada (k-fold)\nRápido y fácil de implementar.\nPuede variar según la partición.\n\n\nBalanceada\nMejora estabilidad en clasificación.\nSólo aplicable en clasificación.\n\n\nLOOCV (Leave-One-Out)\nUsa toda la información disponible.\nComputacionalmente costoso.\n\n\nRepetida (repeatedcv)\nMás robusto que k-fold.\nIncrementa el tiempo de cómputo.\n\n\nBootstrap\nEstimación robusta en datos limitados.\nSesgo si hay alta correlación en los datos."
  },
  {
    "objectID": "sobreajuste_y_entrenamiento.html#ejercicio-2",
    "href": "sobreajuste_y_entrenamiento.html#ejercicio-2",
    "title": "Sobreajuste y entrenamiento de modelos",
    "section": "3.8 Ejercicio 2",
    "text": "3.8 Ejercicio 2\n\nUtilice 2 de los métodos de validación cruzada mencionados anteriormente para evaluar el desempeño de un modelo de regresión logística en los datos mtcars.\n\nEl siguiente código simula un juego de datos donde solo los primeros 3 predictores están asociados a la variable respuesta.\n\n\nCódigo\nn &lt;- 100  # Número de observaciones\np &lt;- 20  # Número de predictores\n\n# Crear variables predictoras aleatorias\ndatos &lt;- as.data.frame(matrix(rnorm(n * p), n, p))\ncolnames(datos) &lt;- paste0(\"x\", 1:p)\n\n# Crear variable de respuesta con una combinación de algunas variables\ndatos$y &lt;-\n  3 * datos$x1 - 2 * datos$x2 + 1 * datos$x3 + rnorm(n, 0, 2)\n\n\n\nUtilice la función ‘dredge’ para seleccionar los predictores del modelo de regresión lineal para estos datos.\nUtilice la función ‘dredge’ sobre un modelo de regresión lineal utilizando los datos de mtcars donde la variable respuesta sea mpg (millas por galón).\nUna vez seleccionados los predictores, ajuste el modelo y evalúe su desempeño utilizando validación cruzada."
  },
  {
    "objectID": "instructor.html",
    "href": "instructor.html",
    "title": "Aprendizaje Estadístico, SEP-UCR 2024",
    "section": "",
    "text": "Marcelo Araya-Salas\n\n\n\n\n\nUn ecólogo del comportamiento mas que acabó haciendo mucha programación solo para poder hacer bien el análisis de datos. Ademas de la investigación, también trabajo en hacer herramientas computacionales para el análisis de datos biológicos (no genéticos), principalmente relacionados con el comportamiento animal y la bioacústica. Soy el desarrollador de los paquetes de R warbleR y Rraven los cuales tienen funciones para agilizar el análisis acústico de sonidos de los animales. Más recientemente, sacamos los paquetes baRulho, para cuantificar la transmisión y la degradación de la señal acústica, ohun, para optimizar la detección automática y PhenotypeSpace para cuantificar los espacios de rasgos multidimensionales. Recientemente también desarrollé el paquete sketchy para organizar compendios de investigación."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aprendizaje Estadístico, SEP-UCR 2024",
    "section": "",
    "text": "Sistema de Estudios de Posgrado\n\n\nUniversidad de Costa Rica\n\n\nII semestre, 2024\n\n\n\nMarcelo Araya-Salas, PhD\n\n\nEl aprendizaje estadístico (“machine learning”) es una rama de la inteligencia artificial que se enfoca en el desarrollo de algoritmos y modelos que permiten a las computadoras aprender patrones y hacer predicciones para tomar decisiones basadas en datos. En lugar de seguir instrucciones explícitas para realizar una tarea, estos algoritmos identifican patrones en los datos y usan estos patrones para mejorar su desempeño en tareas específicas. El aprendizaje estadístico se utiliza en una amplia variedad de aplicaciones, desde el reconocimiento de voz hasta el análisis de grandes conjuntos de datos y la automatización de procesos industriales. En este curso los estudiantes podrán conocer los fundamentos y las técnicas básicas del aprendizaje estadístico para responder preguntas de investigación en Ciencias Cognoscitivas.\n\nObjetivos\n\nDescribir y explicar los conceptos y métodos principales del aprendizaje estadístico aplicados a las Ciencias Cognoscitivas.\nDiferenciar entre los tipos de aprendizaje computacional: supervisado, no supervisado, semi-supervisado y por reforzamiento\nComprender los fundamentos teóricos y los supuestos de técnicas como la regresión, las redes neuronales, los árboles de decisión y el análisis de conglomerados\nImplementar, en el lenguaje de programación R, las técnicas desarrolladas a lo largo del curso\nSeleccionar la técnica más adecuada en función del problema práctico o pregunta de investigación en las diversas áreas de las Ciencias Cognoscitivas."
  },
  {
    "objectID": "funciones.html",
    "href": "funciones.html",
    "title": "Funciones",
    "section": "",
    "text": "Identificar los elementos básicos que componen una función\nComprender las características de los principales tipos de argumentos en las funciones\nSer capaz de construir funciones propias"
  },
  {
    "objectID": "funciones.html#nombre",
    "href": "funciones.html#nombre",
    "title": "Funciones",
    "section": "2.1 Nombre",
    "text": "2.1 Nombre\n\n\n\n\n\n\n\n\n\nLos nombres de las funciones tienen pocas restricciones. Siguen las mismas reglas que otros objetos en R. Algunas recomendaciones/reglas:\n\nNo utilice nombres de objetos comunes R (por ejemplo, iris, x) u objetos que ya están en el entorno\nNo use nombres de funciones de uso frecuente (por ejemplo, mean)\nNo puede comenzar con un número\nDebería sugerir lo que hace\nNo debe ser muy largo\n\nEn caso de que tenga varias funciones con el mismo nombre, puede llamarlas individualmente utilizando el nombre del paquete (o namespace) seguido de :::\n\n\nCódigo\n# crear funcion sd\nsd &lt;- function(x) x^10\n\n# no calcula desv. st.\nsd(1:5)\n\n\n[1]       1    1024   59049 1048576 9765625\n\n\nCódigo\n# sd\nstats::sd(1:5)\n\n\n[1] 1.5811\n\n\nCódigo\n# remover nueva sd\nrm(sd)\n\n# usar de nuevo\nsd(1:5)\n\n\n[1] 1.5811\n\n\n \nLa sintaxis namespace::function también se puede usar para llamar a funciones desde paquetes que se han instalado pero que no están cargados en el entorno actual.\n \nLas funciones pueden ser anónimas:\n\n\nCódigo\n(function(x) x^10)(1:5)\n\n\n[1]       1    1024   59049 1048576 9765625\n\n\n \nEsto es más útil cuando se usan las funciones Xapply:\n\n\nCódigo\nl &lt;- list(1:5, 1:4, 1:3)\n\nlapply(l, function(x) x^10)\n\n\n[[1]]\n[1]       1    1024   59049 1048576 9765625\n\n[[2]]\n[1]       1    1024   59049 1048576\n\n[[3]]\n[1]     1  1024 59049"
  },
  {
    "objectID": "funciones.html#argumentos",
    "href": "funciones.html#argumentos",
    "title": "Funciones",
    "section": "2.2 Argumentos",
    "text": "2.2 Argumentos\n\n\n\n\n\n\n\n\n\n \nPermiten a los usuarios ingresar objetos en la función. Los argumentos pueden o no tener valores predeterminados. Cuando los argumentos tienen valores predeterminados, no es necesario proporcionarlos:\n\n\nCódigo\nf1 &lt;- function(x, y = 2) x + y\n\nf1(1)\n\n\n[1] 3\n\n\n \nPor supuesto, se pueden modificar:\n\n\nCódigo\nf1(3, 4)\n\n\n[1] 7\n\n\n \nLos argumentos sin valor predeterminado deben ser proporcionados:\n\n\nCódigo\nf1()\n\n\nError in f1(): argument \"x\" is missing, with no default\n\n\n \nSi todos los argumentos tienen un valor predeterminado, se puede invocar la función sin proporcionar ningún argumento:\n\n\nCódigo\nf1 &lt;- function(x = -2, y = 2) x + y\n\nf1()\n\n\n[1] 0\n\n\n \nEse es el caso de dev.off() y Sys.time():\n\n\nCódigo\nSys.time()\n\n\n[1] \"2024-12-03 16:20:13.282 CST\"\n\n\n \nLos argumentos pueden especificarse implícitamente por posición o por nombres incompletos:\n\n\nCódigo\nf2 &lt;- function(a1, b1, b2) {\n  list(a1 = a1, b1 = b1, b2 = b2)\n}\n\n# por posicion\nstr(f2(1, 2, 3))\n\n\nList of 3\n $ a1: num 1\n $ b1: num 2\n $ b2: num 3\n\n\nCódigo\n# por posicion y nombre\nstr(f2(a = 1, 2, 3))\n\n\nList of 3\n $ a1: num 1\n $ b1: num 2\n $ b2: num 3\n\n\nCódigo\nstr(f2(1, a = 2, 3))\n\n\nList of 3\n $ a1: num 2\n $ b1: num 1\n $ b2: num 3\n\n\nCódigo\nstr(f2(1, 2, a = 3))\n\n\nList of 3\n $ a1: num 3\n $ b1: num 1\n $ b2: num 2\n\n\nCódigo\n# por posicion y nombre parcial\nstr(f2(1, a = 2, b1 = 3))\n\n\nList of 3\n $ a1: num 2\n $ b1: num 3\n $ b2: num 1\n\n\n \nSin embargo, esto no funciona si los nombres son ambiguos:\n\n\nCódigo\nf2(b = 1, 2, a = 3)\n\n\nError in f2(b = 1, 2, a = 3): argument 1 matches multiple formal arguments\n\n\n \nEs más seguro (y, por lo tanto, una mejor práctica) usar los nombres completos de los argumentos.\n \nLas funciones también pueden tomar argumentos lógicos. Estos son útiles para modificar el comportamiento de la función para que coincida con diferentes escenarios. Por ejemplo, mean() permite a los usuarios ignorar los NA:\n\n\nCódigo\nv1 &lt;- c(1, 2, 3, NA)\n\n# sin ignorar NAs\nmean(v1, na.rm = FALSE)\n\n\n[1] NA\n\n\nCódigo\n# ignorando NAs\nmean(v1, na.rm = TRUE)\n\n\n[1] 2"
  },
  {
    "objectID": "funciones.html#ejercicio-1",
    "href": "funciones.html#ejercicio-1",
    "title": "Funciones",
    "section": "2.3 Ejercicio 1",
    "text": "2.3 Ejercicio 1\n1.1 ¿Qué hace la función cor.test()?\n1.2 Úsela con “Sepal.Length” y “Sepal.Width” de los datos de ejemplo iris (use data(iris))\n1.3 ¿Qué argumentos deben proporcionarse?\n1.4 ¿Qué hace el argumento alternative? Use diferentes valores para este argumento y compare los resultados\n1.5 ¿Cómo se puede calcular la correlación de Spearman?\n1.6 ¿Qué tipo de objeto devuelve esta función?\n1.7 ¿Cómo puede obtener el valor de p directamente del resultado de la función (sin guardar el resultado como un objeto)?\n1.8 Escoga una función de R y lea su documentación con el fin de entender su uso, cada uno de sus argumentos (y que tipo de objetos requieren) y el resultado que produce (es probable que se le pida que explique esto al grupo)"
  },
  {
    "objectID": "funciones.html#cuerpo",
    "href": "funciones.html#cuerpo",
    "title": "Funciones",
    "section": "2.4 Cuerpo",
    "text": "2.4 Cuerpo\n\n\n\n\n\n\n\n\n\n \nEl cuerpo de una función puede contener:\n\nComprobación de argumentos\nManipulación de datos\nDefinición de resultados\n\n \nEl cuerpo de la función puede tomar el mismo tipo de código utilizado en cualquier rutina de R. Sin embargo, los objetos creados no estarán disponibles en el entorno.\nCuando se realizan varios cálculos, debemos incluir una declaración de retorno (return statement), que define explícitamente la salida de la función. Esto se hace usando la función return():\n\n\nCódigo\n# sin \"return statement\"\nf1 &lt;- function(x, y) {\n  z1 &lt;- 2 * x + y\n  z2 &lt;- x + 2 * y\n  z3 &lt;- 2 * x + 2 * y\n  z4 &lt;- x / y\n}\n\nf1(5, 3)\n\nf1_out &lt;- f1(5, 3)\n\nf1_out\n\n\n[1] 1.6667\n\n\nCódigo\n# con \"return statement\"\nf2 &lt;- function(x, y) {\n  z1 &lt;- 2 * x + y\n  z2 &lt;- x + 2 * y\n  z3 &lt;- 2 * x + 2 * y\n  z4 &lt;- x / y\n\n  return(c(z1, z2, z3, z4))\n}\n\nf2(5, 3)\n\n\n[1] 13.0000 11.0000 16.0000  1.6667\n\n\nCódigo\nf2_out &lt;- f2(5, 3)\n\nf2_out\n\n\n[1] 13.0000 11.0000 16.0000  1.6667\n\n\n \nPor lo tanto, cuando no se proporciona una declaración de retorno, la función devolverá el último objeto que se creó en el cuerpo de la función:\n\n\nCódigo\n# sin \"return statement\"\nf3 &lt;- function(x, y) {\n  z1 &lt;- 2 * x + y\n  z2 &lt;- x + 2 * y\n  z3 &lt;- 2 * x + 2 * y\n  z4 &lt;- x / y\n\n  c(z1, z2, z3, z4)\n}\n\nf3(5, 3)\n\n\n[1] 13.0000 11.0000 16.0000  1.6667\n\n\nCódigo\nf3_out &lt;- f3(5, 3)\n\nf3_out\n\n\n[1] 13.0000 11.0000 16.0000  1.6667\n\n\n \nEs más seguro usar return().\n \nCuando una función realiza varias tareas, podemos usar una lista para juntar los diferentes objetos. Esto es particularmente útil cuando se generan objetos de diferentes clases (por ejemplo, vectores y listas):\n\n\nCódigo\nf4 &lt;- function(x, y) {\n  # 1 elemento\n  z1 &lt;- x + y\n\n  # 2 elementos\n  z2 &lt;- c(x, y / 3)\n\n  # vector logico\n  z3 &lt;- z2 &lt; 10\n\n\n  l &lt;- list(z1, z2, z3)\n\n  return(l)\n}\n\nf4(10, 5)\n\n\n[[1]]\n[1] 15\n\n[[2]]\n[1] 10.0000  1.6667\n\n[[3]]\n[1] FALSE  TRUE\n\n\n \nPodemos acceder a elementos específicos de la salida de una función mediante indexación:\n\n\nCódigo\n# elemento 1\nf4(10, 5)[[1]]\n\n\n[1] 15\n\n\nCódigo\n# elemento 2\nf4(10, 5)[[2]]\n\n\n[1] 10.0000  1.6667\n\n\nCódigo\n# elemento 3\nf4(10, 5)[[3]]\n\n\n[1] FALSE  TRUE\n\n\n \nPor supuesto, también podemos guardar el resultado como un objeto y acceder a cada elemento mediante la indexación:\n\n\nCódigo\n# elemento 1\nout &lt;- f4(10, 5)\n\n# elemento 1\nout[[1]]\n\n\n[1] 15\n\n\nCódigo\n# elemento 2\nout[[2]]\n\n\n[1] 10.0000  1.6667"
  },
  {
    "objectID": "funciones.html#ejercicio-2",
    "href": "funciones.html#ejercicio-2",
    "title": "Funciones",
    "section": "2.5 Ejercicio 2",
    "text": "2.5 Ejercicio 2\n2.1 Cree una función llamada promedio que calcule el promedio de un vector numérico. Internamente solo puede utilizar las funciones sum() y length() (suma y división, no puede llamar la función mean())\n2.2 Cree una función que tome 2 argumentos numéricos (llámelos ‘x’ y ‘y’), eleve cada uno al cuadrado y luego los sume\n2.3 Agregue valores predeterminados a cada argumento\n2.4 Ejecute la función usando los valores predeterminados\n2.5 Ejecute la función usando un valor predeterminado y uno proporcionado en la llamada\n2.6 Ejecute la función proporcionando ambos valores en la llamada"
  },
  {
    "objectID": "funciones.html#ejercicio-3",
    "href": "funciones.html#ejercicio-3",
    "title": "Funciones",
    "section": "3.1 Ejercicio 3",
    "text": "3.1 Ejercicio 3\n3.1 Cree una función que tome 3 argumentos numéricos, multiplíquelos y luego calcule el logaritmo natural del resultado (función log())\n3.2 Agregue valores predeterminados a cada argumento\n3.3 Ejecute la función con uno de los argumentos con un número negativo. ¿Qué pasa? ¿Por qué?\n3.4 Agregue un argumento lógico que permita a los usuarios (si lo desean) convertir los argumentos de entrada a su valor absoluto (usando la función abs()). Agregue las modificaciones necesarias para que la función haga los cálculos con y sin valores absolutos."
  },
  {
    "objectID": "funciones.html#código-mas-limpio",
    "href": "funciones.html#código-mas-limpio",
    "title": "Funciones",
    "section": "4.1 Código mas limpio",
    "text": "4.1 Código mas limpio\nLos objetos creados dentro del cuerpo no están disponibles en el entorno actual:\n\n\nCódigo\n# primero remover todo los objetos\nrm(list = ls())\n\nf5 &lt;- function(x) {\n  sqr &lt;- x * x\n  lg_sqr &lt;- log(x)\n  return(lg_sqr)\n}\n\nf5(7)\n\n\n[1] 1.9459\n\n\nCódigo\nexists(\"sqr\")\n\n\n[1] FALSE\n\n\nCódigo\nexists(\"lg_sqr\")\n\n\n[1] FALSE\n\n\n\n\nCódigo\nx &lt;- 7\nsqr &lt;- x * x\nlg_sqr &lt;- log(x)\n\n\nexists(\"sqr\")\n\n\n[1] TRUE\n\n\nCódigo\nexists(\"lg_sqr\")\n\n\n[1] TRUE"
  },
  {
    "objectID": "funciones.html#facil-de-correr-y-compartir",
    "href": "funciones.html#facil-de-correr-y-compartir",
    "title": "Funciones",
    "section": "4.2 Facil de correr y compartir",
    "text": "4.2 Facil de correr y compartir\nSe pueden invocar funciones desde archivos de R externos sin estar definidas en el código actual con la función source(). En este ejemplo creamos la función fnctn_X:\n\n\nCódigo\nfnctn_X &lt;- function(sq_mt, vctr) {\n  # trasponer matriz y calcular est dev\n  stp1 &lt;- t(sq_mt)\n  stp2 &lt;- vctr * vctr\n\n  # guardar en lista\n  rslt &lt;- list(stp1, stp2)\n  return(rslt)\n}\n\nfnctn_X(sq_mt = cbind(c(1, 2), c(3, 4)), vctr = c(2, 3))\n\n\n[[1]]\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n[[2]]\n[1] 4 9\n\n\n \nGuarde el código anterior en un archivo R llamado ‘function_X.R’. Ahora la función se puede cargar usando source():\n\n\nCódigo\n# removamosla del ambiente\nrm(fnctn_X)\n\n# cargar funcion\nsource(\"function_X.R\")\n\n# run it\nfnctn_X(sq_mt = cbind(c(1, 2), c(3, 4)), vctr = c(2, 3))\n\n\n\n\n[[1]]\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n[[2]]\n[1] 4 9\n\n\n \nAdemás, este código se puede compartir fácilmente. Se puede enviar por correo electrónico o publicar en línea. Incluso se puede cargar desde repositorios en línea:\n\n\nCódigo\n# remover objetos\nrm(list = ls())\n\n# revisar si existe en ambiente actual\nexists(\"fnctn_X\")\n\n\n[1] FALSE\n\n\nCódigo\n# cargar desde el internet\nsource(\n  paste0(\n    \"https://raw.githubusercontent.com/maRce10/\",\n    \"r_avanzado_2023/master/data/function_x.r\"\n  )\n)\n\n\n# revisar si existe en ambiente actual\nexists(\"fnctn_X\")\n\n\n[1] TRUE"
  },
  {
    "objectID": "funciones.html#se-aplica-fácilmente-a-nuevos-objetos",
    "href": "funciones.html#se-aplica-fácilmente-a-nuevos-objetos",
    "title": "Funciones",
    "section": "4.3 Se aplica fácilmente a nuevos objetos",
    "text": "4.3 Se aplica fácilmente a nuevos objetos\nEsto ya debería ser obvio a este punto."
  },
  {
    "objectID": "funciones.html#otros-consejos",
    "href": "funciones.html#otros-consejos",
    "title": "Funciones",
    "section": "4.4 Otros consejos",
    "text": "4.4 Otros consejos\n\nConstruir funciones cortas:\n\nFácil de leer\nFácil de arreglar y actualizar\nSi es demasiado largo, probablemente debería dividirse en varias funciones\nGenera modularidad\n\nAñadir comentarios a todo el código  \nAgregar descripciones a cada uno de los argumentos que toma  \nFunción de prueba con diferentes valores/escenarios"
  },
  {
    "objectID": "funciones.html#ejercicio-4",
    "href": "funciones.html#ejercicio-4",
    "title": "Funciones",
    "section": "4.5 Ejercicio 4",
    "text": "4.5 Ejercicio 4\n4.1 Cree una funcion que calcule el promedio, la desviación estándar y el error estándar de un vector numérico. Estos valores deben ser devueltos como una lista con nombres.\n4.2 Permita a los usuarios ignorar los NAs (similar al argumento na.rm en mean(), pista: añada un argumento lógico)\n4.3 Haga que la función además produzca un histograma del vector numérico proporcionado por el usuario\n4.4 Haga que la función escoja un color al azar para las barras del histograma cada vez que se corre (pista: sample(vector.de.colores, 1))\n4.5 Añada un argumento a la función que permita el usuario, si lo desea, calcular la sumatoria (sum()) junto con el resto de los descriptores estadísticos\n4.6 Agregue el promedio y la desviación estándar al título del histograma (pista: use paste())\n4.7 Modifique la función para que también cree una linea vertical indicando el promedio del vector numérico proporcionado por el usuario\n4.8 Modifique la función para que añada un polígono transparente sobre el área que cubre el promedio +/- la desviación estándar (puede usar la función rect())\n4.9 Modifique la función con un argumento lógico que permita al usuario controlar si se crea un gráfico o no"
  },
  {
    "objectID": "funciones.html#lecturas-adicionales",
    "href": "funciones.html#lecturas-adicionales",
    "title": "Funciones",
    "section": "4.6 Lecturas adicionales",
    "text": "4.6 Lecturas adicionales\n\n Introduction to R guide to writing functions with information for a total beginner\nInformation on functions for intermediate and advanced users (Hadley Wickham).\nThe official R intro material on writing your own functions (ir a “Writing your own function”)"
  },
  {
    "objectID": "funciones.html#referencias",
    "href": "funciones.html#referencias",
    "title": "Funciones",
    "section": "4.7 Referencias",
    "text": "4.7 Referencias\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for data science: import, tidy, transform, visualize, and model data. website\n\n\nInformación de la sesión\n\n\nR version 4.4.1 (2024-06-14)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 22.04.4 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0 \nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=es_CR.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=es_CR.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=es_CR.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=es_CR.UTF-8 LC_IDENTIFICATION=C       \n\ntime zone: America/Costa_Rica\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.1    fastmap_1.2.0     cli_3.6.3        \n [5] tools_4.4.1       htmltools_0.5.8.1 rstudioapi_0.16.0 yaml_2.3.10      \n [9] rmarkdown_2.28    knitr_1.48        jsonlite_1.8.9    xfun_0.48        \n[13] digest_0.6.37     rlang_1.1.4       evaluate_1.0.1"
  },
  {
    "objectID": "regresion_logística_y_multinomial.html",
    "href": "regresion_logística_y_multinomial.html",
    "title": "Regresión logística y multinomial",
    "section": "",
    "text": "Expandir la regresión lineal a otros tipos de variable respuesta\nIntroducir modelos que predicen variables categóricas\nFamiliarizarse con el uso de estos modelos\nPaquetes a utilizar en este manual:\nCódigo\n# instalar/cargar paquetes\n\nsketchy::load_packages(\n  c(\"ggplot2\", \n    \"viridis\", \n    \"nnet\",\n    \"caret\"\n   )\n  )\n\n\nLoading required package: caret\n\n\nLoading required package: lattice"
  },
  {
    "objectID": "regresion_logística_y_multinomial.html#ejemplos-de-uso",
    "href": "regresion_logística_y_multinomial.html#ejemplos-de-uso",
    "title": "Regresión logística y multinomial",
    "section": "1.1 Ejemplos de uso",
    "text": "1.1 Ejemplos de uso\nEn un estudio sobre comportamiento juvenil, los investigadores quieren predecir si los adolescentes se involucran en conductas de riesgo (por ejemplo, consumo de drogas) en función de factores como la influencia de pares, el rendimiento escolar, y el apoyo parental.\n\nVariable dependiente: Conducta riesgosa (1 = Sí, 0 = No).\nVariables independientes: Influencia de pares, rendimiento escolar, apoyo parental.\nAplicación de regresión logística: Se usa para predecir la probabilidad de que un adolescente participe en conductas riesgosas.\n\nUn psicólogo quiere predecir si un paciente tiene depresión (Sí/No) en función de factores como el nivel de estrés, la duración del sueño, el historial familiar de trastornos mentales, y la puntuación en un test de ansiedad.\n\nVariable dependiente: Depresión (1 = Sí, 0 = No).\nVariables independientes: Nivel de estrés, horas de sueño, historial familiar de trastornos, puntuación de ansiedad.\nAplicación de regresión logística: Se usa para predecir si una persona tiene depresión a partir de los predictores."
  },
  {
    "objectID": "regresion_logística_y_multinomial.html#simular-datos",
    "href": "regresion_logística_y_multinomial.html#simular-datos",
    "title": "Regresión logística y multinomial",
    "section": "1.2 Simular datos",
    "text": "1.2 Simular datos\nPrimero, vamos a simular un conjunto de datos donde la respuesta sea binaria. En este ejemplo, supongamos que tenemos dos variables predictoras: x1 y x2.\n\n\nCódigo\n# Simulación de datos\nset.seed(123)  # Para reproducibilidad\nn &lt;- 100  # Número de observaciones\nx1 &lt;- rnorm(n)  # Predictor 1: variable normal\nx2 &lt;- rnorm(n)  # Predictor 2: variable normal\n\n# Coeficientes reales para la simulación\nb0 &lt;- -0.5  # Intercepto\nb1 &lt;- 3.4  # Coeficiente de x1\nb2 &lt;- -3  # Coeficiente de x2\n\n# Calcular la probabilidad usando la función logística\nlogit_p &lt;- b0 + b1 * x1 + b2 * x2\np &lt;- 1 / (1 + exp(-logit_p))\n\n# Generar la respuesta binaria\ny &lt;- rbinom(n, 1, p)\n\n# Crear un data frame\ndatos &lt;- data.frame(x1 = x1, x2 = x2, y = y)\n\n# explorar datos\nhead(datos)\n\n\n\n\n\n\nx1\nx2\ny\n\n\n\n\n-0.56048\n-0.71041\n1\n\n\n-0.23018\n0.25688\n0\n\n\n1.55871\n-0.24669\n1\n\n\n0.07051\n-0.34754\n1\n\n\n0.12929\n-0.95162\n1\n\n\n1.71506\n-0.04503\n1\n\n\n\n\n\n\nEstos graficos nos muestran las relaciones entre x1, x2 y Y:\n\n\nCódigo\n# graficar\nggplot(datos, aes(x = x1, y = y)) + \n  geom_point(color = \"#3E4A89FF\")\n\n\n\n\n\n\n\n\n\nCódigo\nggplot(datos, aes(x = x2, y = y)) + \n  geom_point(color =  \"#1F9E89FF\")"
  },
  {
    "objectID": "regresion_logística_y_multinomial.html#ajustar-el-modelo",
    "href": "regresion_logística_y_multinomial.html#ajustar-el-modelo",
    "title": "Regresión logística y multinomial",
    "section": "1.3 Ajustar el modelo",
    "text": "1.3 Ajustar el modelo\nPara ajustar el modelo de regresión logística en R, utilizamos la función glm() con el argumento family = binomial. glm() es una función de R básico para ajustar modelos lineales generalizados.\n\n\nCódigo\n# Ajustar el modelo de regresión logística\nmodelo_log &lt;- glm(y ~ x1 + x2, data = datos, family = binomial)\n\n# Resumen del modelo\nsummary(modelo_log)\n\n\n\nCall:\nglm(formula = y ~ x1 + x2, family = binomial, data = datos)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -0.759      0.379   -2.00    0.045 *  \nx1             2.885      0.699    4.13  3.7e-05 ***\nx2            -3.515      0.771   -4.56  5.2e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 138.629  on 99  degrees of freedom\nResidual deviance:  57.381  on 97  degrees of freedom\nAIC: 63.38\n\nNumber of Fisher Scoring iterations: 7\n\n\nAhora podemos graficar los datos crudos junto a la curva de mejor ajuste. Para esto debemos estimar los valores predichos por el modelo primero:\n\n\nCódigo\n# Crear un nuevo data frame con las predicciones para x1\nnuevos_datos &lt;-\n  expand.grid(\n    x1 = seq(min(datos$x1), max(datos$x1), length.out = 100),\n    x2 = seq(min(datos$x2), max(datos$x2), length.out = 100)\n  )\n\n# anadir predicciones\nnuevos_datos$pred_prob &lt;-\n  predict(object = modelo_log,\n          newdata = nuevos_datos,\n          type = \"response\")\n\n# Crear el gráfico de puntos y la curva de mejor ajuste para x1\nggplot(datos, aes(x = x1, y = y)) +\n  geom_point(alpha = 0.5, color = \"#3E4A89FF\") +  # Datos crudos\n  geom_smooth(data = nuevos_datos, aes(y = pred_prob, x = x1), method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE) +\n  labs(y = \"Probabilidad de y = 1\") \n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n\n\n\n\n\n\n\n\nCódigo\n# Crear el gráfico de puntos y la curva de mejor ajuste para x2\nggplot(datos, aes(x = x2, y = y)) +\n  geom_point(alpha = 0.5, color = \"#1F9E89FF\") +  # Datos crudos\n  geom_smooth(data = nuevos_datos, aes(y = pred_prob, x = x2), method = \"glm\", method.args = list(family = \"binomial\"), se = FALSE) +\n  labs(y = \"Probabilidad de y = 1\") \n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!"
  },
  {
    "objectID": "regresion_logística_y_multinomial.html#interpretación-del-modelo",
    "href": "regresion_logística_y_multinomial.html#interpretación-del-modelo",
    "title": "Regresión logística y multinomial",
    "section": "1.4 Interpretación del modelo",
    "text": "1.4 Interpretación del modelo\nEn el modelo de regresión logística, los coeficientes estan dados como el logaritmo de los chances (log-odds) de que Y = 1:\n\n\nCódigo\n# Obtener los log-chances\ncoefs &lt;- coef(modelo_log)\n\ncoefs\n\n\n(Intercept)          x1          x2 \n    -0.7590      2.8846     -3.5149 \n\n\nLos chances (“odds”, a veces traducido como probabilidades) se definen como la razón entre la probabilidad de la ocurrencia de un evento y la probabilidad de que ese evento no ocurra:\n\\[\n\\text{Odds} = \\frac{\\text{Probabilidad de éxito}}{\\text{Probabilidad de fracaso}} = \\frac{P(E)}{1 - P(E)}\n\\]\nEl ‘log-odds’ es simplemente el logaritmo natural de ese cociente:\n\\[\n\\text{Log-Odds} = \\log(\\text{Odds}) = \\log\\left(\\frac{P(E)}{1 - P(E)}\\right)\n\\]\nEsto significa que por cada aumento de una unidad de x1, los ‘log-odds’ de que Y = 1 aumentan en 2.88.\nPueden interpretarse mas facilmente en términos de la razón de los chances (odds ratio). Para obtener las razones de chances, simplemente tomamos el exponente de los coeficientes:\n\n\nCódigo\n# Obtener las razones de probabilidades\nexp_coefs &lt;- exp(coefs)\n\nexp_coefs\n\n\n(Intercept)          x1          x2 \n   0.468134   17.896103    0.029752 \n\n\nEsto quiere decir que el chance de Y=1 aumenta en 17.9 por cada aumento de una unidad de x1. Esto puede ser aun mas intuitivo si lo interpretamos como un porcentaje: un aumento de una unidad de x1 esta asociado con un aumento de aproximadamente 1690% en los chances de que Y=1."
  },
  {
    "objectID": "regresion_logística_y_multinomial.html#ejercicio-1",
    "href": "regresion_logística_y_multinomial.html#ejercicio-1",
    "title": "Regresión logística y multinomial",
    "section": "1.5 Ejercicio 1",
    "text": "1.5 Ejercicio 1\nLos datos de sobrevivencia de pasajeros del Titanic son ampliamente utilizado para ilustrar modelos de predicción. Contiene información sobre los pasajeros del Titanic, incluyendo variables como:\n\nSobrevivencia (Survived): indica si el pasajero sobrevivió o no\nClase (Pclass): clase del pasajero (1, 2 o 3)\nSexo (Sex): género del pasajero\nEdad (Age): edad del pasajero\n\nPuedes cargarlo directamente en R usando el paquete titanic o manipularlo desde el conjunto de datos base. El siguiente código carga y da formato a los datos para que puedan ser utilizados en una regresión logística:\n\n\nCódigo\n# cargar datos\ndata(\"Titanic\")\n\n# dar formato con una observacion por fila\ndatos_titanic &lt;- as.data.frame(Titanic)\ndatos_titanic &lt;- datos_titanic[datos_titanic$Freq &gt; 0, ]\n\ndatos_tab_titanic &lt;- do.call(rbind, lapply(1:nrow(datos_titanic), function(x) datos_titanic[rep(x, datos_titanic$Freq[x]),]))\ndatos_tab_titanic$Freq &lt;- NULL\n\n# explorar datos\nhead(datos_tab_titanic, 10)\n\n\n\n\n\n\n\nClass\nSex\nAge\nSurvived\n\n\n\n\n3\n3rd\nMale\nChild\nNo\n\n\n3.1\n3rd\nMale\nChild\nNo\n\n\n3.2\n3rd\nMale\nChild\nNo\n\n\n3.3\n3rd\nMale\nChild\nNo\n\n\n3.4\n3rd\nMale\nChild\nNo\n\n\n3.5\n3rd\nMale\nChild\nNo\n\n\n3.6\n3rd\nMale\nChild\nNo\n\n\n3.7\n3rd\nMale\nChild\nNo\n\n\n3.8\n3rd\nMale\nChild\nNo\n\n\n3.9\n3rd\nMale\nChild\nNo\n\n\n\n\n\n\nEl siguiente codigo ajusta un modelo de regresión logística a estos datos con la variable “clase” como único predictor:\n\n\nCódigo\n# Ajustar el modelo de regresión logística\nmodelo_log &lt;- glm(Survived ~ Class, data = datos_tab_titanic, family = binomial)\n\n# Resumen del modelo\nsummary(modelo_log)\n\n\n\nCall:\nglm(formula = Survived ~ Class, family = binomial, data = datos_tab_titanic)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    0.509      0.115    4.44  8.8e-06 ***\nClass2nd      -0.856      0.166   -5.16  2.5e-07 ***\nClass3rd      -1.596      0.144  -11.11  &lt; 2e-16 ***\nClassCrew     -1.664      0.139  -11.97  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2769.5  on 2200  degrees of freedom\nResidual deviance: 2588.6  on 2197  degrees of freedom\nAIC: 2597\n\nNumber of Fisher Scoring iterations: 4\n\n\nPodemos evaluar el desempeño del modelo con una matriz de confusión:\n\n\nCódigo\n# predecir valores\npred_vals &lt;- predict(object = modelo_log, newdata = datos_tab_titanic, type = \"response\")\n\n# binarizar\npred_cat &lt;- ifelse(pred_vals &gt; 0.5, \"Yes\", \"No\")\n\n# hacer la matriz de confusion\nmat_conf &lt;-\n    confusionMatrix(factor(pred_cat), factor(datos_tab_titanic$Survived))\n\n# imprimir resultado\nmat_conf$table\n\n\n          Reference\nPrediction   No  Yes\n       No  1368  508\n       Yes  122  203\n\n\n\n\nCódigo\n# convertir a data frame\nconf_df &lt;- as.data.frame(mat_conf$table)\n\n# agregar totales por categoria\nconf_df$total &lt;-\n    sapply(conf_df$Reference, function(x)\n        sum(datos_tab_titanic$Survived ==\n                x))\n\n# calcular proporciones\nconf_df$proportion &lt;- conf_df$Freq / conf_df$total\n\n# graficar\nggplot(conf_df, aes(x = Reference, y = Prediction, fill = proportion)) +\n  geom_tile() + \n  coord_equal() + \n  scale_fill_distiller(palette = \"Greens\", direction = 1) + \n  geom_text(aes(label = round(proportion, 2)), color = \"black\", size = 3) + \n  labs(x = \"Observado\", y = \"Predicho\", fill = \"Proporción\") \n\n\n\n\n\n\n\n\n\n\n\n\n\nCódigo\n# precision\nmat_conf$overall[\"Accuracy\"]\n\n\nAccuracy \n 0.71377 \n\n\n\n1.1 Construya un modelo de regresión logística que prediga la sobrevivencia de los pasajeros del Titanic en función de su género. ¿Cuál es la precisión del modelo?\n1.2 Añada una variable predictora mas a la vez (e.g. Survived ~ Sex; Survived ~ Sex + Class; Survived ~ Sex + Class + Age) y a cada nuevo modelo evalue su precisión.\n1.3 ¿Qué modelo tiene la mejor precisión? ¿Qué variables parecen ser más importantes para predecir la sobrevivencia de los pasajeros del Titanic?"
  },
  {
    "objectID": "regresion_logística_y_multinomial.html#ejemplos-de-uso-1",
    "href": "regresion_logística_y_multinomial.html#ejemplos-de-uso-1",
    "title": "Regresión logística y multinomial",
    "section": "2.1 Ejemplos de uso",
    "text": "2.1 Ejemplos de uso\nUn psiquiatra quiere predecir a qué tipo de trastorno mental padece un paciente (por ejemplo, depresión, ansiedad o trastorno bipolar) en función de factores como el historial familiar, nivel de estrés, horas de sueño, y puntuaciones en diversos tests psicológicos.\n\nVariable dependiente: Tipo de trastorno mental (1 = Depresión, 2 = Ansiedad, 3 = Trastorno Bipolar).\nVariables independientes: Historial familiar, nivel de estrés, horas de sueño, puntuaciones en tests psicológicos.\nAplicación de la regresión multinomial: Se utiliza para predecir la probabilidad de pertenecer a una de las tres categorías diagnósticas.\n\nUna empresa de alimentos quiere predecir qué tipo de envase (plástico, vidrio o biodegradable) preferirán los consumidores para sus productos, en función de factores como el precio del producto, la percepción ambiental y la facilidad de uso.\n\nVariable dependiente: Tipo de envase preferido (1 = Plástico, 2 = Vidrio, 3 = Biodegradable).\nVariables independientes: Precio del producto, percepción ambiental, facilidad de uso.\nAplicación de la regresión multinomial: Permite predecir qué tipo de envase es más probable que prefieran los consumidores, basado en sus actitudes y características."
  },
  {
    "objectID": "regresion_logística_y_multinomial.html#simular-datos-1",
    "href": "regresion_logística_y_multinomial.html#simular-datos-1",
    "title": "Regresión logística y multinomial",
    "section": "2.2 Simular datos",
    "text": "2.2 Simular datos\nPara ilustrar cómo funciona la regresión multinomial, vamos a simular un conjunto de datos con tres categorías posibles para la variable dependiente.\n\n\nCódigo\n# Establecer una semilla para reproducibilidad\nset.seed(123)\n\n# Simular variables predictoras\nn &lt;- 500  # número de observaciones\nx1 &lt;- rnorm(n)\nx2 &lt;- rnorm(n)\n\n# Probabilidades para cada categoría\np_y1 &lt;- exp(1 + 2 * x1 - 5 * x2) / (1 + exp(1 + 2 * x1 - 5 * x2) + exp(-2 + x1 + 10 * x2))\np_y2 &lt;- exp(-2 + x1 + 10 * x2) / (1 + exp(1 + 2 * x1 - 5 * x2) + exp(-2 + x1 + 10 * x2))\np_y3 &lt;- 1 - p_y1 - p_y2  # Probabilidad de la tercera categoría\n\n# Asignar categorías de acuerdo con las probabilidades\ny &lt;- sapply(1:n, function(i) sample(1:3, size = 1, prob = c(p_y1[i], p_y2[i], p_y3[i])))\n\n# Crear un dataframe con las variables simuladas\ndatos_multin &lt;- data.frame(y = factor(y, labels = c(\"a\", \"b\", \"c\")), x1 = x1, x2 = x2)\n\n# Visualizar las primeras filas de los datos simulados\nhead(datos_multin)\n\n\n\n\n\n\ny\nx1\nx2\n\n\n\n\na\n-0.56048\n-0.60189\n\n\na\n-0.23018\n-0.99370\n\n\nb\n1.55871\n1.02679\n\n\nb\n0.07051\n0.75106\n\n\na\n0.12929\n-1.50917\n\n\na\n1.71506\n-0.09515"
  },
  {
    "objectID": "regresion_logística_y_multinomial.html#explorar-datos",
    "href": "regresion_logística_y_multinomial.html#explorar-datos",
    "title": "Regresión logística y multinomial",
    "section": "2.3 Explorar datos",
    "text": "2.3 Explorar datos\nCategorias de Y en el espacio X1 vs X2\n\n\nCódigo\nggplot(datos_multin, aes(x = x1, y = x2, color = y)) +\n  geom_point() +\n  scale_color_viridis_d(option = \"G\", begin = 0.4, end = 0.9) +\n  labs(x = \"x1\", y = \"x2\", color = \"Categoría\")"
  },
  {
    "objectID": "regresion_logística_y_multinomial.html#ajustar-el-modelo-1",
    "href": "regresion_logística_y_multinomial.html#ajustar-el-modelo-1",
    "title": "Regresión logística y multinomial",
    "section": "2.4 Ajustar el modelo",
    "text": "2.4 Ajustar el modelo\nEl siguiente paso es ajustar un modelo de regresión multinomial a los datos simulados utilizando la función multinom() del paquete nnet.\n\n\nCódigo\n# Ajustar el modelo de regresión multinomial\nmodelo_multinom &lt;- multinom(y ~ x1 + x2, data = datos_multin)\n\n\n# weights:  12 (6 variable)\ninitial  value 549.306144 \niter  10 value 124.300458\niter  20 value 123.855606\nfinal  value 123.845494 \nconverged\n\n\nCódigo\n# Resumen del modelo\nsummary(modelo_multinom)\n\n\nCall:\nmultinom(formula = y ~ x1 + x2, data = datos_multin)\n\nCoefficients:\n  (Intercept)       x1     x2\nb     -2.7457 -0.72491 14.052\nc     -1.0776 -2.06886  4.611\n\nStd. Errors:\n  (Intercept)      x1      x2\nb     0.49286 0.33465 1.67940\nc     0.25239 0.31713 0.75636\n\nResidual Deviance: 247.69 \nAIC: 259.69 \n\n\nPodemos agregar los valores de p para cada tamaño de efecto de esta forma:\n\n\nCódigo\nsumm_modelo_multinom &lt;- summary(modelo_multinom)\n\ncoefs &lt;- summ_modelo_multinom$coefficients\n\n# anadir valores de p\np_x1 &lt;- (1 - pnorm(abs(coefs[, 2]), 0, 1)) * 2\np_x2 &lt;- (1 - pnorm(abs(coefs[, 3]), 0, 1)) * 2\n\ncoefs &lt;- cbind(coefs, p_x1, p_x2)\n\ncoefs\n\n\n  (Intercept)       x1     x2    p_x1       p_x2\nb     -2.7457 -0.72491 14.052 0.46851 0.0000e+00\nc     -1.0776 -2.06886  4.611 0.03856 4.0081e-06\n\n\n \n\n \n\nInterpretación del modelo\n\n \nCuadro con los coeficientes (estimados):\n\n\n  (Intercept)       x1     x2    p_x1       p_x2\nb     -2.7457 -0.72491 14.052 0.46851 0.0000e+00\nc     -1.0776 -2.06886  4.611 0.03856 4.0081e-06\n\n\n\nEl modelo encontró que \\(\\beta_1b\\) es -0.72491 y que no es significativamente diferente de 0 (p = 0.46851). Podemos expresarlo como chances al calcular exp(coefs[1, 2]) que es 0.48. Es decir, la probabilidad de pertenecer a la categoría “b” aumenta en 0.48 comparado con la probabilidad de pertenecer categoría “a” con un aumento de una unidad de x1. En este caso el aumento es menor a uno lo que quiere decir que la probabilidad de ocurrencia de “b” es menor a la de “a” con un aumento de x1. Si mas bien calculamos 1 / exp(coefs[1, 2]) podemos decir que la probabilidad de pertenecer a la categoría “a” aumenta en 2.06 comparado con la probabilidad de pertenecer categoría “b” con un aumento de una unidad de x1.\nEl modelo encontró que \\(\\beta_1c\\) es -2.06886 y que es significativamente diferente de 0 (p = 0.03856). Al calcular los chances (exp(coefs[2, 2])) podemos decir que la probabilidad de pertenecer a la categoría “c” aumenta en 0.48 comparado con la probabilidad de pertenecer a la categoría “a”.\nTambién se encontró que el \\(\\beta_2b\\) es 14.05159 y que es significativamente diferente de 0 (p = 0). Al calcular los chances (exp(coefs[1, 3])) podemos decir que la probabilidad de pertenecer a la categoría “b” aumenta en 1.26627^{6} comparado con la probabilidad de pertenecer a la categoría “a”.\nTambién se encontró que el \\(\\beta_2c\\) es 4.61096 y que también es significativamente diferente de 0 (p = 4.0081^{-6}). Al calcular los chances (exp(coefs[2, 3])) podemos decir que la probabilidad de pertenecer a la categoría “c” aumenta en 100.58 comparado con la probabilidad de pertenecer a la categoría “a”."
  },
  {
    "objectID": "regresion_logística_y_multinomial.html#visualización-de-resultados",
    "href": "regresion_logística_y_multinomial.html#visualización-de-resultados",
    "title": "Regresión logística y multinomial",
    "section": "2.5 Visualización de resultados",
    "text": "2.5 Visualización de resultados\nUna forma útil de visualizar los resultados del modelo es graficar las probabilidades predichas para cada categoría en función de los predictores. Esto permite observar cómo cambian las probabilidades a medida que cambian los valores de las variables independientes.\nEn el siguiente ejemplo, graficaremos las probabilidades predichas de pertenecer a cada categoría con respecto a una de las variables independientes (x1):\n\n\nCódigo\n# Crear una secuencia de valores para x1\nx1_nuevo &lt;- seq(min(datos_multin$x1), max(datos_multin$x1), length.out = 100)\n\n# Generar un nuevo conjunto de datos con x1 variando y x2 fijo\nnuevo_data &lt;- data.frame(x1 = x1_nuevo, x2 = 0)\n\n# Obtener las probabilidades predichas para el nuevo conjunto de datos\nprob_predichas &lt;- predict(modelo_multinom, newdata = nuevo_data, type = \"probs\")\n\n# definir colores\ncols &lt;- mako(3, begin = 0.4, end = 0.9)\n\n# Graficar las probabilidades predichas\nmatplot(x1_nuevo, prob_predichas, type = \"l\", lty = 1, col = cols,\n        xlab = \"x1\", ylab = \"Probabilidad predicha\",\n        main = \"Probabilidades predichas para cada categoría\")\nlegend(\"topright\", legend = levels(datos_multin$y), col = cols, lty = 1)\n\n\n\n\n\n\n\n\n\nEste gráfico muestra cómo varían las probabilidades predichas de pertenecer a cada categoría conforme el valor de x1 cambia, con x2 fijo en 0. Las líneas de diferentes colores corresponden a las diferentes categorías de la variable dependiente.\n\n2.5.1 Matriz de confusión\nAl ser este en escencia un modelo de clasificación, podemos evaluar su desempeño con una matriz de confusión:\n\n\nCódigo\n# predecir valores\npred_cat &lt;- predict(modelo_multinom, datos_multin, type = \"class\")\n\n# hacer la matriz de confusion\nmat_conf &lt;-\n    confusionMatrix(factor(pred_cat), factor(datos_multin$y))\n\n# imprimir resultado\nmat_conf$table\n\n\n          Reference\nPrediction   a   b   c\n         a 238   7  17\n         b   7 186  13\n         c   7   1  24\n\n\nEsta función tambien estima otros descriptores del desempeño del modelo. Quizas el mas relevante es la precisión (accuracy), que es la proporción de observaciones que fueron clasificadas correctamente:\n\n\nCódigo\nmat_conf$overall[\"Accuracy\"]\n\n\nAccuracy \n   0.896 \n\n\nMas adelante en el curso hablaremos de otros de estos descriptores. Una forma mas facil de interpretar la matriz de confusión es representarla gráficamente:\n\n\nCódigo\n# convertir a data frame\nconf_df &lt;- as.data.frame(mat_conf$table)\n\n# agregar totales por categoria\nconf_df$total &lt;-\n    sapply(conf_df$Reference, function(x)\n        sum(datos_multin$y ==\n                x))\n\n# calcular proporciones\nconf_df$proportion &lt;- conf_df$Freq / conf_df$total\n\nggplot(conf_df, aes(x = Reference, y = Prediction, fill = proportion)) +\n  geom_tile() + \n  coord_equal() + \n  scale_fill_distiller(palette = \"Greens\", direction = 1) + \n  geom_text(aes(label = round(proportion, 2)), color = \"black\", size = 3) + \n  labs(x = \"Observado\", y = \"Predicho\", fill = \"Proporción\")"
  },
  {
    "objectID": "regresion_logística_y_multinomial.html#ejercicio-2",
    "href": "regresion_logística_y_multinomial.html#ejercicio-2",
    "title": "Regresión logística y multinomial",
    "section": "2.6 Ejercicio 2",
    "text": "2.6 Ejercicio 2\nPara este ejercicio puedo utilizar el código del ejercicio 1 para la evaluación de los modelos.\n2.1 Utilice los datos iris para ajustar una regresión multinomial utilizando como respuesta el nombre de la especie (“Species”) y el largo del pétalo como predictor. ¿Cuál es la precisión del modelo?\n2.2 Añada un segundo predictor (e.g. largo del sépalo) y evalúe la precisión del modelo.\n2.3 Ahora use todas las variables como predictoras y evalúe la precisión del modelo.\n2.4 ¿Cual es el modelo con el mejor desempeño? ¿Qué variables parecen ser más importantes para predecir la especie de la flor?"
  },
  {
    "objectID": "reportes_dinamicos.html",
    "href": "reportes_dinamicos.html",
    "title": "Reportes dinámicos",
    "section": "",
    "text": "Familiarizarse con el formato para reportes dinámicos Rmarkdown/quarto\nSer capaz de documentar el manejo y análisis de datos en R usando Rmarkdown/quarto\nPaquetes a utilizar en este manual:\nCódigo\n# cargar paquetes\n\nsketchy::load_packages(\n    c(\n      \"leaflet\",\n      \"remotes\",\n      \"hadley/emo\",\n      \"maRce10/sketchy\",\n      \"knitr\",\n      \"rmarkdown\",\n      \"rmdformats\",\n      \"revealjs\",\n      \"rticles\",\n      \"tufte\"\n    )\n)"
  },
  {
    "objectID": "reportes_dinamicos.html#ventajas-de-los-reportes-dinámicos-con-rmarkdownquarto",
    "href": "reportes_dinamicos.html#ventajas-de-los-reportes-dinámicos-con-rmarkdownquarto",
    "title": "Reportes dinámicos",
    "section": "1.1 Ventajas de los reportes dinámicos con Rmarkdown/quarto:",
    "text": "1.1 Ventajas de los reportes dinámicos con Rmarkdown/quarto:\n\nEl código R se puede incrustar en el informe, por lo que no es necesario mantener el informe y el código de R por separado\nIncluir el código R directamente en un informe proporciona una estructura intuitiva para reproducir los análisis\nEl texto del informe está escrito como texto normal, por lo que no se requieren conocimientos de programación (i.e. R o HTML) para comprenderlos\nEl resultado es un archivo HTML que incluye imágenes, bloques de código con los comandos de R, los resultados de estos códigos y texto. No se necesitan archivos adicionales, todo está incorporado en el archivo HTML.\nLos informes son fáciles de compartir por correo o publicarlos en línea\nEstos informes facilitan la colaboración y mejoran la reproducibilidad (entender los análisis es mucho mas fácil cuando hay texto explicativo, código de R, los resultados del código y los gráficos en un mismo archivo)\nSe actualizan fácilmente para incluir nuevos análisis y/o integrar nuevos datos"
  },
  {
    "objectID": "reportes_dinamicos.html#ejercicio-1",
    "href": "reportes_dinamicos.html#ejercicio-1",
    "title": "Reportes dinámicos",
    "section": "2.1 Ejercicio 1",
    "text": "2.1 Ejercicio 1\nCree un nuevo archivo R Markdown o quarto, escriba algunas líneas de texto y haga clic en “Knit” para ver cómo se verá su reporte."
  },
  {
    "objectID": "reportes_dinamicos.html#crear-encabezados-de-varios-tamaños",
    "href": "reportes_dinamicos.html#crear-encabezados-de-varios-tamaños",
    "title": "Reportes dinámicos",
    "section": "3.1 Crear encabezados de varios tamaños",
    "text": "3.1 Crear encabezados de varios tamaños\nCódigo:\n\n\n\n\n\n\n\n\n\nResultado:"
  },
  {
    "objectID": "reportes_dinamicos.html#encabezado-2",
    "href": "reportes_dinamicos.html#encabezado-2",
    "title": "Reportes dinámicos",
    "section": "4.1 Encabezado 2",
    "text": "4.1 Encabezado 2\n\n4.1.1 Encabezado 3"
  },
  {
    "objectID": "reportes_dinamicos.html#opciones-del-texto",
    "href": "reportes_dinamicos.html#opciones-del-texto",
    "title": "Reportes dinámicos",
    "section": "4.2 Opciones del texto",
    "text": "4.2 Opciones del texto\n \nCódigo:\n\n\n\n\n\n\n\n\n\nResultado:\nHacer texto en negrita, itálico, tachado, o superíndice"
  },
  {
    "objectID": "reportes_dinamicos.html#añadir-una-imagen",
    "href": "reportes_dinamicos.html#añadir-una-imagen",
    "title": "Reportes dinámicos",
    "section": "4.3 Añadir una imagen",
    "text": "4.3 Añadir una imagen\nCódigo:\n\n\n\n\n\n\n\n\n\nResultado:\n\n\n\n\n\n\n\n\n\nTambién podemos añadir una imagen con la función include_graphics() del paquete knitr.\nCódigo:\n\n\nCódigo\ninclude_graphics(\"./images/rmarkdown_icon.png\")\n\n\nResultado:\n\n\n\n\n\n\n\n\n\nNote que esta opción nos permite hacer uso de los argumentos del bloque de código dedicados a la graficación, y po r tanto es una opción mas flexible."
  },
  {
    "objectID": "reportes_dinamicos.html#crear-enlaces-hyperlinks",
    "href": "reportes_dinamicos.html#crear-enlaces-hyperlinks",
    "title": "Reportes dinámicos",
    "section": "4.4 Crear enlaces (‘hyperlinks’)",
    "text": "4.4 Crear enlaces (‘hyperlinks’)\nCódigo:\n\n\n\n\n\n\n\n\n\nResultado:\nCrear un enlace a figshare"
  },
  {
    "objectID": "reportes_dinamicos.html#ejercicio-2",
    "href": "reportes_dinamicos.html#ejercicio-2",
    "title": "Reportes dinámicos",
    "section": "4.5 Ejercicio 2",
    "text": "4.5 Ejercicio 2\n2.1 Cree unos encabezados y sub-encabezados en su documento Rmarkdown\n2.2 Añada texto con algunas palabras en negrita y en italica\n2.3 Incruste una imagen de su organismo favorito (o un gif)\n2.4 Añada un enlace URL"
  },
  {
    "objectID": "reportes_dinamicos.html#incrustar-código",
    "href": "reportes_dinamicos.html#incrustar-código",
    "title": "Reportes dinámicos",
    "section": "4.6 Incrustar código",
    "text": "4.6 Incrustar código\nPara incrustar el código de R, tenemos que definir un área donde se encuentra el código. Esta ‘área’ se conoce como un bloque de código (o ‘chunk’) y se define mediante:\n\n\n\n\n\n\n\n\n\n \nObserve que el recuadro de R está en gris, mientras que el resto está en fondo blanco. Todo lo que se incluye en el segmento se evalúa y muestra de acuerdo con las especificaciones, aunque estas se pueden modificar.\n \nPodemos, por ejemplo, agregar una nueva columna al conjunto de datos de ejemplo de iris:\n\n\n\n\n\n\n\n\n\nResultado:\n\n\nCódigo\ndata(iris)\n\niris$random.var &lt;- rnorm(n = nrow(iris))\n\nhead(iris)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\nrandom.var\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n0.62156\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n1.48479\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n1.46319\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n1.07184\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n-0.71593\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n-0.52867\n\n\n\n\n\n\n \nCuando se procesa su documento, el segmento de código se muestra en un cuadro gris y los resultados de ese código se muestran en un cuadro blanco. ¿Qué pasa si solo desea que se muestre la salida de su código? ¿O que su código se muestre pero no se ejecute realmente? Hay argumentos que puede agregar a cada uno de sus bloques para especificar estas y otras opciones:"
  },
  {
    "objectID": "reportes_dinamicos.html#ocultar-código",
    "href": "reportes_dinamicos.html#ocultar-código",
    "title": "Reportes dinámicos",
    "section": "4.7 Ocultar código",
    "text": "4.7 Ocultar código\nAñadir el argumento echo=FALSE\nCódigo:\n\n\n\n\n\n\n\n\n\nResultado:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\nrandom.var\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n0.83488\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n-0.18477\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n0.59313\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n1.39341\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n-0.52272\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n0.89779\n\n\n\n\n\n\n \nPuede ver que el código está oculto pero se muestran los resultados.\nEsta guía sobre las opciones de los bloques de código puede ser muy útil:\n\n\n\n\n\n\n\n\n\n \nEn este enlace se detallan todos los argumentos disponibles para personalizar los bloques de código."
  },
  {
    "objectID": "reportes_dinamicos.html#incrustar-gráficos",
    "href": "reportes_dinamicos.html#incrustar-gráficos",
    "title": "Reportes dinámicos",
    "section": "4.8 Incrustar gráficos",
    "text": "4.8 Incrustar gráficos\nLos gráficos se pueden incrustar en documentos Rmarkdown simplemente usando funciones de graficación como lo haría en un script de R normal.\n\n4.8.0.1 Código:\n\n\n\n\n\n\n\n\n\n \n\n\n4.8.0.2 Resultado:"
  },
  {
    "objectID": "reportes_dinamicos.html#ejercicio-3",
    "href": "reportes_dinamicos.html#ejercicio-3",
    "title": "Reportes dinámicos",
    "section": "4.9 Ejercicio 3",
    "text": "4.9 Ejercicio 3\n3.1 Utilice los argumentos eval, collapse con diferentes valores (TRUE o FALSE) en un segmento donde corre head(iris). ¿Cómo afectan el resultado?\n3.2 Haga lo mismo con los argumentos out.width, fig.width,dpi y fig.height en un segmento que cree un gráfico. ¿Cómo afecta esta vez?"
  },
  {
    "objectID": "reportes_dinamicos.html#incrustar-código-de-r-en-el-texto",
    "href": "reportes_dinamicos.html#incrustar-código-de-r-en-el-texto",
    "title": "Reportes dinámicos",
    "section": "4.10 Incrustar código de R en el texto",
    "text": "4.10 Incrustar código de R en el texto\nEs posible que haya notado a lo largo de este tutorial que tengo pequeños fragmentos de texto que parecen “bloques de código”. Esto se conoce como incrustación de código en texto.\nEsto se puede hacer de dos maneras:\n\n4.10.0.1 1.Dar un texto con la apariencia de un segmento de código:\n\n\n\n4.10.0.2 Código:\n\n\n\n\n\n\n\n\n\n \n\n\n4.10.0.3 Resultado:\nEl promedio del largo del sépalo es mean(iris$Sepal.Length)\n \n\n\n4.10.0.4 2. Evaluar el código en el texto\n\n\n4.10.0.5 Código:\n\n\n\n\n\n\n\n\n\n\n\n4.10.0.6 Resultado:\nEl promedio del largo de sépalo para setosa es 5.006."
  },
  {
    "objectID": "reportes_dinamicos.html#metadatos-yaml",
    "href": "reportes_dinamicos.html#metadatos-yaml",
    "title": "Reportes dinámicos",
    "section": "5.1 Metadatos (YAML)",
    "text": "5.1 Metadatos (YAML)\nHay tres componentes básicos de un documento de R Markdown: los metadatos, el texto y el código. Los metadatos se escriben entre el par de tres guiones (“- - -”) generalmente al inicio del documento. La sintaxis de los metadatos es YAML (YAML Ain’t Markup Language), por lo que a veces también se denomina metadatos YAML. La sangría es importante en YAML, así que debe añadirla a los subcampos (con respecto a un campo superior).\nEste encabezado muestra las opciones mas comúnmente usadas en los metadatos YAML:\n\n\nCódigo\n---\ntitle: \"Un titulo descriptivo y sin faltas ortograficas\"\nauthor: \"Marcelo Araya\"\ndate: \"`r Sys.Date()`\"\noutput: # Varios outputs mostrados solo para el ejemplo\n  html_document:\n    fig_caption: yes\n    number_sections: yes\n    toc: yes\n    toc_float: yes\n    df_print: paged\n---\n\n\nEn este enlace se explican en detalle las opciones disponibles en el encabezado YAML de archivos Rmarkdown."
  },
  {
    "objectID": "reportes_dinamicos.html#emojis",
    "href": "reportes_dinamicos.html#emojis",
    "title": "Reportes dinámicos",
    "section": "5.2 Emojis",
    "text": "5.2 Emojis\nEl paquete emo permite añadir emojis al evaluar un código:\n\nCódigo\nemo::ji(\"costa_rica\")\n\n🇨🇷\nTambien se puede incrustar en el texto 🇨🇷, como lo vimos mas arriba ⬆️ (ji(\"up_arrow\"))"
  },
  {
    "objectID": "reportes_dinamicos.html#cuadros-con-knitrkable",
    "href": "reportes_dinamicos.html#cuadros-con-knitrkable",
    "title": "Reportes dinámicos",
    "section": "5.3 Cuadros con knitr::kable",
    "text": "5.3 Cuadros con knitr::kable\nEl paquete knitr también provee una función para mostrar datos tabulares de forma ordenada y ‘limpia’ en los reportes dínamicos:\n\n\nCódigo\nknitr::kable(iris[1:10, ])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\nrandom.var\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n0.83488\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n-0.18477\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n0.59313\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n1.39341\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n-0.52272\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n0.89779\n\n\n4.6\n3.4\n1.4\n0.3\nsetosa\n1.14572\n\n\n5.0\n3.4\n1.5\n0.2\nsetosa\n-0.96932\n\n\n4.4\n2.9\n1.4\n0.2\nsetosa\n-0.20972\n\n\n4.9\n3.1\n1.5\n0.1\nsetosa\n0.46887\n\n\n\n\n\n\n \nEl paquete kableExtra complementa esta función con muchas herramientas para personalizar el formato de las tablas en reportes dinámicos en R."
  },
  {
    "objectID": "reportes_dinamicos.html#opciones-adicionales-en-knitr",
    "href": "reportes_dinamicos.html#opciones-adicionales-en-knitr",
    "title": "Reportes dinámicos",
    "section": "5.4 Opciones adicionales en knitr",
    "text": "5.4 Opciones adicionales en knitr\nEl argumento opts_knit de knitr permite definir opciones globales (aplicables a todos los bloques a menos que se re-definan):\n\n\nCódigo\nopts_chunk$set(root.dir = \"..\", eval = TRUE, echo = FALSE)"
  },
  {
    "objectID": "reportes_dinamicos.html#presentaciones-y-otros-opciones-de-formato",
    "href": "reportes_dinamicos.html#presentaciones-y-otros-opciones-de-formato",
    "title": "Reportes dinámicos",
    "section": "5.5 Presentaciones y otros opciones de formato",
    "text": "5.5 Presentaciones y otros opciones de formato\nNote en la ventana de creación de un nuevo documento Rmarkdown las opciones adicionales de formato:\n\n\n\n\n\n\n\n\n\n \nLos reportes dinámicos se pueden generar en otros formatos incluyendo presentaciones, pdf y documentos de word."
  },
  {
    "objectID": "reportes_dinamicos.html#plantillas-de-rmarkdown",
    "href": "reportes_dinamicos.html#plantillas-de-rmarkdown",
    "title": "Reportes dinámicos",
    "section": "5.6 Plantillas de Rmarkdown",
    "text": "5.6 Plantillas de Rmarkdown\nEl paquete Rmarkdown puede generar resultados en HTML, PDF, MS Word, viñetas de paquetes de R, presentaciones Beamer y HTML5. Los formatos adicionales (o ‘variantes’ de estos formatos) están disponibles en otros paquetes de R. Algunos de esos paquetes son:\n\nrmdformats\nreveljs\nartículos\ntufte\n\nUna vez estos paquetes han sido instalados, los nuevos formatos estarán disponibles a través del nuevo cuadro de diálogo Rmarkdown:"
  },
  {
    "objectID": "reportes_dinamicos.html#html-widgets",
    "href": "reportes_dinamicos.html#html-widgets",
    "title": "Reportes dinámicos",
    "section": "6.1 HTML widgets",
    "text": "6.1 HTML widgets\nLos HTML Widgets se implementan con el paquete R htmlwidgets, que conecta herramientas de JavaScript que crean aplicaciones interactivas, como gráficos y tablas Se han desarrollado varios paquetes que emplean HTML widgets como dygraphs, DT y leaflet. En este sitio (https://www.htmlwidgets.org) se muestran una variedad de widgets disponibles así como instrucciones de como desarrollarlos.\nEl siguiente código utiliza el paquete leaflet para generar un mapa interactivo:\n\nCódigo\nsketchy::load_packages(\"leaflet\")\n\nll_map &lt;- leaflet()\n\nll_map &lt;- addTiles(map = ll_map)\n\nll_map &lt;-\n  setView(\n    map = ll_map,\n    lat = 5.527448,\n    lng = -87.057245,\n    zoom = 13\n  )\n\naddPopups(\n  map = ll_map,\n  lat = 5.527448,\n  lng = -87.057245,\n  popup = 'Isla del Coco'\n)"
  },
  {
    "objectID": "reportes_dinamicos.html#aplicaciones-shiny",
    "href": "reportes_dinamicos.html#aplicaciones-shiny",
    "title": "Reportes dinámicos",
    "section": "6.2 Aplicaciones shiny",
    "text": "6.2 Aplicaciones shiny\nEl paquete shiny crea aplicaciones web interactivas en R. Para llamar al código shiny desde un documento R Markdown, agregue ‘runtime’: shiny a los metadatos YAML, como en este ejemplo:\n\n\nCódigo\n---\ntitle: \"Documento Shiny\"\noutput: html_document\nruntime: shiny\n---\n\n\n \nEl siguiente código crea una aplicación shiny dentro del documento Rmarkdown:\n\n\nCódigo\nsketchy::load_packages(\"shiny\")\n\nui &lt;- fluidPage(\n  \n  titlePanel(\"Ejemplo\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(inputId = \"bins\",\n                  label = \"Numero de barras:\",\n                  min = 1,\n                  max = 50,\n                  value = 30)\n      \n    ),\n    mainPanel(\n      plotOutput(outputId = \"distPlot\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n \n  output$distPlot &lt;- renderPlot({\n    \n    x    &lt;- faithful$waiting\n    bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n    \n    hist(x, breaks = bins, col = \"#3E4A89FF\", border = \"white\",\n         xlab = \"Tiempo de espera para la siguiente erupcion\",\n         main = \"Histograma del tiempo de espera\")\n  })\n}\n\n# Crear Shiny app\nshinyApp(ui = ui, server = server)\n\n\n \nNote que esta aplicación no funciona en documentos estáticos de Rmarkdown. En el sitio https://shiny.rstudio.com/gallery pueden encontrar muchos ejemplos de aplicaciones shiny. Estas aplicaciones son complejas de incluir en archivos auto-contenidos y por ello no son tan amigables para reportes dinámicos como los que podemos generar con R markdown."
  },
  {
    "objectID": "reportes_dinamicos.html#proyectos-de-rstudio",
    "href": "reportes_dinamicos.html#proyectos-de-rstudio",
    "title": "Reportes dinámicos",
    "section": "8.1 Proyectos de Rstudio",
    "text": "8.1 Proyectos de Rstudio\nLos proyectos de Rstudio crean carpetas donde se guardan los archivos relacionados a un análisis específico (código y datos) y hacen de esta carpeta el directorio de trabajo por defecto cuando se abre el proyecto. Se pueden crear de esta forma:\n\n\n\n\n\n\n\n\n\nLuego aparecera una seria de ventanas donde pueden escoger que tipo de proyecto y el nombre de este:"
  },
  {
    "objectID": "reportes_dinamicos.html#compendios-de-investigación",
    "href": "reportes_dinamicos.html#compendios-de-investigación",
    "title": "Reportes dinámicos",
    "section": "8.2 Compendios de investigación",
    "text": "8.2 Compendios de investigación\nLos compendios de investigación son estructuras de carpetas pre-definidas que permiten seguir un orden lógico e intuitivo para organizar los archivos usados y generados en un análisis de datos de un proyecto de investigación. El paquete sketchy genera estos compendios, permitiendo al usuario seleccionar entre una gama de compendios comunes en la comunidad científica. Este ejemplo crea el compendio básico (uno de los 14 que vienen con el paquete):\n\n\nCódigo\npath &lt;- tempdir()\n\nmake_compendium(\n  name = \"proyecto_x\",\n  path = path,\n  format = \"basic\",\n  Rproj = TRUE\n)\n\n\nCreating directories ...\nproyecto_x\n│   \n├── data/  \n│   ├── processed/  # modified/rearranged data\n│   └── raw/  # original data\n├── manuscript/  # manuscript/poster figures\n├── output/  # all non-data products of data analysis\n└── scripts/  # code\nDone.\n\n\nEl paquete crea archivos Rmarkdown con plantillas para el análisis de datos (carpeta “scripts”) y escritura de manuscritos (carpeta “manuscript”). Corra path en la consola de R para ver la dirección de la carpeta donde se creo el compendio."
  },
  {
    "objectID": "reportes_dinamicos.html#ejercicio-4",
    "href": "reportes_dinamicos.html#ejercicio-4",
    "title": "Reportes dinámicos",
    "section": "8.3 Ejercicio 4",
    "text": "8.3 Ejercicio 4\n\nCree un reporte dinámico que incluya un mapa dinámico de Costa Rica usando el paquete leaflet\nReproduzca alguno de los ejemplos disponibles en el sitio del paquete dygraphs (https://rstudio.github.io/dygraphs/) e incrústelo en su reporte dinámico\nInstale el paquete kableExtra y incruste en su reporte el códifo de ejemplo en la documentación de la función kable_styling() de ese paquete\nCree una presentación Rmarkdown utilizando la opción “Presentation” en la ventana de creación\nGenere un reporte dinámico en formato PDF\nCree un proyecto de Rstudio para organizar los contenidos del curso\nCree un compendio de investigación con sketchy"
  },
  {
    "objectID": "reportes_dinamicos.html#referencias",
    "href": "reportes_dinamicos.html#referencias",
    "title": "Reportes dinámicos",
    "section": "8.4 Referencias",
    "text": "8.4 Referencias\n\nR Markdown: The Definitive Guide\nRmarkdown custom format\nRmarkdown website\nJacolien van Rij: Rmarkdown variants\nDanielle Quinn: R Lessons\nVaidyanathan, Ramnath, Yihui Xie, JJ Allaire, Joe Cheng, and Kenton Russell. 2019. Htmlwidgets: HTML Widgets for R. https://github.com/ramnathv/htmlwidgets.\n\n\n\nInformación de la sesión\n\n\nR version 4.4.1 (2024-06-14)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 22.04.4 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0 \nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=es_CR.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=es_CR.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=es_CR.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=es_CR.UTF-8 LC_IDENTIFICATION=C       \n\ntime zone: America/Costa_Rica\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] tufte_0.13       rticles_0.27     revealjs_0.9     rmdformats_1.0.4\n [5] rmarkdown_2.28   knitr_1.48       sketchy_1.0.3    emo_0.0.0.9000  \n [9] remotes_2.5.0    leaflet_2.2.2   \n\nloaded via a namespace (and not attached):\n [1] jsonlite_1.8.9      compiler_4.4.1      crayon_1.5.3       \n [4] stringr_1.5.1       assertthat_0.2.1    jquerylib_0.1.4    \n [7] yaml_2.3.10         fastmap_1.2.0       R6_2.5.1           \n[10] generics_0.1.3      htmlwidgets_1.6.4   bookdown_0.40      \n[13] lubridate_1.9.3     xaringanExtra_0.8.0 rlang_1.1.4        \n[16] stringi_1.8.4       xfun_0.48           timechange_0.3.0   \n[19] cli_3.6.3           magrittr_2.0.3      crosstalk_1.2.1    \n[22] digest_0.6.37       rstudioapi_0.16.0   packrat_0.9.2      \n[25] lifecycle_1.0.4     vctrs_0.6.5         evaluate_1.0.1     \n[28] glue_1.8.0          purrr_1.0.2         tools_4.4.1        \n[31] htmltools_0.5.8.1"
  },
  {
    "objectID": "modelos_de_arboles.html",
    "href": "modelos_de_arboles.html",
    "title": "Modelos basados en árboles",
    "section": "",
    "text": "Entender como se construyen los árboles de decisión\nFamiliarizarse con los principales métodos de regresión y clasificación basados en ensamblado de árboles\nAprender a aplicar estos métodos en R\nPaquetes a utilizar en este manual:\nCódigo\n# instalar/cargar paquetes\nsketchy::load_packages(\n  c(\"ggplot2\", \n    \"viridis\", \n    \"caret\",\n    \"ISLR\",\n    \"rpart\",\n    \"rpart.plot\",\n    \"tree\",\n    \"randomForest\",\n    \"xgboost\"\n   )\n  )\n\n\nLoading required package: caret\n\n\nLoading required package: lattice\nLos métodos basados en árboles estratifican o subdividen el espacio predictor en en una serie de regiones simples. Dado que el conjunto de reglas de división utilizadas para segmentar el espacio predictor puede representarse en un árbol, este tipo de enfoques se conocen como métodos de árboles de decisión. Los árboles de decisión se utilizan tanto para clasificación como para regresión. Su ventaja radica en la simplicidad de interpretación y en su capacidad para capturar interacciones no lineales entre predictores."
  },
  {
    "objectID": "modelos_de_arboles.html#ajuste-del-modelo",
    "href": "modelos_de_arboles.html#ajuste-del-modelo",
    "title": "Modelos basados en árboles",
    "section": "1.1 Ajuste del modelo",
    "text": "1.1 Ajuste del modelo\nEl árbol que se genera para estos datos en realidad es mucho mas complejo y contiene mas divisiones de los datos en mas estratos. Los paquetes rpart y rpart.plot nos permite ajustar y visualizar estos modelos de una forma muy amigable:\n\n\nCódigo\n# Ajustar un árbol de regresión\narbol_regresion &lt;- rpart::rpart(LogSalary ~ Years + Hits, data = Hitters)\n\n# Visualizar el árbol\nrpart.plot::rpart.plot(arbol_regresion, extra = 101)\n\n\n\n\n\n\n\n\n\nLa complejidad del árbol la podemos controlar mediante el parámetro cp (“complexity parameter”). Un valor más alto de cp resulta en un árbol más simple (con menos divisiones o reglas), mientras que un valor más bajo permite árboles más grandes y complejos. El objetivo es encontrar un equilibrio entre un árbol que sea suficientemente complejo para capturar patrones importantes en los datos, pero no tan complejo que incurra en sobreajuste. Por ejemplo, este es un árbol con un valor de cp de 0.05:\n\n\nCódigo\narbol_cp.05 &lt;- rpart(LogSalary ~ Years + Hits,\n                     data = Hitters,\n                     control = rpart.control(cp = 0.05))\n\nrpart.plot(arbol_cp.05, extra = 101)\n\n\n\n\n\n\n\n\n\nEste en cambio tiene un valor de cp de 0.001:\n\n\nCódigo\narbol_cp.001 &lt;- rpart(LogSalary ~ Years + Hits, data = Hitters, control = rpart.control(cp = 0.001))\n\nrpart.plot(arbol_cp.001, extra = 101)"
  },
  {
    "objectID": "modelos_de_arboles.html#validación-cruzada",
    "href": "modelos_de_arboles.html#validación-cruzada",
    "title": "Modelos basados en árboles",
    "section": "1.2 Validación cruzada",
    "text": "1.2 Validación cruzada\nAfortunadamente la librería caret nos permite realizar validación cruzada para encontrar el mejor valor de cp para nuestro modelo. Podemos utilzar cualquiera de los métodos vistos en el manual de “sobreajuste y entrenamiento de modelos”. En este caso usamos el método de “dejar uno afuera” (LOOCV) para optimizar el valor de cp:\n\n\nCódigo\n# Configuración de validación cruzada\nset.seed(42) # Para reproducibilidad\n# Validación cruzada\ntrain_control &lt;- trainControl(method = \"LOOCV\")\n\n# Entrenar el modelo de árbol usando caret\nmodelo_arbol &lt;- train(\n  LogSalary ~ Years + Hits,\n  data = Hitters,\n  method = \"rpart\", # Árbol de decisión\n  trControl = train_control,\n  tuneLength = 10 # Número de combinaciones de parámetros a probar\n)         \n\n# Resumen del modelo ajustado\nprint(modelo_arbol)\n\n\nCART \n\n263 samples\n  2 predictor\n\nNo pre-processing\nResampling: Leave-One-Out Cross-Validation \nSummary of sample sizes: 262, 262, 262, 262, 262, 262, ... \nResampling results across tuning parameters:\n\n  cp         RMSE     Rsquared  MAE    \n  0.0042120  0.60271  0.547947  0.43523\n  0.0046796  0.59658  0.555573  0.42811\n  0.0085782  0.59364  0.557312  0.42568\n  0.0096474  0.59458  0.554649  0.42884\n  0.0110721  0.59198  0.558190  0.42732\n  0.0169020  0.58909  0.562176  0.43504\n  0.0183127  0.59250  0.555813  0.44211\n  0.0444602  0.62855  0.501931  0.47958\n  0.1145455  0.71442  0.361503  0.58884\n  0.4445745  0.97762  0.016381  0.87625\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.016902.\n\n\nEl mejor valor de cp encontrado por la validación cruzada es 0.00421 y un valor de RMSE de 0.60271. Podemos extraer y visualizar el árbol final así:\n\n\nCódigo\n# extraer mejor modelo\nmejor_modelo &lt;- modelo_arbol$finalModel\n\n# Visualizar el árbol final\nrpart.plot(mejor_modelo, extra = 101)"
  },
  {
    "objectID": "modelos_de_arboles.html#comparación-con-modelos-lineales",
    "href": "modelos_de_arboles.html#comparación-con-modelos-lineales",
    "title": "Modelos basados en árboles",
    "section": "1.3 Comparación con modelos lineales",
    "text": "1.3 Comparación con modelos lineales\nCon el siguiente gráfico podemos comparar el comportamiento de un modelo lineal con el de un árbol de regresión, usando un ejemplo de clasificación en dos dimensiones. En este ejemplo en el que la verdadera frontera de decisión es lineal, y está indicada por las regiones sombreadas. En la fila superior se ilustra como un enfoque clásico que asume una frontera lineal (izquierda) superará a un árbol de decisión que realiza divisiones paralelas a los ejes (derecha). En la fila inferior la verdadera frontera de decisión es no lineal. En este caso, un modelo lineal no puede capturar la verdadera frontera de decisión (izquierda), mientras que un árbol de decisión tiene éxito (derecha).\n\nTomado de Gareth et al 2013"
  },
  {
    "objectID": "modelos_de_arboles.html#ejercicio-1",
    "href": "modelos_de_arboles.html#ejercicio-1",
    "title": "Modelos basados en árboles",
    "section": "1.4 Ejercicio 1",
    "text": "1.4 Ejercicio 1\n\nUtilice un árbol de decisión de regresión para resolver el ejercicio 1 de la tarea 3.\nRealice la validación cruzada con el método de remuestreo de “boostrap” para entrenar el modelo."
  },
  {
    "objectID": "modelos_de_arboles.html#ajuste-del-modelo-1",
    "href": "modelos_de_arboles.html#ajuste-del-modelo-1",
    "title": "Modelos basados en árboles",
    "section": "2.1 Ajuste del modelo",
    "text": "2.1 Ajuste del modelo\nAjustaremos un árbol de clasificación simple utilizando la librería rpart y visualizaremos el árbol generado.\n\n\nCódigo\n# Ajustar un árbol de clasificación\narbol_clasificacion &lt;- rpart(High ~ Price + Advertising + ShelveLoc + Age, \n                             data = Carseats, \n                             method = \"class\")\n\n# Visualizar el árbol\nrpart.plot(arbol_clasificacion, extra = 104, fallen.leaves = TRUE, shadow.col = \"gray\")\n\n\n\n\n\n\n\n\n\nEl árbol resultante muestra cómo los datos se dividen en regiones basadas en los predictores. Por ejemplo, el predictor más importante puede ser Price, donde precios más bajos están asociados con mayores ventas.\nEn el gráfico los nodos terminales indican la clase predicha (Yes o No) y el porcentaje de datos que pertenecen a esa clase. Las divisiones están basadas en reglas como Price &gt;= 93."
  },
  {
    "objectID": "modelos_de_arboles.html#evaluación",
    "href": "modelos_de_arboles.html#evaluación",
    "title": "Modelos basados en árboles",
    "section": "2.2 Evaluación",
    "text": "2.2 Evaluación\nPara evaluar el desempeño del modelo, dividiremos los datos en conjuntos de entrenamiento y prueba y generaremos una matriz de confusión:\n\n\nCódigo\n# Dividir en conjunto de entrenamiento y prueba\nset.seed(42)\ntrain_index &lt;- sample(seq_len(nrow(Carseats)), size = 0.7 * nrow(Carseats))\ntrain_data &lt;- Carseats[train_index, ]\ntest_data &lt;- Carseats[-train_index, ]\n\n# Ajustar árbol con datos de entrenamiento\narbol_train &lt;- rpart(High ~ Price + Advertising + ShelveLoc + Age, \n                     data = train_data, \n                     method = \"class\")\n\n# Predicciones en el conjunto de prueba\npredicciones &lt;- predict(arbol_train, test_data, type = \"class\")\n\n# Matriz de confusión\ncaret::confusionMatrix(test_data$High, predicciones)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction No Yes\n       No  64  10\n       Yes 16  30\n                                        \n               Accuracy : 0.783         \n                 95% CI : (0.699, 0.853)\n    No Information Rate : 0.667         \n    P-Value [Acc &gt; NIR] : 0.00351       \n                                        \n                  Kappa : 0.53          \n                                        \n Mcnemar's Test P-Value : 0.32680       \n                                        \n            Sensitivity : 0.800         \n            Specificity : 0.750         \n         Pos Pred Value : 0.865         \n         Neg Pred Value : 0.652         \n             Prevalence : 0.667         \n         Detection Rate : 0.533         \n   Detection Prevalence : 0.617         \n      Balanced Accuracy : 0.775         \n                                        \n       'Positive' Class : No"
  },
  {
    "objectID": "modelos_de_arboles.html#validación-cruzada-1",
    "href": "modelos_de_arboles.html#validación-cruzada-1",
    "title": "Modelos basados en árboles",
    "section": "2.3 Validación Cruzada",
    "text": "2.3 Validación Cruzada\nPara seleccionar el mejor valor de cp, usamos validación cruzada con la librería caret:\n\n\nCódigo\n# Configuración de validación cruzada\nset.seed(42)\ncontrol &lt;- trainControl(method = \"cv\", number = 10)\n\n# Ajustar modelo con validación cruzada\nmodelo_cv &lt;- train(\n  High ~ Price + Advertising + ShelveLoc + Age, \n  data = train_data,\n  method = \"rpart\",\n  trControl = control\n)\n\n# Mostrar los resultados\nprint(modelo_cv)\n\n\nCART \n\n280 samples\n  4 predictor\n  2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 252, 252, 252, 253, 251, 253, ... \nResampling results across tuning parameters:\n\n  cp        Accuracy  Kappa   \n  0.050847  0.69182   0.356741\n  0.080508  0.66680   0.296495\n  0.254237  0.60375   0.089011\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.050847.\n\n\nCódigo\n# Visualizar el árbol final\nrpart.plot(modelo_cv$finalModel, extra = 104)\n\n\n\n\n\n\n\n\n\nEl valor óptimo de cp es 0.05085. El árbol final representa la mejor combinación de simplicidad y precisión según la validación cruzada.\nAhora podemos evaluar el desempeño del mejor modelo:\n\n\nCódigo\narbol_train_cv &lt;-\n  rpart(\n    High ~ Price + Advertising + ShelveLoc + Age,\n    data = Carseats,\n    control = rpart.control(cp = modelo_cv$bestTune$cp),\n    method = \"class\"\n  )\n\n# Predicciones en el conjunto de prueba\npredicciones &lt;- predict(arbol_train_cv, Carseats, type = \"class\")\n\n# Matriz de confusión\ncaret::confusionMatrix(Carseats$High, predicciones)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No  203  33\n       Yes  66  98\n                                        \n               Accuracy : 0.752         \n                 95% CI : (0.707, 0.794)\n    No Information Rate : 0.672         \n    P-Value [Acc &gt; NIR] : 0.000301      \n                                        \n                  Kappa : 0.472         \n                                        \n Mcnemar's Test P-Value : 0.001299      \n                                        \n            Sensitivity : 0.755         \n            Specificity : 0.748         \n         Pos Pred Value : 0.860         \n         Neg Pred Value : 0.598         \n             Prevalence : 0.672         \n         Detection Rate : 0.507         \n   Detection Prevalence : 0.590         \n      Balanced Accuracy : 0.751         \n                                        \n       'Positive' Class : No"
  },
  {
    "objectID": "modelos_de_arboles.html#ejercicio-2",
    "href": "modelos_de_arboles.html#ejercicio-2",
    "title": "Modelos basados en árboles",
    "section": "2.4 Ejercicio 2",
    "text": "2.4 Ejercicio 2\n\nAjusta un árbol de clasificación utilizando otras variables predictoras (por ejemplo, Income o Population).\nRealiza validación cruzada con un enfoque distinto (por ejemplo, validación LOOCV)."
  },
  {
    "objectID": "modelos_de_arboles.html#random-forest",
    "href": "modelos_de_arboles.html#random-forest",
    "title": "Modelos basados en árboles",
    "section": "3.1 Random Forest",
    "text": "3.1 Random Forest\nEl método de Random Forest es una extensión de los árboles de decisión que utiliza una combinación de múltiples árboles para mejorar la precisión y la robustez del modelo. Cada árbol se ajusta utilizando una muestra aleatoria con reemplazo (bootstrap) de los datos de entrenamiento, y en cada división del árbol se selecciona un subconjunto aleatorio de predictores. Esta estrategia reduce la correlación entre los árboles, mejorando el rendimiento general. A diferencia de un único árbol de decisión, los Random Forest son menos propensos a sobreajustar los datos.\nLos principales hiperparámetros del Random Forest son los siguientes:\n\nNúmero de árboles (ntree):Determina cuántos árboles se generarán en el bosque. Más árboles generalmente mejoran la estabilidad y la capacidad de generalización, pero aumentan el tiempo de cálculo. Valor típico: 500 o 100.\nNúmero de predictores seleccionados por división (mtry): Define cuántas variables se seleccionan aleatoriamente de todas las disponibles para considerar en cada división de nodo. Valores más bajos aumentan la diversidad entre los árboles. Valores más altos hacen que los árboles sean más similares. Valor típico en clasificación: raíz cuadrada del número total de predictores. Valor típico en regresión: Total de predictores / 3.\nTamaño mínimo de los nodos terminales (nodesize): Controla el número mínimo de observaciones en los nodos terminales. Valores pequeños permiten modelos más complejos. Valores grandes simplifican los árboles y previenen el sobreajuste. Valor típico: Clasificación: 1. Regresión: 5.\n\nEstos hiperparámetros se pueden ajustar utilizando técnicas como búsqueda en cuadrícula (grid search) o búsqueda aleatoria (random search) en el paquete caret."
  },
  {
    "objectID": "modelos_de_arboles.html#ajuste-de-un-modelo-random-forest",
    "href": "modelos_de_arboles.html#ajuste-de-un-modelo-random-forest",
    "title": "Modelos basados en árboles",
    "section": "3.2 Ajuste de un modelo Random Forest",
    "text": "3.2 Ajuste de un modelo Random Forest\nEn esta sección, utilizaremos el conjunto de datos heart para clasificar si un paciente tiene enfermedad cardíaca (sick) basada en variables clínicas como el colesterol, la frecuencia cardíaca máxima, entre otras. El conjunto de datos incluye observaciones categóricas y numéricas, lo que lo hace ideal para ilustrar la flexibilidad de Random Forest.\nPrimero debemos leer los datos y darles el formato adecuado:\n\n\nCódigo\nheart &lt;- read.csv(\"https://raw.githubusercontent.com/maRce10/aprendizaje_estadistico_2024/refs/heads/master/data/heart_data.csv\")\n\n# Nueva Columna\nheart$sick &lt;- ifelse(heart$sick == 0, \"No\", \"Yes\")\n\n# hacerlo factor para q sea interpretado como categorico\nheart$sick &lt;- factor(heart$sick)\n\n# Eliminar datos faltantes\nheart &lt;- na.omit(heart)\n\n# revisar\nhead(heart)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\nsick\n\n\n\n\n63\n1\n1\n145\n233\n1\n2\n150\n0\n2.3\n3\n0\n6\nNo\n\n\n67\n1\n4\n160\n286\n0\n2\n108\n1\n1.5\n2\n3\n3\nYes\n\n\n67\n1\n4\n120\n229\n0\n2\n129\n1\n2.6\n2\n2\n7\nYes\n\n\n37\n1\n3\n130\n250\n0\n0\n187\n0\n3.5\n3\n0\n3\nNo\n\n\n41\n0\n2\n130\n204\n0\n2\n172\n0\n1.4\n1\n0\n3\nNo\n\n\n56\n1\n2\n120\n236\n0\n0\n178\n0\n0.8\n1\n0\n3\nNo\n\n\n\n\n\n\nEn este enlace podemos ver una descripción de los datos.\nAhora podemos ajustar un modelo Random Forest a los datos:\n\n\nCódigo\n# Ajustar modelo Random Forest\nset.seed(42) # Para reproducibilidad\nmodelo_rf &lt;- randomForest::randomForest(\n  sick ~ ., \n  data = heart, \n  importance = TRUE,  # Para calcular la importancia de las variables\n  ntree = 50000         # Número de árboles\n)\n\n# Resumen del modelo\nprint(modelo_rf)\n\n\n\nCall:\n randomForest(formula = sick ~ ., data = heart, importance = TRUE,      ntree = 50000) \n               Type of random forest: classification\n                     Number of trees: 50000\nNo. of variables tried at each split: 3\n\n        OOB estimate of  error rate: 16.5%\nConfusion matrix:\n     No Yes class.error\nNo  140  20     0.12500\nYes  29 108     0.21168\n\n\nEl resultado muestra la tasa de error para cada clase, así como el error general (Out-of-Bag error, OOB). Este error se calcula al predecir observaciones no incluidas en la muestra bootstrap utilizada para construir cada árbol, lo que proporciona una estimación interna de la precisión del modelo."
  },
  {
    "objectID": "modelos_de_arboles.html#importancia-de-las-variables",
    "href": "modelos_de_arboles.html#importancia-de-las-variables",
    "title": "Modelos basados en árboles",
    "section": "3.3 Importancia de las variables",
    "text": "3.3 Importancia de las variables\nUna ventaja de los Random Forest es que pueden calcular automáticamente la importancia de cada predictor en la clasificación o predicción. Esto se mide mediante la reducción en la pureza del nodo (Gini index) o la precisión del modelo al permutar aleatoriamente los valores de cada predictor.\n\n\nCódigo\n# Importancia de las variables\nimportancia &lt;- importance(modelo_rf)\nprint(importancia)\n\n\n               No      Yes MeanDecreaseAccuracy MeanDecreaseGini\nage       69.9604  47.3872              84.3381          12.9674\nsex      110.7576  63.3700             125.7804           4.6577\ncp       118.5269 172.5159             195.1832          17.8627\ntrestbps  16.2051   8.5533              17.8384          10.8094\nchol       5.5427 -21.7215             -10.0971          11.6011\nfbs       15.0411 -19.7773              -1.5309           1.3610\nrestecg    3.3443  14.8573              12.5993           2.9091\nthalach   91.8999  58.5110             107.8348          17.1350\nexang     43.4908  89.5156              94.4112           7.2705\noldpeak   95.4565 124.2312             155.5998          15.6410\nslope     22.9566  85.8175              80.4838           6.4293\nca       210.9774 182.5273             255.6418          17.9004\nthal     184.6603 165.7431             231.2559          18.8793\n\n\nCódigo\n# print ggplot gini importance\nggplot(importancia, aes(x = reorder(rownames(importancia), MeanDecreaseGini), y = MeanDecreaseGini)) +\n  geom_bar(stat = \"identity\", fill = viridis(10)[3]) +\n  coord_flip() +\n  labs(x = \"Variables\", y = \"Importancia (Mean Decrease Gini)\")\n\n\n\n\n\n\n\n\n\nLa gráfica de importancia muestra las variables que contribuyen más al modelo. En este ejemplo, podemos observar que ca, thal y cp son especialmente importantes para predecir si un paciente tiene enfermedad cardíaca."
  },
  {
    "objectID": "modelos_de_arboles.html#validación-cruzada-2",
    "href": "modelos_de_arboles.html#validación-cruzada-2",
    "title": "Modelos basados en árboles",
    "section": "3.4 Validación cruzada",
    "text": "3.4 Validación cruzada\nPodemos usar el paquete caret para realizar una búsqueda de hiperparámetros en Random Forest, optimizando el número de predictores considerados en cada división del árbol (mtry) mediante validación cruzada.\n\n\nCódigo\nlibrary(caret)\n\n# Configuración de validación cruzada\nset.seed(42)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10) # Validación cruzada 10-fold\n\n# Entrenar modelo usando caret\nmodelo_rf_caret &lt;- train(\n  sick ~ ., \n  data = heart, \n  method = \"rf\", \n  trControl = train_control, \n  tuneLength = 10 # Número de combinaciones de parámetros a probar\n)\n\n# Resultados\nprint(modelo_rf_caret)\n\n\nRandom Forest \n\n297 samples\n 13 predictor\n  2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 267, 267, 267, 267, 268, 268, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy  Kappa  \n   2    0.82517   0.64610\n   3    0.81816   0.63295\n   4    0.80161   0.60046\n   5    0.80828   0.61204\n   6    0.79126   0.57814\n   8    0.79805   0.59108\n   9    0.79816   0.59112\n  10    0.78805   0.57168\n  11    0.78805   0.57137\n  13    0.78805   0.57168\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 2.\n\n\nCódigo\n# Hiperarametros del mejor modelo\nmodelo_rf_caret$bestTune\n\n\n\n\n\n\nmtry\n\n\n\n\n2\n\n\n\n\n\n\nEl resultado muestra el mejor valor de mtry encontrado por validación cruzada y la precisión asociada. Este valor puede ser utilizado para ajustar un modelo final.\nPodemos evaluar el desempeño del modelo con los hiperparametros optimizados:\n\n\nCódigo\n# predecir el modelo en todos los datos\npredicciones &lt;- predict(modelo_rf_caret, heart)\n\n# matriz de confusion\nconfusionMatrix(predicciones, heart$sick)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No  160   1\n       Yes   0 136\n                                    \n               Accuracy : 0.997     \n                 95% CI : (0.981, 1)\n    No Information Rate : 0.539     \n    P-Value [Acc &gt; NIR] : &lt;2e-16    \n                                    \n                  Kappa : 0.993     \n                                    \n Mcnemar's Test P-Value : 1         \n                                    \n            Sensitivity : 1.000     \n            Specificity : 0.993     \n         Pos Pred Value : 0.994     \n         Neg Pred Value : 1.000     \n             Prevalence : 0.539     \n         Detection Rate : 0.539     \n   Detection Prevalence : 0.542     \n      Balanced Accuracy : 0.996     \n                                    \n       'Positive' Class : No"
  },
  {
    "objectID": "modelos_de_arboles.html#visualización-del-error-out-of-bag-oob",
    "href": "modelos_de_arboles.html#visualización-del-error-out-of-bag-oob",
    "title": "Modelos basados en árboles",
    "section": "3.5 Visualización del error Out-of-Bag (OOB)",
    "text": "3.5 Visualización del error Out-of-Bag (OOB)\nEl Random Forest proporciona una estimación del error OOB durante el ajuste, lo que permite analizar cómo se comporta el modelo conforme aumentan los árboles.\n\n\nCódigo\n# Error OOB\na &lt;- plot(\n  modelo_rf, \n  main = \"Error OOB en Random Forest\",\n  col = viridis(10)[3]\n)\n\n\n\n\n\n\n\n\n\nEste gráfico muestra cómo se estabiliza el error conforme se incrementa el número de árboles, lo que ayuda a determinar si se ha utilizado un número suficiente. El gráfico muestra 3 líneas: una para el error global y una para cada una de las categorías de la variable respuesta."
  },
  {
    "objectID": "modelos_de_arboles.html#ejercicio-3",
    "href": "modelos_de_arboles.html#ejercicio-3",
    "title": "Modelos basados en árboles",
    "section": "3.6 Ejercicio 3",
    "text": "3.6 Ejercicio 3\n\nUtilice Random Forest para resolver el ejercicio 4 de la tarea 3.\nRealice la validación cruzada con el método de remuestreo repetido (“repeated CV”) para entrenar el modelo.\nCalcule la matriz de confusión, la exactitud y el área bajo la curva para el modelo del punto anterior."
  },
  {
    "objectID": "modelos_de_arboles.html#boosting",
    "href": "modelos_de_arboles.html#boosting",
    "title": "Modelos basados en árboles",
    "section": "3.7 Boosting",
    "text": "3.7 Boosting\nEl método XGBoost (Extreme Gradient Boosting) es una implementación eficiente y optimizada de Boosting. A diferencia de los métodos de Random Forest, donde se genera un conjunto de árboles entrenados de manera independiente, XGBoost construye los árboles secuencialmente, corrigiendo los errores del modelo anterior en cada iteración. Cada árbol adicional se ajusta a los residuos (errores) del modelo anterior. Esta técnica tiende a ser más poderosa y flexible para resolver problemas de clasificación y regresión.\n\n3.7.1 Hiperparámetros\nLos principales hiperparámetros del XGBoost son los siguientes:\n\nNúmero de árboles (nrounds): Número total de árboles a generar. Más árboles pueden mejorar la precisión, pero también pueden aumentar el sobreajuste si no se regularizan adecuadamente. Valor típico: 100-1000 dependiendo del tamaño de los datos.\nTasa de aprendizaje (eta): Controla cuánto contribuye cada árbol nuevo en el modelo. Un valor más bajo puede mejorar la generalización, pero requiere más árboles. Valores más altos pueden acelerar el entrenamiento pero aumentar el riesgo de sobreajuste. Valor típico: 0.01-0.3.\nProfundidad máxima de los árboles (max_depth): Limita la profundidad máxima de los árboles. Valores más altos permiten que el árbol aprenda más patrones complejos, pero también pueden causar sobreajuste. Valor típico: 3-10.\nTamaño mínimo de los nodos (min_child_weight): Determina el número mínimo de muestras en un nodo para crear una nueva división. Valores más bajos permiten más divisiones, lo que puede llevar al sobreajuste. Valor típico: 1-10.\nSubmuestreo (subsample): Define la proporción de muestras que se usarán para entrenar cada árbol. La submuestreo ayuda a reducir el sobreajuste, especialmente cuando se tienen muchos datos. Valor típico: 0.5-1.0.\nSubmuestreo de características (colsample_bytree): Controla la fracción de características que se utilizan en cada árbol. Ayuda a reducir el sobreajuste al crear árboles más diversos. Valor típico: 0.5-1.0.\n\nEstos hiperparámetros se pueden ajustar utilizando técnicas como búsqueda en cuadrícula o búsqueda aleatoria (como en caret)."
  },
  {
    "objectID": "modelos_de_arboles.html#ajuste-de-un-modelo-xgboost",
    "href": "modelos_de_arboles.html#ajuste-de-un-modelo-xgboost",
    "title": "Modelos basados en árboles",
    "section": "3.8 Ajuste de un modelo XGBoost",
    "text": "3.8 Ajuste de un modelo XGBoost\nEn esta sección, utilizaremos nuevamente el conjunto de datos heart para clasificar si un paciente tiene enfermedad cardíaca (sick) basándonos en las mismas variables que usamos con Random Forest.\nPodemos ajustar el modelo XGBoost así:\n\n\nCódigo\n# Crear el conjunto de datos de entrenamiento\nX &lt;- as.matrix(heart[, -ncol(heart)])  # Variables predictoras\ny &lt;- as.numeric(heart$sick) - 1  # Convertir a valores 0 y 1\n\n# Configuración de los hiperparámetros\nparam &lt;- list(\n  objective = \"binary:logistic\", \n  eval_metric = \"logloss\", \n  max_depth = 6, \n  eta = 0.1, \n  subsample = 0.8, \n  colsample_bytree = 0.8\n)\n\n# Entrenamiento del modelo XGBoost\nmodelo_xgb &lt;- xgboost(\n  data = X, \n  label = y, \n  nrounds = 30, \n  params = param, \n  verbose = 0\n)\n\n# Ver el resultado del modelo\nprint(modelo_xgb)\n\n\n##### xgb.Booster\nraw: 58.7 Kb \ncall:\n  xgb.train(params = params, data = dtrain, nrounds = nrounds, \n    watchlist = watchlist, verbose = verbose, print_every_n = print_every_n, \n    early_stopping_rounds = early_stopping_rounds, maximize = maximize, \n    save_period = save_period, save_name = save_name, xgb_model = xgb_model, \n    callbacks = callbacks)\nparams (as set within xgb.train):\n  objective = \"binary:logistic\", eval_metric = \"logloss\", max_depth = \"6\", eta = \"0.1\", subsample = \"0.8\", colsample_bytree = \"0.8\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.evaluation.log()\n# of features: 13 \nniter: 30\nnfeatures : 13 \nevaluation_log:\n    iter train_logloss\n       1       0.64292\n       2       0.59494\n---                   \n      29       0.20190\n      30       0.19719"
  },
  {
    "objectID": "modelos_de_arboles.html#importancia-de-las-variables-1",
    "href": "modelos_de_arboles.html#importancia-de-las-variables-1",
    "title": "Modelos basados en árboles",
    "section": "3.9 Importancia de las variables",
    "text": "3.9 Importancia de las variables\nAl igual que con Random Forest, podemos ver la importancia de las variables utilizando el método xgb.importance():\n\n\nCódigo\n# Importancia de las variables\nimportancia_xgb &lt;- xgb.importance(colnames(X), model = modelo_xgb)\nprint(importancia_xgb)\n\n\n     Feature       Gain     Cover Frequency\n 1:       cp 0.19424016 0.1287904 0.0676983\n 2:     thal 0.17611240 0.1180942 0.0638298\n 3:       ca 0.16140347 0.1505455 0.0851064\n 4:      age 0.09613041 0.1147223 0.1779497\n 5:  oldpeak 0.09473140 0.1190074 0.1237911\n 6:  thalach 0.05883721 0.0744752 0.1083172\n 7:    slope 0.04642379 0.0437283 0.0406190\n 8:     chol 0.04580415 0.0686996 0.1083172\n 9: trestbps 0.04114643 0.0724745 0.0967118\n10:      sex 0.03351186 0.0375628 0.0406190\n11:    exang 0.02874264 0.0375218 0.0328820\n12:  restecg 0.02221044 0.0332299 0.0522244\n13:      fbs 0.00070564 0.0011479 0.0019342\n\n\nCódigo\n# Visualizar la importancia\nxgb.plot.importance(importance_matrix = importancia_xgb)"
  },
  {
    "objectID": "modelos_de_arboles.html#validación-cruzada-3",
    "href": "modelos_de_arboles.html#validación-cruzada-3",
    "title": "Modelos basados en árboles",
    "section": "3.10 Validación cruzada",
    "text": "3.10 Validación cruzada\nPodemos realizar una búsqueda de hiperparámetros usando validación cruzada con el paquete caret, al igual que con Random Forest. Sin embargo, en este caso, utilizaremos el método xgbTree para ajustar el modelo XGBoost. Ajustar este modelo es computacional intensivo y puede durar varios minutos en correr. Por lo tanto luego de correrlo lo guardamos como un archivo “RDS”. Estos archivos permiten guardar objetos de R de forma que se puedan leer nuevamente con facilidad manteniendo todos sus atributos:\n\n\nCódigo\n# Configuración de validación cruzada\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)  # Validación cruzada 10-fold\n\n# Entrenar el modelo con caret\nmodelo_xgb_caret &lt;- train(\n  sick ~ ., \n  data = heart, \n  method = \"xgbTree\", \n  trControl = train_control\n)\n\nsaveRDS(modelo_xgb_caret, \"modelo_xgb_caret.RDS\")\n\n\nAhora podemos leer el modelo y ver los resultados:\n\n\nCódigo\nmodelo_xgb_caret &lt;- readRDS(\"modelo_xgb_caret.RDS\")\n\n# Resultados\nprint(modelo_xgb_caret)\n\n\neXtreme Gradient Boosting \n\n297 samples\n 13 predictor\n  2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 268, 268, 267, 268, 267, 267, ... \nResampling results across tuning parameters:\n\n  eta  max_depth  colsample_bytree  subsample  nrounds  Accuracy  Kappa  \n  0.3  1          0.6               0.50        50      0.84839   0.69370\n  0.3  1          0.6               0.50       100      0.82506   0.64612\n  0.3  1          0.6               0.50       150      0.80494   0.60524\n  0.3  1          0.6               0.75        50      0.83517   0.66752\n  0.3  1          0.6               0.75       100      0.83851   0.67435\n  0.3  1          0.6               0.75       150      0.82172   0.64039\n  0.3  1          0.6               1.00        50      0.83839   0.67388\n  0.3  1          0.6               1.00       100      0.83506   0.66728\n  0.3  1          0.6               1.00       150      0.82517   0.64684\n  0.3  1          0.8               0.50        50      0.82851   0.65405\n  0.3  1          0.8               0.50       100      0.82184   0.64141\n  0.3  1          0.8               0.50       150      0.81172   0.62081\n  0.3  1          0.8               0.75        50      0.83172   0.66091\n  0.3  1          0.8               0.75       100      0.83517   0.66702\n  0.3  1          0.8               0.75       150      0.82506   0.64675\n  0.3  1          0.8               1.00        50      0.83851   0.67387\n  0.3  1          0.8               1.00       100      0.84184   0.68042\n  0.3  1          0.8               1.00       150      0.83195   0.66002\n  0.3  2          0.6               0.50        50      0.83529   0.66740\n  0.3  2          0.6               0.50       100      0.81529   0.62712\n  0.3  2          0.6               0.50       150      0.80483   0.60651\n  0.3  2          0.6               0.75        50      0.83161   0.65904\n  0.3  2          0.6               0.75       100      0.80471   0.60596\n  0.3  2          0.6               0.75       150      0.80126   0.59690\n  0.3  2          0.6               1.00        50      0.81529   0.62678\n  0.3  2          0.6               1.00       100      0.80506   0.60646\n  0.3  2          0.6               1.00       150      0.80184   0.60011\n  0.3  2          0.8               0.50        50      0.81494   0.62671\n  0.3  2          0.8               0.50       100      0.80483   0.60671\n  0.3  2          0.8               0.50       150      0.78828   0.57256\n  0.3  2          0.8               0.75        50      0.82506   0.64651\n  0.3  2          0.8               0.75       100      0.80816   0.61262\n  0.3  2          0.8               0.75       150      0.78161   0.56061\n  0.3  2          0.8               1.00        50      0.81529   0.62689\n  0.3  2          0.8               1.00       100      0.81874   0.63346\n  0.3  2          0.8               1.00       150      0.80172   0.59824\n  0.3  3          0.6               0.50        50      0.81862   0.63367\n  0.3  3          0.6               0.50       100      0.81184   0.61973\n  0.3  3          0.6               0.50       150      0.79517   0.58706\n  0.3  3          0.6               0.75        50      0.80517   0.60843\n  0.3  3          0.6               0.75       100      0.80828   0.61425\n  0.3  3          0.6               0.75       150      0.79839   0.59358\n  0.3  3          0.6               1.00        50      0.82851   0.65218\n  0.3  3          0.6               1.00       100      0.81851   0.63249\n  0.3  3          0.6               1.00       150      0.80816   0.61214\n  0.3  3          0.8               0.50        50      0.81172   0.62131\n  0.3  3          0.8               0.50       100      0.81149   0.62059\n  0.3  3          0.8               0.50       150      0.79460   0.58541\n  0.3  3          0.8               0.75        50      0.81184   0.62043\n  0.3  3          0.8               0.75       100      0.81517   0.62679\n  0.3  3          0.8               0.75       150      0.81207   0.62033\n  0.3  3          0.8               1.00        50      0.81161   0.62012\n  0.3  3          0.8               1.00       100      0.79483   0.58546\n  0.3  3          0.8               1.00       150      0.79483   0.58576\n  0.4  1          0.6               0.50        50      0.82529   0.64685\n  0.4  1          0.6               0.50       100      0.82195   0.63885\n  0.4  1          0.6               0.50       150      0.81839   0.63323\n  0.4  1          0.6               0.75        50      0.83517   0.66684\n  0.4  1          0.6               0.75       100      0.83517   0.66691\n  0.4  1          0.6               0.75       150      0.80851   0.61285\n  0.4  1          0.6               1.00        50      0.83862   0.67358\n  0.4  1          0.6               1.00       100      0.83517   0.66733\n  0.4  1          0.6               1.00       150      0.82851   0.65326\n  0.4  1          0.8               0.50        50      0.83851   0.67357\n  0.4  1          0.8               0.50       100      0.80161   0.60043\n  0.4  1          0.8               0.50       150      0.80828   0.61231\n  0.4  1          0.8               0.75        50      0.82529   0.64595\n  0.4  1          0.8               0.75       100      0.83517   0.66648\n  0.4  1          0.8               0.75       150      0.81149   0.61958\n  0.4  1          0.8               1.00        50      0.82851   0.65352\n  0.4  1          0.8               1.00       100      0.84195   0.68050\n  0.4  1          0.8               1.00       150      0.82517   0.64665\n  0.4  2          0.6               0.50        50      0.80138   0.59930\n  0.4  2          0.6               0.50       100      0.78793   0.57223\n  0.4  2          0.6               0.50       150      0.78816   0.57336\n  0.4  2          0.6               0.75        50      0.81506   0.62710\n  0.4  2          0.6               0.75       100      0.77793   0.55141\n  0.4  2          0.6               0.75       150      0.79172   0.58011\n  0.4  2          0.6               1.00        50      0.82494   0.64668\n  0.4  2          0.6               1.00       100      0.80138   0.59885\n  0.4  2          0.6               1.00       150      0.78483   0.56461\n  0.4  2          0.8               0.50        50      0.80874   0.61430\n  0.4  2          0.8               0.50       100      0.79839   0.59295\n  0.4  2          0.8               0.50       150      0.79184   0.58005\n  0.4  2          0.8               0.75        50      0.81506   0.62574\n  0.4  2          0.8               0.75       100      0.78425   0.56350\n  0.4  2          0.8               0.75       150      0.77793   0.55016\n  0.4  2          0.8               1.00        50      0.81828   0.63205\n  0.4  2          0.8               1.00       100      0.79138   0.57893\n  0.4  2          0.8               1.00       150      0.78149   0.55882\n  0.4  3          0.6               0.50        50      0.82195   0.64224\n  0.4  3          0.6               0.50       100      0.79839   0.59455\n  0.4  3          0.6               0.50       150      0.77138   0.53919\n  0.4  3          0.6               0.75        50      0.82195   0.64039\n  0.4  3          0.6               0.75       100      0.80494   0.60542\n  0.4  3          0.6               0.75       150      0.79839   0.59277\n  0.4  3          0.6               1.00        50      0.79149   0.57869\n  0.4  3          0.6               1.00       100      0.79149   0.57899\n  0.4  3          0.6               1.00       150      0.78816   0.57214\n  0.4  3          0.8               0.50        50      0.80839   0.61236\n  0.4  3          0.8               0.50       100      0.79851   0.59257\n  0.4  3          0.8               0.50       150      0.81529   0.62681\n  0.4  3          0.8               0.75        50      0.81184   0.62078\n  0.4  3          0.8               0.75       100      0.80184   0.60005\n  0.4  3          0.8               0.75       150      0.82195   0.64113\n  0.4  3          0.8               1.00        50      0.81862   0.63326\n  0.4  3          0.8               1.00       100      0.81851   0.63368\n  0.4  3          0.8               1.00       150      0.81839   0.63304\n\nTuning parameter 'gamma' was held constant at a value of 0\nTuning\n parameter 'min_child_weight' was held constant at a value of 1\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were nrounds = 50, max_depth = 1, eta\n = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1 and subsample\n = 0.5.\n\n\nEstos son los valores optimizados de los hiperparámetros del modelo XGBoost. Podemos usar estos valores para ajustar un modelo final con los mejores hiperparámetros.\n\n\nCódigo\n# Mejor valor de max_depth y eta\nmodelo_xgb_caret$bestTune\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnrounds\nmax_depth\neta\ngamma\ncolsample_bytree\nmin_child_weight\nsubsample\n\n\n\n\n50\n1\n0.3\n0\n0.6\n1\n0.5\n\n\n\n\n\n\nPodemos evaluar el desempeño del modelo con los hiperparametros optimizados:\n\n\nCódigo\n# predecir el modelo en todos los datos\npredicciones &lt;- predict(modelo_xgb_caret, newdata = heart)\n\n# matriz de confusion\nconfusionMatrix(predicciones, heart$sick)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No  146  24\n       Yes  14 113\n                                        \n               Accuracy : 0.872         \n                 95% CI : (0.829, 0.908)\n    No Information Rate : 0.539         \n    P-Value [Acc &gt; NIR] : &lt;2e-16        \n                                        \n                  Kappa : 0.741         \n                                        \n Mcnemar's Test P-Value : 0.144         \n                                        \n            Sensitivity : 0.912         \n            Specificity : 0.825         \n         Pos Pred Value : 0.859         \n         Neg Pred Value : 0.890         \n             Prevalence : 0.539         \n         Detection Rate : 0.492         \n   Detection Prevalence : 0.572         \n      Balanced Accuracy : 0.869         \n                                        \n       'Positive' Class : No"
  },
  {
    "objectID": "modelos_de_arboles.html#comparación-de-random-forest-y-xgboost",
    "href": "modelos_de_arboles.html#comparación-de-random-forest-y-xgboost",
    "title": "Modelos basados en árboles",
    "section": "3.11 Comparación de Random Forest y XGBoost",
    "text": "3.11 Comparación de Random Forest y XGBoost\n\n\n\n\n\n\n\n\n\n\n\nCaracterística\nXGBoost\nRandom_Forest\n\n\n\n\nAlgoritmo base\nGradient Boosting (modelos secuenciales)\nBagging (modelos independientes)\n\n\nVelocidad de entrenamiento\nMás lento debido a su naturaleza secuencial\nMás rápido gracias al entrenamiento paralelo\n\n\nDesempeño en datos complejos\nExcelente para relaciones no lineales complejas\nBueno, pero menos preciso en relaciones complejas\n\n\nRobustez frente al ruido\nPuede ser sensible al ruido si no se regulariza adecuadamente\nMuy robusto frente al ruido\n\n\nRegularización\nIncluye regularización L1 y L2 para evitar sobreajuste\nNo incluye regularización explícita\n\n\nInterpretabilidad\nDifícil de interpretar\nMás sencillo, especialmente con medidas de importancia\n\n\nOptimización de hiperparámetros\nRequiere un ajuste cuidadoso para obtener buen desempeño\nMenos dependiente del ajuste de hiperparámetros\n\n\nEscenarios recomendados\nProblemas grandes y complejos con alta dimensionalidad\nExploración inicial de datos o problemas más simples\n\n\nUso común\nCompetencias de machine learning, predicción precisa\nModelos base y análisis preliminares\n\n\nBibliotecas\nxgboost, lightgbm\nrandomForest, ranger"
  },
  {
    "objectID": "estadistica_tradicional_regresion.html",
    "href": "estadistica_tradicional_regresion.html",
    "title": "Modelos estádisticos tradicionales como regresiones",
    "section": "",
    "text": "Entender los abordajes estadísticos tradicionales como regresiones\nAquí veremos las pruebas estadísticas más comunes y mostraremos cómo pueden representarse en el formato de regresión lineal. Esta sección se basa en este artículo. Consúltelo para obtener una descripción más detallada de las alternativas no paramétricas."
  },
  {
    "objectID": "estadistica_tradicional_regresion.html#prueba-de-wilcoxon-para-dos-promedios",
    "href": "estadistica_tradicional_regresion.html#prueba-de-wilcoxon-para-dos-promedios",
    "title": "Modelos estádisticos tradicionales como regresiones",
    "section": "6.1 Prueba de Wilcoxon para dos promedios",
    "text": "6.1 Prueba de Wilcoxon para dos promedios\nTambién conocida como prueba de Mann-Whitney, es la alternativa no paramétrica a la prueba t de dos promedios:\n\n\nCódigo\n# set seed\nset.seed(123)\n\n# number of observations\nn &lt;- 50\ny &lt;- rnorm(n = n, mean = 0.2, sd = 1)\n\n# create data frame\ny_data &lt;- data.frame(y)\n\n\ny\n\n\n [1] -0.3604756 -0.0301775  1.7587083  0.2705084  0.3292877  1.9150650\n [7]  0.6609162 -1.0650612 -0.4868529 -0.2456620  1.4240818  0.5598138\n[13]  0.6007715  0.3106827 -0.3558411  1.9869131  0.6978505 -1.7666172\n[19]  0.9013559 -0.2727914 -0.8678237 -0.0179749 -0.8260044 -0.5288912\n[25] -0.4250393 -1.4866933  1.0377870  0.3533731 -0.9381369  1.4538149\n[31]  0.6264642 -0.0950715  1.0951257  1.0781335  1.0215811  0.8886403\n[37]  0.7539177  0.1380883 -0.1059627 -0.1804710 -0.4947070 -0.0079173\n[43] -1.0653964  2.3689560  1.4079620 -0.9231086 -0.2028848 -0.2666554\n[49]  0.9799651  0.1166309\n\n\nCódigo\n# Wilcoxon / Mann-Whitney U\nwilcx_mod &lt;- wilcox.test(y_data$y)\n\nwilcx_mod\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  y_data$y\nV = 797, p-value = 0.12\nalternative hypothesis: true location is not equal to 0\n\n\nCódigo\nsigned_rank &lt;- function(x) sign(x) * rank(abs(x))\n\n# As linear model with our dummy-coded group_y2:\nwicx_lm_mod &lt;- lm(signed_rank(y) ~ 1, data = y_data) # compare to\n\nsummary(wicx_lm_mod)\n\n\n\nCall:\nlm(formula = signed_rank(y) ~ 1, data = y_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-53.38 -24.13   0.12  25.37  43.62 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)     6.38       4.09    1.56     0.12\n\nResidual standard error: 28.9 on 49 degrees of freedom\n\n\n\n\n\n\n\n\nmodel\np.value\n\n\n\n\n1 mean wilcoxon\n0.12482\n\n\nwilcoxon_lm\n0.12480"
  },
  {
    "objectID": "preparacion_curso.html",
    "href": "preparacion_curso.html",
    "title": "1 Instalacion de software",
    "section": "",
    "text": "1 Instalacion de software\n\nInstalá o actualizá R en el ordenador que utilizarás durante el curso (https://cran.r-project.org). Asumo que ya lo tenés instalado, pero intentá actualizarlo si tenés una versión de R &lt; 4.0.0. Puedes averiguar qué versión de R tenés ejecutando esto en la consola de R:\n\n\n\nCódigo\nversion$version.string\n\n\n\nActualizá todos los paquetes de R si ya tenías instalado R (⚠️ este paso puede tardar mucho en ejecutarse ⚠️):\n\n\n\nCódigo\nupdate.packages(ask = FALSE)\n\n\n\nInstalá R en la computadora que utilizarás durante el curso (https://cran.r-project.org/). Probablemente ya lo tenés instalado, pero intentá actualizarlo si tenés una versión de R &lt; 4.0.0. Podés encontrar qué versión de R tenés ejecutando esto en la consola de R:\n\n\n\nCódigo\nversion$version.string\n\n\n\nInstalá la interfaz de RStudio (https://posit.co/download/rstudio-desktop/, elejí la versión gratuita).\nAbrí RStudio y seleccioná la pestaña “Tools” y luego “Global options” (última opción). Seleccioná la opción “Code”, luego seleccioná la casilla de “Soft wrap”.\nTambién en Rstudio: Seleccioná la opción “Pane Layout” y mové “Source” al panel superior izquierdo y “Console” al panel superior derecho. Esta disposición asigna más espacio en la pantalla a los paneles más útiles. Dalé “Apply” y “Ok”. Para aquellos que no estén familiarizados con RStudio, “Source” es un editor de texto donde se escribe el código y donde guardas el código en un archivo físico (normalmente en formato .R) y la consola evalúa el código que se envia desde el “Source” e imprime los resultados (si los hay). Podés escribir código en la consola, pero no se guardará en un archivo físico.\nAhora en la consola de R en Rstudio ejecutá el siguiente código para instalar los paquetes que estaremos usando durante el curso:\n\n\n\nCódigo\n# # cargar funciones del paquete \"sketchy\"\n# source(\n#   paste0(\n#     \"https://raw.githubusercontent.com/maRce10/\",\n#     \"sketchy/main/R/load_packages.R\"\n#   )\n# )\n# source(\n#   paste0(\n#     \"https://raw.githubusercontent.com/maRce10/\",\n#     \"sketchy/main/R/internal_functions.R\"\n#   )\n# )\n# \n# instalar paquete sketchy para instalar otros paquetes y creer proyectos\n# install.packages(\"sketchy\")\n\n# cargar paquete\nlibrary(sketchy)\n\n# instalar/ cargar paquetes\nload_packages(\n  packages = c(\n    \"remotes\",\n    \"RColorBrewer\",\n    \"ggplot2\",\n    \"viridis\",\n    \"MASS\",\n    \"psych\",\n    \"lme4\",\n    \"caret\",\n    \"glmnet\",\n    \"lmerTest\",\n    \"sjPlot\",\n    \"car\",\n    \"nnet\",\n    \"neuralnet\",\n    \"randomForest\",\n    \"dimRed\",\n    \"MuMIn\"\n    github = \"maRce10/sketchy\"\n  )\n)\n\n\n\nCreá un proyecto para el curso corriendo este código:\n\n\n\nCódigo\nlibrary(sketchy)\n\nmake_compendium(\n  name = \"r_avanzado\",\n  path = \"DIRECTORIO_DONDE_HACER_EL_PROYECTO\",\n  Rproj = TRUE,\n  readme = TRUE\n)\n\n\n(en realidad este es tambien un compendio de investigación)\nAbrí el proyecto en Rstudio:\n\nEste proyecto debe usarse cada vez que se trabaja código del curso ✌️\n\n\n2 Recursos adicionales\n\nIntroducción a Rstudio video\nkaggle: juegos de datos de ejemplo link:\nDatos para classificar personalidad\nR data sets: link"
  },
  {
    "objectID": "evaluacion_de_modelos.html",
    "href": "evaluacion_de_modelos.html",
    "title": "Evaluación de modelos",
    "section": "",
    "text": "Familiarizarse con las metricas de evaluación de modelos de aprendizaje estadístico\nSer capaz de evaluar modelos de regresión y clasificación en R\nPaquetes a utilizar en este manual:\nCódigo\n# instalar/cargar paquetes\n\nsketchy::load_packages(\n  c(\"ggplot2\", \n    \"viridis\", \n    \"nnet\",\n    \"caret\",\n    \"glmnet\",\n    \"pROC\",\n    \"nnet\",\n    \"Metrics\")\n  )\nLa idea es evaluar la capacidad del modelo para hacer predicciones precisas y generalizar bien a datos no vistos. Por lo tanto, evaluar correctamente los modelos permite entender qué tan bien el modelo se ajusta a los datos e idealmente, entender si sus predicciones son útiles para datos no vistos. La naturaleza de la métrica depende de la estructura de los datos predichos (si estos son continuos o categóricos):\nflowchart LR\n    classDef largeText font-size:18px, padding:15px;\n\n    A(Evaluación de modelos) --&gt; B(Respuesta continua)\n    A --&gt; C(\"Respuesta categórica\")\n    B --&gt; D(RMSE)\n    B --&gt; E(R cuadrado)\n    C --&gt; F(\"Matriz de confusión\")\n    C --&gt; G(\"Precisión, sensibilidad, F1\")\n    C --&gt; H(\"Pérdida logarítmica\")\n\n    style A fill:#40498E66, stroke:#000, stroke-width:2px, color:#FFF, width:180px\n    style B fill:#348AA666, stroke:#000, stroke-width:2px, color:#FFF \n    style C fill:#348AA666, stroke:#000, stroke-width:2px, color:#FFF\n    style D fill:#49C1AD66, stroke:#000, stroke-width:2px, color:#000\n    style E fill:#49C1AD66, stroke:#000, stroke-width:2px, color:#000\n    style F fill:#49C1AD66, stroke:#000, stroke-width:2px, color:#000\n    style G fill:#49C1AD66, stroke:#000, stroke-width:2px, color:#000\n    style H fill:#49C1AD66, stroke:#000, stroke-width:2px, color:#000"
  },
  {
    "objectID": "evaluacion_de_modelos.html#raíz-del-error-cuadrático-medio-rmse",
    "href": "evaluacion_de_modelos.html#raíz-del-error-cuadrático-medio-rmse",
    "title": "Evaluación de modelos",
    "section": "1.1 Raíz del Error Cuadrático Medio (RMSE)",
    "text": "1.1 Raíz del Error Cuadrático Medio (RMSE)\nLa raíz del error cuadrático medio (RMSE por sus siglas en inglés) es quizás la medida mas comunmente utilizada para medir la precisión en la predicción de una variable continua. Las unidades de esta métrica son las mismas que la variable de respuesta. La podemos calcular fácilmente:\n\n\nCódigo\n# Generar datos simulados\nset.seed(123)\nx &lt;- rnorm(100)\ny &lt;- 3 * x + rnorm(100, sd = 0.5)\n\n# Crear un marco de datos\ndatos &lt;- data.frame(x = x, y = y)\n\n# correr modelo\nmod &lt;- lm(y ~ x, data = datos)\n\n# Step 2: Obtener predicciones\ny_pred &lt;- predict(mod)\n\n# Step 3: Calcular RMSE\nsqrt(mean((datos$y - y_pred)^2))\n\n\n[1] 0.48048\n\n\nLa raíz del error cuadrático medio penaliza más los errores grandes, ya que los errores se elevan al cuadrado. Esto lo hace útil para detectar errores grandes que podrían ser problemáticos. Sin embargo, es sensible a outliers, ya que los errores grandes tienen un impacto más significativo en el resultado."
  },
  {
    "objectID": "evaluacion_de_modelos.html#coeficiente-de-determinación-r2",
    "href": "evaluacion_de_modelos.html#coeficiente-de-determinación-r2",
    "title": "Evaluación de modelos",
    "section": "1.2 Coeficiente de determinación R2",
    "text": "1.2 Coeficiente de determinación R2\nEl R2 indica qué proporción de la variabilidad en la variable respuesta es explicada por el modelo. Se calcula así:\n\n\nCódigo\nr2 &lt;- 1 - sum((datos$y - y_pred)^2) / sum((datos$y - mean(datos$y))^2)\n    \n# imprimir R^2\nr2\n\n\n[1] 0.96932\n\n\nEste es el mismo valor que se obtiene al evaluar el modelo con la función summary:\n\n\nCódigo\nsummary(mod)$r.squared\n\n\n[1] 0.96932\n\n\nEntre R2 sea mas cercano a 1 indica el modelo tiene mejor ajuste. Sin embargo, es importante tener en cuenta que un R2 alto no garantiza que el modelo sea útil o que generalice bien a nuevos datos. No es una buena métrica para modelos no lineales, ya que puede no reflejar bien la calidad del ajuste en estos casos.\n\nEjercicio 1\nLos datos mtcars se tomaron de la revista Motor Trend US de 1974, y contienen informacion sobre 10 características del diseño y desempeño de 32 marcas de carros.\n\n\nCódigo\n# cargar datos\ndata(mtcars)\n\n\n\nAjusta un modelo de regresión lineal con los datos de mtcars donde la variable respuesta sea mpg (millas por galón) y las variables predictoras sean hp (caballos de fuerza) y wt (peso).\n\n\nAjuste 2 modelos más, cada uno con 2 variables predictoras más que el modelo ajustado en el paso 1.\nCompare los modelos generados en los puntos 1 y 2 usando la raíz del error cuadrático medio (RMSE) y el R2."
  },
  {
    "objectID": "evaluacion_de_modelos.html#matriz-de-confusión",
    "href": "evaluacion_de_modelos.html#matriz-de-confusión",
    "title": "Evaluación de modelos",
    "section": "2.1 Matriz de confusión",
    "text": "2.1 Matriz de confusión\nLa matriz de confusión organiza las predicciones y los resultados observados en categorías que indican si el valor predicho es una ocurrencia del evento (positivo vs negativo) y si coincide o no con el valor observado (verdadero vs falso).\n\n\n\n\n\n\n\n\n\nEsta matriz la podemos estimar usando la función confusionMatrix del paquete caret:\n\n\nCódigo\n# hacer la matriz de confusion\nmat_conf &lt;-\n    confusionMatrix(factor(pred_cat), factor(datos_tab_titanic$Survived))\n\n# imprimir resultado\nmat_conf$table\n\n\n          Reference\nPrediction   No  Yes\n       No  1364  367\n       Yes  126  344\n\n\nPodemos graficar esta matriz con un gradiente de colores donde los valores esten dado como proporciones, lo cual mas fácil su interpretación:\n\n\nCódigo\n# convertir a data frame\nconf_df &lt;- as.data.frame(mat_conf$table)\n\n# agregar totales por categoria\nconf_df$total &lt;-\n    sapply(conf_df$Reference, function(x)\n        sum(datos_tab_titanic$Survived ==\n                x))\n\n# calcular proporciones\nconf_df$proportion &lt;- conf_df$Freq / conf_df$total\n\n# graficar\nggplot(conf_df, aes(x = Reference, y = Prediction, fill = proportion)) +\n  geom_tile() + \n  coord_equal() + \n  scale_fill_distiller(palette = \"Greens\", direction = 1) + \n  geom_text(aes(label = round(proportion, 2)), color = \"black\", size = 3) + \n  labs(x = \"Observado\", y = \"Predicho\", fill = \"Proporción\") \n\n\n\n\n\n\n\n\n\nVarias métricas se derivan de los valores contenidos en la matriz de confusion."
  },
  {
    "objectID": "evaluacion_de_modelos.html#exactitud-accuracy",
    "href": "evaluacion_de_modelos.html#exactitud-accuracy",
    "title": "Evaluación de modelos",
    "section": "2.2 Exactitud (Accuracy)",
    "text": "2.2 Exactitud (Accuracy)\nLa exactitud es la proporción de predicciones correctas en relación con el total de predicciones.\nLa fórmula es:\n\n \\(\\frac{VP + VN}{VP + VN + FP + FN}\\) \n\nEsta también es estimada por la función confusionMatrix:\n\n\nCódigo\nmat_conf$overall[\"Accuracy\"]\n\n\nAccuracy \n 0.77601"
  },
  {
    "objectID": "evaluacion_de_modelos.html#precisión-precision-y-sensibilidad-sensitivity-o-recall",
    "href": "evaluacion_de_modelos.html#precisión-precision-y-sensibilidad-sensitivity-o-recall",
    "title": "Evaluación de modelos",
    "section": "2.3 Precisión (Precision) y Sensibilidad (Sensitivity o Recall)",
    "text": "2.3 Precisión (Precision) y Sensibilidad (Sensitivity o Recall)\nLa precisión es la proporción de verdaderos positivos entre todas las predicciones positivas:\n\n \\(\\frac{VP}{VP + FP}\\) \n\nLa sensibilidad es la proporción de verdaderos positivos entre todas las observaciones positivas:\n\n \\(\\frac{VP}{VP + FN}\\) \n\nAmbas son estimadas por la función confusionMatrix:\n\n\nCódigo\nmat_conf$byClass[c(\"Precision\", \"Sensitivity\")]\n\n\n  Precision Sensitivity \n    0.78798     0.91544"
  },
  {
    "objectID": "evaluacion_de_modelos.html#índice-f1-o-f-score",
    "href": "evaluacion_de_modelos.html#índice-f1-o-f-score",
    "title": "Evaluación de modelos",
    "section": "2.4 Índice F1 (o F-Score)",
    "text": "2.4 Índice F1 (o F-Score)\nEl índice F1 es la media armónica entre la precisión y la sensibilidad:\n\n \\(2 \\times \\frac{\\text{Precisión} \\times \\text{Sensibilidad}}{\\text{Precisión} + \\text{Sensibilidad}}\\) \n\n\n\nCódigo\nprecision &lt;- mat_conf$byClass[\"Precision\"]\nsensitivity &lt;- mat_conf$byClass[\"Sensitivity\"]\n\n\nf1_score &lt;- (2 * precision * sensitivity) / (precision + sensitivity)\n\nf1_score\n\n\nPrecision \n  0.84694 \n\n\nTambien lo podemos extraer del resultado de la función confusionMatrix:\n\n\nCódigo\n mat_conf$byClass[\"F1\"]\n\n\n     F1 \n0.84694"
  },
  {
    "objectID": "evaluacion_de_modelos.html#área-bajo-la-curva-roc-auc-roc",
    "href": "evaluacion_de_modelos.html#área-bajo-la-curva-roc-auc-roc",
    "title": "Evaluación de modelos",
    "section": "2.5 Área Bajo la Curva ROC (AUC-ROC)",
    "text": "2.5 Área Bajo la Curva ROC (AUC-ROC)\nEl AUC métrica originalmente diseñada para evaluar la precision en la predicción de variables binarias. Esta mide la capacidad del modelo para discriminar entre clases. Se calcula basándose en la curva ROC (Receiver Operating Characteristic). La curva ROC compara la tasa de verdaderos positivos contra la tasa de falsos positivos a diferentes umbrales.La curva ROC es una representación gráfica que evalúa el desempeño de un modelo de clasificación binaria. La curva muestra la relación entre la Tasa de Verdaderos Positivos (True Positive Rate, TPR), también llamada sensibilidad, y la Tasa de Falsos Positivos (False Positive Rate, FPR) para diferentes umbrales de decisión. Los umbrales de decisión son los valores que determinan si una observación se clasifica como positiva o negativa. Podemos ejemplificar su uso con el modelo de regresión logística ajustado mas arriba:\n\n\nCódigo\n# estimar las probabilidades\npred_vals &lt;- predict(object = modelo_log, newdata = datos_tab_titanic, type = \"response\")\n\n# Curva ROC\nroc_curve &lt;- roc(datos_tab_titanic$Survived, pred_vals, smooth = TRUE)\n\n# Extraer los datos de la curva ROC\nroc_data &lt;- data.frame(\n  tpr = roc_curve$sensitivities, # True Positive Rate (Sensibilidad)\n  fpr = 1 - roc_curve$specificities # False Positive Rate (1 - Especificidad)\n)\n\n# Graficar la curva ROC con ggplot2\nggplot(roc_data, aes(x = fpr, y = tpr)) +\n  geom_line(color = \"tomato\") +\n  geom_abline(linetype = \"dashed\", color = \"gray\") + # Línea diagonal\n  labs(\n    x = \"Tasa de Falsos Positivos (FPR)\",\n    y = \"Tasa de Verdaderos Positivos (TPR)\"\n  ) +\n  scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) +  # Limitar el eje X entre 0 y 1\n  scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +  # Limitar el eje Y entre 0 y \n  theme_classic() +\n  theme(panel.grid = element_blank()) +# Remover cuadrícula\n  # add polygon showin area under the curve\n  geom_polygon(data = roc_data, aes(x = fpr, y = tpr), fill = \"tomato\", alpha = 0.3) + \n# add polygon coloring lower diagonal \n  geom_polygon(data = data.frame(x = c(0, 1, 1), y = c(0, 0, 1)), aes(x = x, y = y), fill = \"tomato\", alpha = 0.3) +\n  annotate(\"text\", x = 0.5, y = 0.5, label = paste(\"AUC = \", round(pROC::auc(roc_curve), 2)), size = 6)\n\n\n\n\n\n\n\n\n\nPodemos estimar el área bajo la curva con la función auc del paquete pROC:\n\n\nCódigo\n# estimar AUC\npROC::auc(roc_curve)\n\n\nArea under the curve: 0.758\n\n\nValores mas cercanos a 1 indican un mejor desempeño del modelo en discriminar entre clases tomando en cuenta la incertidumbre relacionada a la escogencia de un umbral de decisión.\nEn los modelos que predicen más de dos categorías (clasificación multiclase), no se puede aplicar la curva ROC directamente como en el caso binario. Aun así, hay métodos extendidos para calcular una métrica similar a la AUC para clasificación multiclase. Esta métrica se puede extender para modelos que predicen multiples clases. Por ejemplo el metodo de “uno vs el resto” (one vs rest) calcula una curva ROC para cada categoría de manera binaria, considerando una clase como “positiva” y las demás como “negativas”. Luego, se calcula el AUC para cada una de estas curvas ROC y se promedia. Para ejemplificar el uso de este método usaremos una regresión multinomial para predecir la especie de las observaciones del juego de datos iris:\n\n\nCódigo\n# Ajustar el modelo de regresión multinomial\nmodelo_multinom &lt;- multinom(Species ~ ., data = iris)\n\n\n# weights:  18 (10 variable)\ninitial  value 164.791843 \niter  10 value 16.177348\niter  20 value 7.111438\niter  30 value 6.182999\niter  40 value 5.984028\niter  50 value 5.961278\niter  60 value 5.954900\niter  70 value 5.951851\niter  80 value 5.950343\niter  90 value 5.949904\niter 100 value 5.949867\nfinal  value 5.949867 \nstopped after 100 iterations\n\n\nCódigo\n# Predecir las probabilidades para cada clase\npred_prob &lt;- predict(modelo_multinom, newdata = iris, type = \"prob\")\n\n# Aplicar multiclass.roc (One-vs-Rest)\nroc_multiclass &lt;- multiclass.roc(iris$Species, pred_prob)\n\n# Ver resultados\nroc_multiclass\n\n\n\nCall:\nmulticlass.roc.default(response = iris$Species, predictor = pred_prob)\n\nData: multivariate predictor pred_prob with 3 levels of iris$Species: setosa, versicolor, virginica.\nMulti-class area under the curve: 0.999"
  },
  {
    "objectID": "evaluacion_de_modelos.html#pérdida-logarítmica-log-loss",
    "href": "evaluacion_de_modelos.html#pérdida-logarítmica-log-loss",
    "title": "Evaluación de modelos",
    "section": "2.6 Pérdida logarítmica (log-loss)",
    "text": "2.6 Pérdida logarítmica (log-loss)\nMide el rendimiento de un modelo de clasificación al penalizar predicciones erróneas basadas en la probabilidad que asigna el modelo a cada clase.\n\n \\(-\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]\\) \n\nDonde \\({y_i}\\) es el valor real (0 o 1) y \\({p_i}\\) es la probabilidad asignada a la clase positiva. Es útil cuando se desea medir la confianza del modelo en sus predicciones y no solo si acierta o falla. Aunque puede ser más complejo de interpretar en comparación con otras métricas.\n\n\nCódigo\n# Convertir las etiquetas reales a una matriz de indicadores (one-hot encoding)\n# note que Species es un factor\ny_obs &lt;- model.matrix(~Species-1, data = iris)\n\n# Calcular el log-loss\nlog_loss_value &lt;- logLoss(y_obs, pred_prob)\nlog_loss_value\n\n\n[1] 0.026444\n\n\nEl valor del pérdida logarítmica representa qué tan bien o mal el modelo predice las clases correctas. Un valor más bajo (mas cerano a 0) indica un mejor ajuste, mientras que un valor alto sugiere que el modelo está asignando bajas probabilidades a las clases correctas. El valor de pérdida logarítmica puede tomar cualquier valor real no negativo, y en general se encuentra entre 0 y ∞."
  },
  {
    "objectID": "evaluacion_de_modelos.html#ejercicio-2",
    "href": "evaluacion_de_modelos.html#ejercicio-2",
    "title": "Evaluación de modelos",
    "section": "Ejercicio 2",
    "text": "Ejercicio 2\nUsaremos los datos de Cortez y Silva (2008) sobre el desempeño académico de estudiantes en la clase de matemáticas. Los datos contienen 395 observaciones y 31 variables. La variable respuesta es G3 (calificación final), y las variables predictoras son las demás variables del conjunto de datos. Pueden econtrar una descripción detallada de los datos aqui. El siguiente código carga los datos y remueve las variables G1 y G2 (calificaciones parciales) para evitar multicolinealidad:\n\n\nCódigo\n# leer datos\ndatos_mate &lt;- read.csv(\"https://raw.githubusercontent.com/maRce10/aprendizaje_estadistico_2024/refs/heads/master/data/student/student-mat.csv\", sep = \";\")\n\n# remover G1 y G2 (calificaciones parciales)\ndatos_mate$G1 &lt;- datos_mate$G2 &lt;- NULL\n\nstr(datos_mate)\n\n\n'data.frame':   395 obs. of  31 variables:\n $ school    : chr  \"GP\" \"GP\" \"GP\" \"GP\" ...\n $ sex       : chr  \"F\" \"F\" \"F\" \"F\" ...\n $ age       : int  18 17 15 15 16 16 16 17 15 15 ...\n $ address   : chr  \"U\" \"U\" \"U\" \"U\" ...\n $ famsize   : chr  \"GT3\" \"GT3\" \"LE3\" \"GT3\" ...\n $ Pstatus   : chr  \"A\" \"T\" \"T\" \"T\" ...\n $ Medu      : int  4 1 1 4 3 4 2 4 3 3 ...\n $ Fedu      : int  4 1 1 2 3 3 2 4 2 4 ...\n $ Mjob      : chr  \"at_home\" \"at_home\" \"at_home\" \"health\" ...\n $ Fjob      : chr  \"teacher\" \"other\" \"other\" \"services\" ...\n $ reason    : chr  \"course\" \"course\" \"other\" \"home\" ...\n $ guardian  : chr  \"mother\" \"father\" \"mother\" \"mother\" ...\n $ traveltime: int  2 1 1 1 1 1 1 2 1 1 ...\n $ studytime : int  2 2 2 3 2 2 2 2 2 2 ...\n $ failures  : int  0 0 3 0 0 0 0 0 0 0 ...\n $ schoolsup : chr  \"yes\" \"no\" \"yes\" \"no\" ...\n $ famsup    : chr  \"no\" \"yes\" \"no\" \"yes\" ...\n $ paid      : chr  \"no\" \"no\" \"yes\" \"yes\" ...\n $ activities: chr  \"no\" \"no\" \"no\" \"yes\" ...\n $ nursery   : chr  \"yes\" \"no\" \"yes\" \"yes\" ...\n $ higher    : chr  \"yes\" \"yes\" \"yes\" \"yes\" ...\n $ internet  : chr  \"no\" \"yes\" \"yes\" \"yes\" ...\n $ romantic  : chr  \"no\" \"no\" \"no\" \"yes\" ...\n $ famrel    : int  4 5 4 3 4 5 4 4 4 5 ...\n $ freetime  : int  3 3 3 2 3 4 4 1 2 5 ...\n $ goout     : int  4 3 2 2 2 2 4 4 2 1 ...\n $ Dalc      : int  1 1 2 1 1 1 1 1 1 1 ...\n $ Walc      : int  1 1 3 1 2 2 1 1 1 1 ...\n $ health    : int  3 3 3 5 5 5 3 1 1 5 ...\n $ absences  : int  6 4 10 2 4 10 0 6 0 0 ...\n $ G3        : int  6 6 10 15 10 15 11 6 19 15 ...\n\n\n\nAjuste un modelo de regresión lineal con los datos de datos_mate donde la variable respuesta sea G3. Utilice su conocimiento del sistema (como estudiante) para escoger las variables predictoras que usted espere tengan un mayor efecto sobre el desempeño académico. Calcule el R2 para este modelo.\n\n\nAjuste un modelo de regresión multinomial análogo al del punto anterior (e.g. mismos predictores y misma respuesta).\n\n\nCalcule la matriz de confusión, la exactitud, el área bajo la curva y pérdida logarítmica para el modelo del punto anterior.\n\nEl siguiente código binariza la variable respuesta G3. Si la calificación es menor a 9, se considera que el estudiante reprobó (Reprobado), de lo contrario, se considera que aprobó (Aprobado):\n\n\nCódigo\ndatos_mate$G3_bin &lt;- ifelse(datos_mate$G3 &lt; 9, 0, 1) \n\n\n\nUtilizando la variable binaria G3_bin como respuesta, ajuste un modelo de regresión logística con las mismas variables predictoras que el modelo ajustado en el punto 2.\n\n5.Calcule la matriz de confusión, la exactitud, precisión, indice F1, area bajo la curva y pérdida logarítmica para el modelo del punto anterior.\n\nCompare los modelos generados en los puntos 2 y 4. ¿Cual modelo parece tener un mejor desempeño? ¿Por qué?\nExplore la sensibilidad de los modelos multinomial (punto 2) y logístico (punto 4). Para esto puede simplemente imprimir en la consola el resultado de la función confusionMatrix. ¿Cómo difieren los valores y la forma en que estos se estructuran entre los 2 modelos?"
  },
  {
    "objectID": "modelos_de_regresion.html",
    "href": "modelos_de_regresion.html",
    "title": "Modelos de regresión",
    "section": "",
    "text": "Paquetes a utilizar en este manual:\nCódigo\n# instalar/cargar paquetes\n\nsketchy::load_packages(\n  c(\"ggplot2\", \n    \"viridis\", \n    \"lmerTest\", \n    \"sjPlot\")\n  )"
  },
  {
    "objectID": "modelos_de_regresion.html#modelo-solo-con-intercepto",
    "href": "modelos_de_regresion.html#modelo-solo-con-intercepto",
    "title": "Modelos de regresión",
    "section": "1.1 Modelo solo con intercepto",
    "text": "1.1 Modelo solo con intercepto\nPrimero vamos a crear una variable numérica de respuesta:\n\n\nCódigo\n# definir semilla\nset.seed(123)\n\n# numero de observaciones\nn &lt;- 50\n\n#  variables aleatorias\ny &lt;- rnorm(n = n, mean = 0, sd = 1)\n\n# put it in a data frame\ny_data &lt;- data.frame(y)\n\n\n \nEsta única variable puede introducirse en un modelo de regresión sólo con intercepto. Para ello, debemos suministrar la fórmula del modelo y los datos a lm():\n\n\nCódigo\n# run model\ny_mod &lt;- lm(formula = y ~ 1, data = y_data)\n\n\n \nLo que equivale a:\n\n\\(\\hat{Y} \\sim \\beta_{o}\\)\n\n \nPodemos obtener el resumen por defecto de los resultados del modelo ejecutando summary() en el objeto de salida ‘y_mod’:\n\n\nCódigo\nsummary(y_mod)\n\n\n\nCall:\nlm(formula = y ~ 1, data = y_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.001 -0.594 -0.107  0.664  2.135 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   0.0344     0.1309    0.26     0.79\n\nResidual standard error: 0.926 on 49 degrees of freedom\n\n\n \nPuede ser bastante informativo graficar los tamaños de efecto (aunque en este caso sólo tenemos uno):\n\n\nCódigo\nci_df &lt;- data.frame(param = names(y_mod$coefficients), \n                    est = y_mod$coefficients, confint(y_mod))\n\nggplot(ci_df, aes(x=param, y=est)) + \n  geom_hline(yintercept = 0, color=\"red\", lty = 2) +\n  geom_pointrange(aes(ymin = X2.5.., ymax = X97.5..)) + \n  labs(x = \"Parámetro\", y = \"Tamaño de efecto\") + \n  coord_flip()\n\n\n\n\n\n\n\n\n\n \n\n \n\nInterpretación del modelo\n\n \nPara evaluar la importancia de la asociación nos centramos en la tabla de coeficientes:\n\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) 0.034404    0.13094 0.26275  0.79385\n\n\n\nEn este ejemplo no hay predictores en el modelo, por lo que sólo tenemos una estimación para el intercepto (\\(\\beta_0\\))\nEl modelo nos dice que el intercepto se estima en 0.0344 y que este valor no es significativamente diferente de 0 (valor p = 0.79385)\nEn este caso el intercepto es simplemente la media de la variable de respuesta\n\n\n\nCódigo\nmean(y_data$y)\n\n\n[1] 0.034404\n\n\n \n\nRara vez tenemos predicciones sobre el intercepto, por lo que tendemos a ignorar esta estimación.\n\n\n \n\n\nCaso de estudio\n\n \n\nRuhs, E. C., Martin, L. B., & Downs, C. J. (2020). The impacts of body mass on immune cell concentrations in birds. Proceedings of the Royal Society B, 287(1934), 20200655.\n\n“Encontramos que un modelo sólo con intercepto (intercept-only model) explicaba mejor las concentraciones de linfocitos y eosinófilos en las aves, indicando que las concentraciones de estos tipos de células eran independientes de la masa corporal.”\n\n\n\n\n\n\n\n\n\n \n\n \n\nEjercicio 3\n \n\nCambie el argumento ‘mean’ en la llamada de la función “rnorm()` en la simulación para el modelo de solo intercepto un valor distinto de 0 y observe cómo cambian los valores en la tabla de coeficientes\nCambia el argumento sd en la llamada a la función rnorm() por un valor más alto y observe cómo cambian los valores en la tabla de coeficientes"
  },
  {
    "objectID": "modelos_de_regresion.html#añadir-un-predictor-no-asociado",
    "href": "modelos_de_regresion.html#añadir-un-predictor-no-asociado",
    "title": "Modelos de regresión",
    "section": "1.2 Añadir un predictor no asociado",
    "text": "1.2 Añadir un predictor no asociado\nPodemos crear 2 variables numéricas no relacionadas así:\n\n\nCódigo\n# definir semilla\nset.seed(123)\n\n# numero de observaciones\nn &lt;- 50\n\n#  variables aleatorias\ny &lt;-  rnorm(n = n, mean = 0, sd = 1)\nx1 &lt;-  rnorm(n = n, mean = 0, sd = 1)\n\n# crear data frame\nxy_datos &lt;- data.frame(x1, y)\n\n\n \nEstas dos variables pueden introducirse en un modelo de regresión para evaluar la asociación entre ellas:\n\n\nCódigo\n# construir model\nxy_mod &lt;- lm(formula = y ~ x1, data = xy_datos)\n\n# graficar\nggplot(xy_datos, aes(x = x1, y = y)) + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  geom_point() # graficar points\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n \nQue es equivalente a esto:\n\n\\(\\hat{Y} \\sim \\beta_{o} + \\beta_{1} * x_{1}\\)\n\n \nImprimamos el resumen de este modelo:\n\n\nCódigo\nsummary(xy_mod)\n\n\n\nCall:\nlm(formula = y ~ x1, data = xy_datos)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.004 -0.624 -0.123  0.687  2.106 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   0.0398     0.1340    0.30     0.77\nx1           -0.0367     0.1475   -0.25     0.80\n\nResidual standard error: 0.935 on 48 degrees of freedom\nMultiple R-squared:  0.00129,   Adjusted R-squared:  -0.0195 \nF-statistic: 0.0618 on 1 and 48 DF,  p-value: 0.805\n\n\n \n… y graficar los tamaños de efecto:\n\n\nCódigo\nci_df &lt;- data.frame(param = names(xy_mod$coefficients), \n                    est = xy_mod$coefficients, confint(xy_mod))\n\nggplot(ci_df, aes(x=param, y=est)) + \n  geom_hline(yintercept = 0, color=\"red\", lty = 2) +\n  geom_pointrange(aes(ymin = X2.5.., ymax = X97.5..)) + \n  labs(x = \"Parámetro\", y = \"Tamaño de efecto\") + \n  coord_flip()\n\n\n\n\n\n\n\n\n\n \nDeberíamos “diagnosticar” la idoneidad del modelo inspeccionando más de cerca la distribución de los residuos. La función plot_model() del paquete sjPlot hace un buen trabajo para crear gráficos de diagnóstico para modelos lineales:\n\n\nCódigo\nplot_model(xy_mod, type = \"diag\")\n\n\n[[1]]\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n \n\n \n\nInterpretación del modelo\n\n \nCuadro con coeficientes:\n\n\n             Estimate Std. Error  t value Pr(&gt;|t|)\n(Intercept)  0.039774    0.13396  0.29690  0.76782\nx1          -0.036679    0.14750 -0.24867  0.80467\n\n\n \n\nEn este ejemplo hemos añadido un predictor al modelo, por lo que hemos obtenido una estimado adicional (y una fila extra, ‘x1’)\nEl modelo nos dice que la estimación de ‘x1’ es -0.03668 y que no es significativamente diferente de 0 (p = 0.80467)"
  },
  {
    "objectID": "modelos_de_regresion.html#simular-un-predictor-asociado",
    "href": "modelos_de_regresion.html#simular-un-predictor-asociado",
    "title": "Modelos de regresión",
    "section": "1.3 Simular un predictor asociado",
    "text": "1.3 Simular un predictor asociado\nPodemos utilizar la fórmula del modelo lineal anterior para simular dos variables continuas asociadas así:\n\n\nCódigo\n# definir semilla\nset.seed(123)\n\n# numero de observaciones\nn &lt;- 50\nb0 &lt;- -4\nb1 &lt;- 3\nerror &lt;- rnorm(n = n, sd = 3)\n\n# variables aleatorias\nx1 &lt;-  rnorm(n = n, mean = 0, sd = 1)\ny &lt;- b0 + b1 * x1 + error\n\n# crear data frame\nxy_datos2 &lt;- data.frame(x1, y)\n\n\n \nNote que también hemos añadido un término de error, por lo que la asociación no es perfecta. Vamos a correr el modelo y graficar la asociación entre las dos variables:\n\n\nCódigo\n# construir model\nxy_mod2 &lt;- lm(formula = y ~ x1, data = xy_datos2)\n\n# graficar\nggplot(xy_datos2, aes(x = x1, y = y)) + \n  geom_smooth(method = \"lm\", se = FALSE) +  \n  geom_point() # graficar points\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n \nLa fórmula es la misma que la del modelo anterior:\n\n\\(\\hat{Y} \\sim \\beta_{o} + \\beta_{1} * x_{1}\\)\n\n \nEste es el resumen del modelo:\n\n\nCódigo\nsummary(xy_mod2)\n\n\n\nCall:\nlm(formula = y ~ x1, data = xy_datos2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -6.01  -1.87  -0.37   2.06   6.32 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -3.881      0.402   -9.66  7.9e-13 ***\nx1             2.890      0.442    6.53  3.9e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.8 on 48 degrees of freedom\nMultiple R-squared:  0.471, Adjusted R-squared:  0.459 \nF-statistic: 42.7 on 1 and 48 DF,  p-value: 3.85e-08\n\n\n \n.. el gráfico con los tamaños de efecto:\n\n\nCódigo\nci_df &lt;- data.frame(param = names(xy_mod2$coefficients), \n                    est = xy_mod2$coefficients, confint(xy_mod2))\n\nggplot(ci_df, aes(x=param, y=est)) + \n  geom_hline(yintercept = 0, color=\"red\", lty = 2) +\n  geom_pointrange(aes(ymin = X2.5.., ymax = X97.5..)) + \n  labs(x = \"Parámetro\", y = \"Tamaño de efecto\") + \n  coord_flip()\n\n\n\n\n\n\n\n\n\n \n… y los gráficos diagnósticos del modelo:\n\n\nCódigo\nplot_model(xy_mod2, type = \"diag\")\n\n\n[[1]]\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n \n\n \n\nInterpretación del modelo\n\n \nCuadro con los coeficientes (estimados):\n\n\n            Estimate Std. Error t value   Pr(&gt;|t|)\n(Intercept)  -3.8807    0.40188 -9.6562 7.8616e-13\nx1            2.8900    0.44249  6.5311 3.8537e-08\n\n\n\nEl modelo nos dice que \\(beta_1\\) (el tamaño de efecto de ‘x1’) es 2.88996 y que es significativamente diferente de 0 (p = 3.85365^{-8})\nLos valores simulados de los parámetros de regresión pueden compararse con el resumen del modelo lm() para tener una idea de la precisión del modelo:\n\n\\(\\beta_1\\) (the effect size of ‘x1’) was set to 3 and was estimated as 2.89 by the model\n\n\n\n \n\n\nCaso de estudio\n\n \nWilkinson R, Pickett K, Cato MS. The spirit level. Why more equal societies almost always do better. 2009.\nIndex of: Life expectancy, Math & Literacy, Infant mortality, Homicides, Imprisonment, Teenage births, Trust, Obesity Mental illness, incl. drug & alcohol addiction, Social mobility\n\n\n\n\n\n\n\n\n\n\n \n\nEjercicio 4\n \n\nAumente el tamaño de la muestra (n) a 1000 o más\n¿Cómo cambiaron los estimados del tamaño de efecto (\\(\\beta\\))?\n¿Cómo cambió los intervalos de confianza del tamaño de efecto?\nAhora cambie n a 15 y compruebe de nuevo las estimaciones del modelo (esta vez compruebe también el valor p)"
  },
  {
    "objectID": "modelos_de_regresion.html#varios-predictores-regresión-múltiple",
    "href": "modelos_de_regresion.html#varios-predictores-regresión-múltiple",
    "title": "Modelos de regresión",
    "section": "1.4 Varios predictores: regresión múltiple",
    "text": "1.4 Varios predictores: regresión múltiple\nLa regresión lineal múltiple es una extensión del modelo de regresión lineal simple que puede tomar varios predictores:\n\n\\(\\hat{Y} \\sim \\beta_{o} + \\beta_{1} * x_{1} + \\cdots + \\beta_{n} * x_{n}\\)\n\n \nLa fórmula parece compleja, pero sólo quiere decir que cualquier parámetro adicional tendrá su propia estimación (\\(\\beta\\)). La fórmula para una regresión lineal de dos predictores se ve así:\n\n\\(\\hat{Y} \\sim \\beta_{o} + \\beta_{1} * x_{1} + \\beta_{2} * x_{2}\\)\n\n \n… y se puede simular así:\n\n\nCódigo\n# semila\nset.seed(123)\n\n# numero de observaciones\nn &lt;- 50\nb0 &lt;- -4\nb1 &lt;- 3\nb2 &lt;- -2\nerror &lt;- rnorm(n = n, mean = 0, sd = 3)\n\n# variables aleatorias\nx1 &lt;-  rnorm(n = n, mean = 0, sd = 1)\nx2 &lt;-  rnorm(n = n, mean = 0, sd = 1)\ny &lt;- b0 + b1 * x1 + b2 * x2 + error\n\n# crear un data frame\nxy_datos_multp &lt;- data.frame(x1, x2, y)\n\n# construir el modelo\nxy_mod_multp &lt;- lm(formula = y ~ x1 + x2, data = xy_datos_multp)\n\nsummary(xy_mod_multp)\n\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = xy_datos_multp)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.986 -1.893 -0.363  2.002  6.413 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -3.865      0.417   -9.27  3.5e-12 ***\nx1             2.901      0.453    6.41  6.4e-08 ***\nx2            -1.932      0.414   -4.67  2.6e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.83 on 47 degrees of freedom\nMultiple R-squared:  0.612, Adjusted R-squared:  0.595 \nF-statistic:   37 on 2 and 47 DF,  p-value: 2.23e-10\n\n\n \n… graficar los tamaños de efecto:\n\n\nCódigo\nci_df &lt;- data.frame(param = names(xy_mod_multp$coefficients), \n                    est = xy_mod_multp$coefficients, confint(xy_mod_multp))\n\nggplot(ci_df, aes(x=param, y=est)) + \n  geom_hline(yintercept = 0, color=\"red\", lty = 2) +\n  geom_pointrange(aes(ymin = X2.5.., ymax = X97.5..)) + \n  labs(x = \"Parámetro\", y = \"Tamaño de efecto\") + \n  coord_flip()\n\n\n\n\n\n\n\n\n\n \n… y los gráficos diagnósticos:\n\n\nCódigo\nplot_model(xy_mod_multp, type = \"diag\")\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n \nHay un punto importante que es necesario enfatizar aquí: la regresión múltiple estima el efecto de un predictor después de tener en cuenta el efecto de los demás predictores del modelo. En otras palabras, los nuevos predictores del modelo tratarán de explicar la variación de los datos que no fue explicada por los otros predictores. Así que el resultado de la regresión múltiple no es equivalente a los resultados de las regresiones lineales simples sobre los predictores por separado. Esto puede demostrarse fácilmente corriendo esas regresiones:\n\n\nCódigo\ncoef(xy_mod_multp)# Resumen del modelo de multiple \n\n\n(Intercept)          x1          x2 \n    -3.8652      2.9015     -1.9324 \n\n\nCódigo\n# Regresión simple de Y con A\nmod_x1 &lt;- lm(y ~ x1, data = xy_datos_multp)\ncoef(mod_x1)  # Resumen del modelo de regresión \n\n\n(Intercept)          x1 \n    -3.4228      3.2310 \n\n\nCódigo\n# Regresión simple de Y con A\nmod_x2 &lt;- lm(y ~ x2, data = xy_datos_multp)\ncoef(mod_x2)  # Resumen del modelo de regresión\n\n\n(Intercept)          x2 \n    -3.5456     -2.3468 \n\n\nComo vemos los tamaños de efecto para el mismo predictor difieren entre la regresión multiple y la simple. Esto se debe a que en la regresion multiple ajusta los predictores para que se comporten como si fueran ortogonales entre sí. Esto lo podemos simular sacando los residuos de cada predictor en una regresión contra todos los otros predictores en la regresión multiple. En nuestro caso es fácil ya que solo tenemos 2 predictores:\n\n\nCódigo\n# Regresión de x1 sobre x2 para obtener los residuos\nmod_resid_x1 &lt;- lm(x1 ~ x2, data = xy_datos_multp)\n\n# Obtener los residuos\nxy_datos_multp$residuos_x1 &lt;- resid(mod_resid_x1)  \n\n# Regresión de x2 sobre x1 para obtener los residuos\nmod_resid_x2 &lt;- lm(x2 ~ x1, data = xy_datos_multp)\n\n# Obtener los residuos \nxy_datos_multp$residuos_x2 &lt;- resid(mod_resid_x2)  \n\n\nAhora si corremos las regresiones simples con estos residuos, los tamaños de efecto coinciden con los obtenidos en la regresión multiple:\n\n\nCódigo\n# Regresión de Y sobre los residuos de A\nmod_resid_reg_x1 &lt;- lm(y ~ residuos_x1, data = xy_datos_multp)\n\n# Regresión de Y sobre los residuos de B\nmod_resid_reg_x2 &lt;- lm(y ~ residuos_x2, data = xy_datos_multp)\n\n# Resúmenes finales de los modelos para comparar\ncoef(xy_mod_multp)[-1]\n\n\n     x1      x2 \n 2.9015 -1.9324 \n\n\nCódigo\nc(coef(mod_resid_reg_x1)[-1], coef(mod_resid_reg_x2)[-1])\n\n\nresiduos_x1 residuos_x2 \n     2.9015     -1.9324 \n\n\nPor comodidad, hemos utilizado coef() para extraer sólo los estimado de la regresión, pero los valores son los mismos que obtenemos con summary(model).\nEste punto se demuestra además por el hecho de que, si uno de los predictores no tiene ninguna influencia en la respuesta, el efecto del predictor adicional convergerá a su efecto en una regresión lineal simple. Para simular este escenario, fijamos b2 en 0:\n\n\nCódigo\n# definir semilla\nset.seed(123)\n\n# numero de observaciones\nn &lt;- 50\nb0 &lt;- -4\nb1 &lt;- 3\nb2 &lt;- 0\nerror &lt;- rnorm(n = n, mean = 0, sd = 1)\n\n#  variables aleatorias\nx1 &lt;-  rnorm(n = n, mean = 0, sd = 1)\nx2 &lt;-  rnorm(n = n, mean = 0, sd = 1)\ny &lt;- b0 + b1 * x1 + b2 * x2 + error\n\n# crear data frame\nxy_datos &lt;- data.frame(x1, x2, y)\n\n# construir modelos\nxy_mod &lt;- lm(formula = y ~ x1 + x2, data = xy_datos)\nx1y_mod &lt;- lm(formula = y ~ x1, data = xy_datos)\n\n# shortcut to coefficients\ncoef(xy_mod)\n\n\n(Intercept)          x1          x2 \n  -3.955064    2.967166    0.022549 \n\n\nCódigo\ncoef(x1y_mod)\n\n\n(Intercept)          x1 \n    -3.9602      2.9633 \n\n\nEl estimado de \\(\\beta_1\\) fue casi el mismo en la regresión múltiple (2.96717) y en la regresión de un solo predictor (2.96332)\n\n \n\nInterpretación del modelo\n\n \nCuadro con los coeficientes (estimados):\n\n\n            Estimate Std. Error t value   Pr(&gt;|t|)\n(Intercept)  -3.8652    0.41695 -9.2702 3.4856e-12\nx1            2.9015    0.45260  6.4108 6.4137e-08\nx2           -1.9324    0.41422 -4.6650 2.5851e-05\n\n\n\nEl modelo encontró que \\(\\beta_1\\) (el tamaño de efecto de ‘x1’) es 2.9015 y que es significativamente diferente de 0 (p = 6.41368^{-8}). Este es el efecto de ‘x1’ sobre ‘y’ una vez removida la variación explicada por ‘x2’.\nTambién se encontró que el \\(\\beta_2\\) (el tamaño de efecto de ‘x2’) es -1.93235 y que también es significativamente diferente de 0 (p = 2.58513^{-5}). Este es el efecto de ‘x2’ sobre ‘y’ una vez removida la variación explicada por ‘x1’.\nLos valores simulados de los parámetros de regresión pueden compararse con el resumen del modelo lm() para tener una idea de la precisión del modelo:\n\n\\(\\beta_1\\) se fijó en 3 y se estimó como 2.901.\n\\(\\beta_2\\) (el tamaño de efecto de ‘x2’) se fijó en 0 y se estimó como -1.932.\n\n\n\n \n\n\nCaso de estudio\n\n \n\nAraya-Salas M, P González-Gómez, K Wojczulanis-Jakubas, V López III & T Wright. 2018. Spatial memory is as important as weapon and body size for territorial ownership in a lekking hummingbird. Scientific Reports. 13, e0189969\n\n“La memoria espacial, el tamaño corporal y el largo de la punta del pico … predijeron positivamente la probabilidad de adquirir y defender un territorio.”\n\n\n\n\n\n\n\n\n\n \n\n\nEjercicio 5\n \nLa siguiente simulación genera un par de predictores continuos altamente colineales:\n\n\nCódigo\n# definir semilla\nset.seed(123)\n\n# numero de observaciones\nn &lt;- 30\nb0 &lt;- -4\nb1 &lt;- 3\nb2 &lt;- -2\nerror &lt;- rnorm(n = n, mean = 0, sd = 1)\n\n#  variables aleatorias colineales\nx1 &lt;-  rnorm(n = n, mean = 0, sd = 1)\nx2 &lt;-  x1 + rnorm(n = n, mean = 0, sd = 0.2) # hacer x2 colineal con x1\ny &lt;- b0 + b1 * x1 + b2 * x2 + error\n\n# crear data frame\nxy_datos &lt;- data.frame(x1, x2, y)\n\ncor(x1, x2)\n\n\n[1] 0.97812\n\n\n\n\nCódigo\n# graficar\nggplot(data = xy_datos, aes(x = x1, y = x2)) +\n    geom_point(size = 3)\n\n\n\n\n\n\n\n\n\n\n\n\nConstruya un modelo lineal multiple (con lm()) con ‘y’ como respuesta y ‘x1’ y ‘x2’ como predictores\nCompare los betas estimados por el modelo con los usados para generar los datos. ¿Es una buena estimación?. También observe los valores de p. ¿Esperaría que fueran significativos?"
  },
  {
    "objectID": "modelos_de_regresion.html#añadir-un-predictor-categórico",
    "href": "modelos_de_regresion.html#añadir-un-predictor-categórico",
    "title": "Modelos de regresión",
    "section": "1.5 Añadir un predictor categórico",
    "text": "1.5 Añadir un predictor categórico\nPara los predictores categóricos podemos crear primero una variable binaria (0, 1) y luego añadir etiquetas a cada valor:\n\n\nCódigo\n# definir semilla\nset.seed(13)\n\n# numero de observaciones\nn &lt;- 50\nb0 &lt;- -3\nb1 &lt;- 2\nerror &lt;- rnorm(n = n, mean = 0, sd = 3)\n\n#  variables aleatorias\nx1_num &lt;- sample(0:1, size = n, replace = TRUE)\ny &lt;- b0 + b1 * x1_num + error\n\nx1 &lt;- factor(x1_num, labels = c(\"a\", \"b\"))\n\n# crear data frame\nxy_datos_cat &lt;- data.frame(x1, x1_num, y)\n\nhead(xy_datos_cat)\n\n\n\n\n\n\nx1\nx1_num\ny\n\n\n\n\nb\n1\n0.66298\n\n\na\n0\n-3.84082\n\n\na\n0\n2.32549\n\n\nb\n1\n-0.43804\n\n\na\n0\n0.42758\n\n\na\n0\n-1.75342\n\n\n\n\n\n\n \nY así es como se escribe formalmente:\n\n\\(\\hat{Y} \\sim \\beta_{o} + \\beta_{1} * x_{1}\\)\n\n \nLo mismo que con los predictores continuos.\nPodemos explorar el patrón de los datos utilizando un diagrama de cajas (boxplot):\n\n\nCódigo\n# graficar\nggplot(xy_datos_cat, aes(x = x1, y = y)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n \n… y obtener los estimados del modelo:\n\n\nCódigo\n# construir modelos\nxy_mod_cat &lt;- lm(formula = y ~ x1, data = xy_datos_cat)\n\nsummary(xy_mod_cat)\n\n\n\nCall:\nlm(formula = y ~ x1, data = xy_datos_cat)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.898 -1.909 -0.094  1.809  5.506 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -2.997      0.558   -5.37  2.3e-06 ***\nx1b            1.814      0.842    2.16    0.036 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.95 on 48 degrees of freedom\nMultiple R-squared:  0.0882,    Adjusted R-squared:  0.0693 \nF-statistic: 4.65 on 1 and 48 DF,  p-value: 0.0362\n\n\n \n… graficar los tamaños de efecto:\n\n\nCódigo\nci_df &lt;- data.frame(param = names(xy_mod_cat$coefficients), \n                    est = xy_mod_cat$coefficients, confint(xy_mod_cat))\n\nggplot(ci_df, aes(x=param, y=est)) + \n  geom_hline(yintercept = 0, color=\"red\", lty = 2) +\n  geom_pointrange(aes(ymin = X2.5.., ymax = X97.5..)) + \n  labs(x = \"Parámetro\", y = \"Tamaño de efecto\") + \n  coord_flip()\n\n\n\n\n\n\n\n\n\n \n… y los gráficos diagnósticos del modelo:\n\n\nCódigo\nplot_model(xy_mod_cat, type = \"diag\")[[2]]\n\n\n\n\n\n\n\n\n\n \n\n \n\nInterpretación del modelo\n\n \nCuadro con los coeficientes (estimados):\n\n\n            Estimate Std. Error t value   Pr(&gt;|t|)\n(Intercept)  -2.9974    0.55825 -5.3693 2.2677e-06\nx1b           1.8140    0.84160  2.1554 3.6172e-02\n\n\n\nEl modelo encontró que \\(\\beta_1\\) (el tamaño de efecto de ‘x1’) es 1.814 y que es significativamente diferente de 0 (p = 0.03617)\nLos valores simulados de los parámetros de regresión pueden compararse con el resumen del modelo lm() para tener una idea de la precisión del modelo:\n\n\\(\\beta_1\\) se fijó en 2 y se estimó como 1.814.\n\nTenga en cuenta que en este caso el intercepto se refiere a la estimación del nivel ‘a’ en el predictor categórico, que se utilizó como base:\n\n\n\nCódigo\n# graficar\nggplot(xy_datos_cat, aes(x = x1, y = y)) + \n  geom_boxplot() +\ngeom_hline(yintercept = xy_mod_cat$coefficients[1], col = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n\nPor lo tanto, el intercepto es el mismo que la media de y para la categoría ‘a’:\n\n\n\nCódigo\nmean(xy_datos_cat$y[xy_datos_cat$x1 == \"a\"])\n\n\n[1] -2.9974\n\n\n\nObserve también que la etiqueta del estimado es ‘x1b’, no ‘x1’ como en los predictores continuos. Esto se debe a que en este caso el estimado se refiere a la diferencia entre los dos niveles de la variable categórica (‘a’ y ‘b’). Más concretamente, nos dice que en promedio las observaciones de la categoría ‘b’ son 1.814 más altas que las observaciones de la categoría ‘a’.\n\n \n\n \n\n\nCaso de estudio\n\n \n\nRico-Guevara A, & M Araya-Salas. 2015. Bills as daggers? A test for sexually dimorphic weapons in a lekking hummingbird species. Behavioral Ecology. 26 (1): 21-29.\n\n“Los machos con puntas de pico más grandes y puntiagudas tuvieron más éxito en ganar control de territorios en el lek.”\n\n\n\n\n\n\n\n\n\n \n\n \n\nEjercicio 6\n \n\nLos datos desbalanceados cuando hay categorías (es decir, algunas categorías tienen muchas más observaciones que otras) pueden ser problemáticos para la inferencia estadística. Modifique el código arriba para simular un juego de datos muy desbalanceado y compruebe la precisión del modelo.\n\n\n\n\n\n\nVariables indicadoras (Dummy variables)\n\n\nEn un modelo de regresión, los predictores categóricos también se representan como vectores numéricos. Más concretamente, los predictores categóricos se codifican como 0s y 1s, en los que 1 significa “pertenece a la misma categoría” y 0 “pertenece a una categoría diferente”. Mantuvimos el vector numérico original (‘x1_num’) al simular el juego de datos con el predictor categórico:\n\n\nCódigo\nhead(xy_datos_cat)\n\n\n\n\n\n\nx1\nx1_num\ny\n\n\n\n\nb\n1\n0.66298\n\n\na\n0\n-3.84082\n\n\na\n0\n2.32549\n\n\nb\n1\n-0.43804\n\n\na\n0\n0.42758\n\n\na\n0\n-1.75342\n\n\n\n\n\n\nObserve que las “b” de la columna “x1” se convierten en 1 en la columna “x1_num” y las “a” se convierten en 0. Esto se denomina variable indicadora y el proceso se conoce como ‘codificación indicadora’ (dummy coding).\nEn realidad, podemos utilizar el vector numérico en el modelo de regresión y obtener exactamente los mismos resultados:\n\n\nCódigo\n# summary model with categorical variable\nsummary(xy_mod_cat)$coefficients\n\n\n            Estimate Std. Error t value   Pr(&gt;|t|)\n(Intercept)  -2.9974    0.55825 -5.3693 2.2677e-06\nx1b           1.8140    0.84160  2.1554 3.6172e-02\n\n\nCódigo\n# construir modelos with dummy variable\nxy_mod_num &lt;- lm(formula = y ~ x1_num, data = xy_datos_cat)\n\n# summary with dummy coding\nsummary(xy_mod_num)$coefficients\n\n\n            Estimate Std. Error t value   Pr(&gt;|t|)\n(Intercept)  -2.9974    0.55825 -5.3693 2.2677e-06\nx1_num        1.8140    0.84160  2.1554 3.6172e-02\n\n\n \nLas cosas se complican un poco más cuando se codifica un predictor categórico con más de dos niveles. Pero la lógica es la misma."
  },
  {
    "objectID": "modelos_de_regresion.html#interacciones",
    "href": "modelos_de_regresion.html#interacciones",
    "title": "Modelos de regresión",
    "section": "1.6 Interacciones",
    "text": "1.6 Interacciones\nUna interacción estadística se refiere a un efecto de una variable predictora que está mediado por una segunda variable.\n\n\\(\\hat{Y} \\sim \\beta_{o} + \\beta_{1} * x_{1} + \\beta_{2} * x_{2} + \\beta_{3} * x_{1} * x_{2}\\)\n\n \nEsto es más fácil de entender si se observa la interacción de una variable continua y una binaria:\n\n\nCódigo\n# definir semilla\nset.seed(123)\n\n# numero de observaciones\nn &lt;- 50\nb0 &lt;- -4\nb1 &lt;- 3\nb2 &lt;- 1.7\nb3 &lt;- -3\nerror &lt;- rnorm(n = n, mean = 0, sd = 3)\n\n#  variables aleatorias\nx1 &lt;- rbinom(n = n, size = 1, prob = 0.5)\nx2 &lt;-  rnorm(n = n, mean = 0, sd = 1)\n\n# interaccion se añade como el producto dex1 y x2\ny &lt;- b0 + b1 * x1 + b2 * x2 + b3 * x1 * x2 + error\n\nx1 &lt;- factor(x1, labels = c(\"a\", \"b\"))\n\n# crear data frame\nxy_datos_intr &lt;- data.frame(x1, x2, y)\n\nhead(xy_datos_intr)\n\n\n\n\n\n\nx1\nx2\ny\n\n\n\n\nb\n1.02557\n-4.0147\n\n\na\n-0.28477\n-5.1746\n\n\na\n-1.22072\n-1.3991\n\n\nb\n0.18130\n-1.0242\n\n\na\n-0.13889\n-3.8483\n\n\nb\n0.00576\n4.1377\n\n\n\n\n\n\nCódigo\n# construir modelos\nxy_mod_intr &lt;- lm(formula = y ~ x1 + x2 + x1 * x2, data = xy_datos_intr)\n\n# guardar resumen para graficar lineas de mejor ajuste\nxy_summ_intr &lt;- summary(xy_mod_intr)\n\nxy_summ_intr\n\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x1 * x2, data = xy_datos_intr)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.222 -1.664 -0.158  1.650  6.383 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -4.193      0.570   -7.35  2.7e-09 ***\nx1b            3.597      0.806    4.46  5.2e-05 ***\nx2             1.327      0.682    1.95   0.0576 .  \nx1b:x2        -2.972      0.962   -3.09   0.0034 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.83 on 46 degrees of freedom\nMultiple R-squared:  0.396, Adjusted R-squared:  0.357 \nF-statistic:   10 on 3 and 46 DF,  p-value: 3.29e-05\n\n\nTambién ayuda graficar los datos:\n\n\nCódigo\n# graficar\nggplot(data = xy_datos_intr, aes(x = x2, y = y, color = x1)) +\n    geom_point(size = 3) +\n    geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n \n… y los tamaños de efecto:\n\n\nCódigo\nci_df &lt;- data.frame(param = names(xy_mod_intr$coefficients), \n                    est = xy_mod_intr$coefficients, confint(xy_mod_intr))\n\nggplot(ci_df, aes(x=param, y=est)) + \n  geom_hline(yintercept = 0, color=\"red\", lty = 2) +\n  geom_pointrange(aes(ymin = X2.5.., ymax = X97.5..)) + \n  labs(x = \"Parámetro\", y = \"Tamaño de efecto\") + \n  coord_flip()\n\n\n\n\n\n\n\n\n\n \nTambién deberíamos revisar los gráficos de diagnóstico:\n\n\nCódigo\nplot_model(xy_mod_intr, type = \"diag\")\n\n\nthere are higher-order terms (interactions) in this model\nconsider setting type = 'predictor'; see ?vif\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n \n\n \n\nInterpretación del modelo\n\n \nCuadro con los coeficientes:\n\n\n            Estimate Std. Error t value   Pr(&gt;|t|)\n(Intercept)  -4.1930    0.57023 -7.3532 2.6978e-09\nx1b           3.5974    0.80607  4.4629 5.1950e-05\nx2            1.3274    0.68161  1.9474 5.7610e-02\nx1b:x2       -2.9720    0.96227 -3.0885 3.4061e-03\n\n\n\nEl modelo encontró que \\(\\beta_1\\) (el tamaño de efecto de ‘x1-b’ a ‘x1-a’) es 3.59741 y que es significativamente diferente de 0 (valor p = 5.19496^{-5})\nEl modelo encontró que \\(\\beta_2\\) (el tamaño de efecto de ‘x2’) es 1.32735 y que es significativamente diferente de 0 (p = 0.05761). Esto es en realidad la pendiente de la relación entre x2 e y cuando x1 = `a’\nEl modelo encontró que \\(\\beta_3\\) (el tamaño de efecto del término de interacción ‘x1 * x2’) es -2.97196 y que es significativamente diferente de 0 (p = 0.00341). Esta es la diferencia entre las pendientes de x2 vs y cuando x1 = ‘a’ y x2 vs y cuando x1 = ‘b’.\nLos valores simulados para los parámetros de regresión pueden compararse con el resumen del modelo lm() para tener una idea de la precisión del modelo:\n\n\\(\\beta_1\\) se simuló con un valor de 3 y se estimó en 3.597\n\\(\\beta_2\\) se simuló con un valor de 1.7 y se estimó en 1.327\n\\(\\beta_3\\) se simuló con un valor de -3 y se estimó en -2.972\n\n\n\n\n \n\n\nCaso de estudio\n\n \n\nChirino F, B Wilink, Araya-Salas M. In prep. Climatic factors affecting vocal activity in lemur leaf frogs.\n\n“El aumento de la luz de la luna disminuye la actividad vocal de Agalychnis lemur aunque esta relación es mediada por la temperatura.”"
  },
  {
    "objectID": "modelos_de_regresion.html#lectura-understanding-it-depends-in-ecology-a-guide-to-hypothesising-visualising-and-interpreting-statistical-interactions",
    "href": "modelos_de_regresion.html#lectura-understanding-it-depends-in-ecology-a-guide-to-hypothesising-visualising-and-interpreting-statistical-interactions",
    "title": "Modelos de regresión",
    "section": "Lectura: Understanding ‘it depends’ in ecology: a guide to hypothesising, visualising and interpreting statistical interactions",
    "text": "Lectura: Understanding ‘it depends’ in ecology: a guide to hypothesising, visualising and interpreting statistical interactions\nLos ecólogos utilizan habitualmente modelos estadísticos para detectar y explicar interacciones entre factores ecológicos, con el objetivo de evaluar si un efecto de interés cambia de signo o magnitud en distintos contextos. El artículo trata de los peligros de interpretan las interacciones estadísticas sin prestar atención a su propiedad fundamental de simetría, o a la escala de medida, ya sea aditiva o multiplicativa. Acá pueden acceder al artículo."
  },
  {
    "objectID": "modelos_de_regresion.html#modelos-lineales-generalizados-glm",
    "href": "modelos_de_regresion.html#modelos-lineales-generalizados-glm",
    "title": "Modelos de regresión",
    "section": "2.1 Modelos lineales generalizados (GLM)",
    "text": "2.1 Modelos lineales generalizados (GLM)\nLos GLM nos permiten modelar la asociación a variables respuesta que no siguen una distribución normal. Además, permiten modelar distribuciones que se asemejan más al proceso que generó los datos. El siguiente código crea un juego de datos con una respuesta que representa cuentas (por lo tanto, no normal):\n\n\nCódigo\n# Simulación de datos\nset.seed(123)  # Para reproducibilidad\nn &lt;- 100  # Número de observaciones\nx1 &lt;- rnorm(n)  # Predictor 1: variable normal\nx2 &lt;- rnorm(n)  # Predictor 2: variable normal\n\n# Coeficientes reales para la simulación\nb0 &lt;- 0.5  # Intercepto\nb1 &lt;- 1.2  # Coeficiente para x1\nb2 &lt;- -0.8  # Coeficiente para x2\n\n# Calcular lambda (tasa de eventos) utilizando la combinación lineal de los predictores\n# En el modelo de Poisson, la tasa esperada lambda está relacionada exponencialmente con los predictores\nlog_lambda &lt;- b0 + b1 * x1 + b2 * x2\nlambda &lt;- exp(log_lambda)  # Lambda debe ser positiva, por eso usamos exp()\n\n# Generar la variable respuesta (cuentas) a partir de la distribución de Poisson\ny &lt;- rpois(n, lambda)\n\n# Crear un data frame\nxy_datos_pois &lt;- data.frame(x1 = x1, x2 = x2, y = y)\n\n# Mostrar las primeras filas del data frame\nhead(xy_datos_pois)\n\n\n\n\n\n\nx1\nx2\ny\n\n\n\n\n-0.56048\n-0.71041\n5\n\n\n-0.23018\n0.25688\n0\n\n\n1.55871\n-0.24669\n17\n\n\n0.07051\n-0.34754\n2\n\n\n0.12929\n-0.95162\n4\n\n\n1.71506\n-0.04503\n15\n\n\n\n\n\n\nTambién grafiquemos ‘x1’ vs ‘y’:\n\n\nCódigo\n# graficar\nggplot(xy_datos_pois, aes(x = x1, y = y)) + \n  geom_point() \n\n\n\n\n\n\n\n\n\n \nLa relación no parece muy lineal ni la varianza parece ser constante a través de ‘x1’.\nPodemos relajar el requisito de la distribución normal con GLMs. glm() es una función básica de R que nos ayuda a hacer el truco. Para este ejemplo la distribución más apropiada es Poisson. Esto se puede establecer en el argumento `family’ así:\n\n\nCódigo\nglm_pois  &lt;- glm(formula = y ~ x1 + x2, data = xy_datos_pois, family = poisson())\n\n\n \nQue es equivalente a esto:\n\n\\(\\hat{Y} \\sim_{poisson} \\beta_{o} + \\beta_{1} * x_{1} + \\beta_{2} * x_{2}\\)\n\n \nComo puede ver el único argumento extra comparado con lm() es family. El resto es simplemente la “fórmula” y los “datos” con los que ya estamos familiarizados. Así que, de nuevo, podemos aprovechar nuestros conocimientos sobre los modelos lineales para extenderlos a estructuras de datos más complejas.\nTambién necesitamos ejecutar summary() para obtener el resultado del modelo:\n\n\nCódigo\nsummary(glm_pois)\n\n\n\nCall:\nglm(formula = y ~ x1 + x2, family = poisson(), data = xy_datos_pois)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.4377     0.0867    5.05  4.4e-07 ***\nx1            1.2092     0.0565   21.40  &lt; 2e-16 ***\nx2           -0.8040     0.0582  -13.80  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 920.85  on 99  degrees of freedom\nResidual deviance: 110.28  on 97  degrees of freedom\nAIC: 347.4\n\nNumber of Fisher Scoring iterations: 5\n\n\n \n \n\n \n\nInterpretación del modelo\n\n \nCuadro de los coeficientes (estimados):\n\n\n            Estimate Std. Error  z value    Pr(&gt;|z|)\n(Intercept)  0.43769   0.086656   5.0509  4.3971e-07\nx1           1.20915   0.056501  21.4007 1.3161e-101\nx2          -0.80402   0.058242 -13.8049  2.3824e-43\n\n\n\nEl modelo nos dice que \\(\\beta_1\\) (el tamaño de efecto de ‘x1’) es 1.20915 y que es significativamente diferente de 0 (p = 1.31613^{-101}). Esto se interpreta en realidad como que un aumento de 1 unidad de ‘x1’ resulta en ‘y’ (tasa) en un factor de exp(1.20915) = 3.35064.\nEl modelo también nos dice que \\(\\beta_2\\) (el tamaño de efecto de ‘x2’) es -0.80402 y que es significativamente diferente de 0 (p = 2.38235^{-43}). Esto significa que un aumento en 1 unidad de ‘x2’ resulta en ‘y’ (tasa) en un factor de exp(-0.80402) = 0.44752.\n\n\n \n\n\nCaso de estudio\n\n \n\nTopp, E. N., Tscharntke, T., & Loos, J. (2022). Fire and landscape context shape plant and butterfly diversity in a South African shrubland. Diversity and Distributions, 28(3), 357-371.\n\n“La riqueza de especies de mariposas es de tres a cuatro veces mayor cuando aumenta el hábitat natural en el paisaje circundante (en un radio de 2 km), mientras que la abundancia de mariposas se asociaba negativamente con el aumento del tiempo transcurrido desde el último incendio.”\n\n\n\n\n\n\n\n\n\n \n\n \n\nEjercicio 8\n\nIntente correr el modelo anterior lm() (con una distribución gaussiana), compare los resultados y compruebe los residuos (plot_model(model_name, type = \"diag\"))\n\n\nExisten muchas otras funciones de distribución y enlace:"
  },
  {
    "objectID": "modelos_de_regresion.html#modelos-de-efectos-variables-modelos-mixtos",
    "href": "modelos_de_regresion.html#modelos-de-efectos-variables-modelos-mixtos",
    "title": "Modelos de regresión",
    "section": "2.2 Modelos de efectos variables (modelos mixtos)",
    "text": "2.2 Modelos de efectos variables (modelos mixtos)\nA veces nuestros conjuntos de datos incluyen estructuras con varios niveles de organización. Por ejemplo, cuando tomamos muestras de varios individuos de diferentes poblaciones. En esos casos, la variación en el nivel estructural superior (poblaciones) podría impedir la detección de patrones en el nivel inferior (individuos).\nVamos a simular algunos datos que se asemejan a ese escenario. Tenemos dos predictores continuos (x1) y una respuesta continua (y). Cada muestra procede de 1 de 8 poblaciones diferentes (poblacion):\n\n\nCódigo\n# x&lt;- 1\n# definir semilla\nset.seed(28)\n\n# numero de observaciones\nn &lt;- 300\nb0 &lt;- 1\nb1 &lt;- 1.3\npoblacion &lt;- sample(0:8, size = n, replace = TRUE)\nerror &lt;- rnorm(n = n, mean = 0, sd = 2)\n\n#  variables aleatorias\nx1 &lt;-  rnorm(n = n, mean = 0, sd = 1)\ny &lt;- b0 + poblacion * 2 + b1 * x1 + error\n\n# add letters\npoblacion &lt;- letters[poblacion + 1]\n\n# create data set\nxy_datos_mixto &lt;- data.frame(x1, y, poblacion)\n\nhead(xy_datos_mixto, 10)\n\n\n\n\n\n\nx1\ny\npoblacion\n\n\n\n\n1.54607\n3.3378\na\n\n\n0.56834\n3.1399\na\n\n\n-0.63855\n15.2824\ni\n\n\n0.98424\n2.6610\na\n\n\n1.05355\n3.7312\nb\n\n\n-0.82294\n3.0189\na\n\n\n-1.32373\n3.2186\nc\n\n\n2.17854\n19.0542\ni\n\n\n2.26484\n15.4549\ni\n\n\n-0.77843\n11.6670\nh\n\n\n\n\n\n\n \nPodemos explorar la relación entre ‘y’ y ‘x1’ con un gráfico:\n\n\nCódigo\nggplot(data = xy_datos_mixto, aes(x = x1, y = y)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\n \n¿Puede ver claramente el patrón de asociación entre las dos variables que hemos utilizado para simular los datos? Podemos seguir explorando los datos con un modelo de regresión lineal simple:\n\n\nCódigo\nsummary(lm(y ~ x1, data = xy_datos_mixto))\n\n\n\nCall:\nlm(formula = y ~ x1, data = xy_datos_mixto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.755  -5.119   0.186   4.602  12.409 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    8.852      0.325   27.21   &lt;2e-16 ***\nx1             0.633      0.329    1.92    0.055 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.63 on 298 degrees of freedom\nMultiple R-squared:  0.0123,    Adjusted R-squared:  0.00895 \nF-statistic:  3.7 on 1 and 298 DF,  p-value: 0.0554\n\n\n \nA pesar de haber simulado un \\(\\beta_1\\) distinto de cero, no tenemos ninguna asociación significativa según este modelo y la estimación de \\(\\beta_1\\) está muy lejos de la simulada. Esta pobre inferencia se debe a que estamos ignorando una característica importante de nuestros datos, la agrupación de las muestras en “poblaciones”.\nLos modelos de efectos mixtos (también conocidos como modelos multinivel o modelos de efectos variables) pueden ayudarnos a tener en cuenta estas características adicionales, mejorando significativamente nuestro poder de inferencia. Coloreemos cada una de las poblaciones para ver cómo covarían las variables en cada subgrupo de datos:\n\n\nCódigo\nggplot(data = xy_datos_mixto, aes(x = x1, y = y, color = poblacion)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\n \nParece haber un patrón claro de asociación positiva entre x1 e y. El patrón se hace un poco más evidente si mostramos cada población en su propio panel:\n\n\nCódigo\nggplot(data = xy_datos_mixto, aes(x = x1, y = y, color = poblacion)) +\n  geom_point() +\n  facet_wrap( ~ poblacion) +\n  geom_smooth(method = \"lm\", se = FALSE) \n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n \nConstruyamos un modelo de efectos mixtos utilizando la población como intercepto variable:\n\n\nCódigo\nmix_eff_mod &lt;- lmer(formula = y ~ x1 + (1 | poblacion), data = xy_datos_mixto)\n\n\nQue es equivalente a esto:\n\n\\(\\hat{Y} \\sim_{gausiana} (\\beta_{o} + \\beta_{grupo}) + \\beta_{1} * x_{1}\\)\n\n \nPodemos ver el resultado del modelo igual que con un modelo lm():\n\n\nCódigo\nsummary(mix_eff_mod)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: y ~ x1 + (1 | poblacion)\n   Data: xy_datos_mixto\n\nREML criterion at convergence: 1296.4\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-3.524 -0.657 -0.032  0.612  3.245 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n poblacion (Intercept) 30.07    5.48    \n Residual               3.76    1.94    \nNumber of obs: 300, groups:  poblacion, 9\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(&gt;|t|)    \n(Intercept)    8.788      1.831   8.003     4.8   0.0014 ** \nx1             1.317      0.115 290.068    11.4   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n   (Intr)\nx1 -0.003\n\n\n \nEl modelo detectó correctamente el patrón simulado y la estimación de \\(\\beta_1\\) (1.317) es muy cercana al valor simulado.\n\n \n\nInterpretación del modelo\n\n \nCuadro con los coeficientes (estimados):\n\n\n            Estimate Std. Error       df t value   Pr(&gt;|t|)\n(Intercept)   8.7881     1.8315   8.0031  4.7983 1.3569e-03\nx1            1.3166     0.1151 290.0679 11.4395 2.9326e-25\n\n\n \n\nEl modelo encontró que \\(\\beta_1\\) (el tamaño de efecto de ‘x1’) es 1.31665 y que es significativamente diferente de 0 (p = 2.93263^{-25})\nLos valores simulados de los parámetros de regresión pueden compararse con el resumen del modelo lmer() para tener una idea de la precisión del modelo:\n\n\\(\\beta_1\\) se fijó en 1.3 y se estimó como 1.317.\n\nLa varianza del intercepto para cada población fue de 29.95624 y la desviación estándar de 5.47323\nLos interceptos estimados para cada población son:\n\n\n\nCódigo\nranef(mix_eff_mod)$poblacion\n\n\n\n\n\n\n\n(Intercept)\n\n\n\n\na\n-8.22835\n\n\nb\n-6.17134\n\n\nc\n-4.23048\n\n\nd\n-1.30838\n\n\ne\n0.24573\n\n\nf\n2.22552\n\n\ng\n3.91556\n\n\nh\n5.75609\n\n\ni\n7.79566\n\n\n\n\n\n\n \n\nPodemos compararlos con los promedios para cada población en los datos:\n\n\n\nCódigo\n# sumar interceptos por poblacion a el intercepto total\ninterceptos &lt;- ranef(mix_eff_mod)$poblacion + fixef(mix_eff_mod)[1]\n\n# calcular los promedios por poblacion en los datos \nprom_pobs &lt;- aggregate(y ~ poblacion, xy_datos_mixto, mean)\n\n# unir\ncbind(interceptos, prom_pobs= prom_pobs[,2])\n\n\n\n\n\n\n\n(Intercept)\nprom_pobs\n\n\n\n\na\n0.55975\n0.86151\n\n\nb\n2.61677\n2.76860\n\n\nc\n4.55762\n4.90112\n\n\nd\n7.47972\n7.73174\n\n\ne\n9.03384\n8.87649\n\n\nf\n11.01363\n10.77650\n\n\ng\n12.70366\n12.62651\n\n\nh\n14.54420\n14.62110\n\n\ni\n16.58376\n16.42520\n\n\n\n\n\n\n \n\nLa varianza entre los niveles del factor aleatorio (poblacion) es de 29.95624 y la desviación estándar de 5.47323\n\n \n\n\n\nCaso de estudio\n\n\nAraya-Salas M, P González-Gómez, K Wojczulanis-Jakubas, V López III & T Wright. 2018. Spatial memory is as important as weapon and body size for territorial ownership in a lekking hummingbird. Scientific Reports. 13, e0189969\n\n“… el tamaño corporal mostró una correlación negativa con la frecuencia baja del canto.”\n\n\n\n\n\n\n\n\n\n \n\n\n2.2.1 La paradoja de Simpson\nLa paradoja de Simpson es un fenómeno en estadística en el que una tendencia aparece en varios grupos de datos pero desaparece o se invierte cuando se combinan los grupos. Se ha utilizado para ilustrar el tipo de resultados engañosos que puede generar el ignorar la estructura en los datos y las relaciones causales entre variables.\n\n\n\n\n\n\n\n\n\nTomado de wikipedia\n \nLa estructura de los datos debido a observaciones pertenecientes al mismo grupo puede ser tomada en cuenta usando el grupo como un factor aleatorio, tal y como se hizo en el ejemplo anterior.\n \n\n\nModelos lineales como abordaje estadístico estándar para la inferencia causal \n\n\n\n\n\n\n\n\n\n\n\n\\(\\hat{Y} \\sim_{función\\ de\\ enlace} (\\beta_{o} + \\beta_{grupo}) + \\beta_{1} * x_{1} + \\cdots + \\beta_{n} * x_{n}\\)\n\n\nEste modelo básico incluye una función de enlace que cuando es gausiana (i.e. normal) equivale a un modelo lineal y cuando no, a un modelo generalizado\nCuando el \\(\\beta_{grupo}\\) es diferente entre grupos (i.e. cuando el intercepto difiere entre grupos) es un modelo mixto con intercepto variable (i.e. aleatorio)\n\n \n\n \nEste manual solo pretende sugerir un abordaje estadístico centrado en los modelos de regresión desde el cual se pueden llevar a cabo adecuadamente la mayoría de los análisis estadísticos que usamos en investigación. El manual no tiene como objetivo explicar en detalle las particularidades de los modelos generalizados y mixtos. Note que estos modelos, principalmente los mixtos, pueden adaptarse a otras estructuras de datos mas complejas no cubiertas aquí, como lo son las pendientes aleatorias, pendientes e interceptos aleatorios, estructuras de correlación (i.e. autocorrelación espacial o temporal, pedigrís o árboles filogenéticos), medidas repetidas y el uso de pseudoreplicas, entre otras."
  },
  {
    "objectID": "modelos_de_regresion.html#referencias",
    "href": "modelos_de_regresion.html#referencias",
    "title": "Modelos de regresión",
    "section": "2.3 Referencias",
    "text": "2.3 Referencias\n\nRichard McElreath’s Statistical Rethinking book\nSpake, R., Bowler, D. E., Callaghan, C. T., Blowes, S. A., Doncaster, C. P., Antao, L. H., … & Chase, J. M. (2023). Understanding ‘it depends’ in ecology: a guide to hypothesising, visualising and interpreting statistical interactions. Biological Reviews.\nComplete introduction to linear regression in R\nLinear regression in R\nHarrison, X. A., Donaldson, L., Correa-Cano, M. E., Evans, J., Fisher, D. N., Goodwin, C. E., & Inger, R. (2018). A brief introduction to mixed effects modelling and multi-model inference in ecology. PeerJ, 6, e4794."
  },
  {
    "objectID": "importar_datos.html",
    "href": "importar_datos.html",
    "title": "Importar y exportar datos en R",
    "section": "",
    "text": "Código\nsketchy::load_packages(c(\n  \"RColorBrewer\",\n  \"ggplot2\",\n  \"readxl\",\n  \"tidyr\",\n  \"dplyr\"\n))"
  },
  {
    "objectID": "importar_datos.html#establecer-el-directorio-de-trabajo",
    "href": "importar_datos.html#establecer-el-directorio-de-trabajo",
    "title": "Importar y exportar datos en R",
    "section": "2.1 Establecer el directorio de trabajo",
    "text": "2.1 Establecer el directorio de trabajo\nPara leer datos en R es necesario especificar el directorio de trabajo. Se puede establecer con la función setwd(). La forma de hacerlo depende del sistema operativo (windows, mac, Linux). La sintaxis del directorio sigue la estructura anidada de las carpetas. Por ejemplo:\n\n\nCódigo\nsetwd(\"/home/m/Desktop/\")\n\n\n… establece el directorio de trabajo en la carpeta “Desktop”, que se encuentra dentro de “m”, que se encuentra dentro de “home”.\n..;\nAlgunos consejos básicos para establecer el directorio de trabajo:\n\nAsegúrese de que la ubicación está entre comillas\nAsegúrese de que hay una barra diagonal (/) entre los nombres de las carpetas (aunque las barras diagonales dobles parecen funcionar en Windows).\nNo incluya ningún nombre de archivo en el nombre del directorio de la carpeta.\nPara encontrar la ubicación, puede consultar las propiedades de un archivo de esa carpeta y copiarlo.\nLa ruta a la carpeta debe ir entre comillas (““)\nEl nombre debe coincidir exactamente (mejor copiar/pegar)\nUtilice list.files() para comprobar qué archivos están en el directorio de trabajo\nR puede sugerir y autocompletar los nombres de las carpetas pulsando “tab” cuando están entre comillas:\n\n \n\n \n\n2.1.1 Establecer el directorio de trabajo en windows\nEn windows debería ser algo como esto:\n\n\nCódigo\nsetwd(\"C:/location\")\n\n\nTambién pueden hacerlo manualmente (¡solo en windows!):\n\n\nCódigo\nsetwd(choose.dir())\n\n\n \nDebería aparecer una ventana en la que puede elegir la ubicación. Sin embargo, esto sólo se debe utilizar para averiguar la forma correcta de escribir la ubicación del directorio, no como parte de la propia secuencia de comandos.\n \n\n\n2.1.2 Establecer el directorio de trabajo en macOS\nPara mac establecer el directorio de trabajo debe ser algo como esto:\n\n\nCódigo\nsetwd(\"/Users/yourname/..\")\n\n\nno incluya lo que tenga antes de “usuarios” (como en windows)\n \n\n\n2.1.3 Establecer el directorio de trabajo en Linux\nSimilar al código usado en OSX:\n\n\nCódigo\nsetwd(\"/home/m/Desktop/\")\n\n\n \nLa “~” (virgulilla) también puede utilizarse para omitir la carpeta “home” y “user” en Linux:\n\n\nCódigo\nsetwd(\"~/Desktop/\")\n\n\n \nEl directorio de trabajo actual se puede comprobar de la siguiente manera:\n\n\nCódigo\ngetwd()\n\n\n[1] \"/home/m/Dropbox/courses_and_workshops/estadistica/aprendizaje_estadistico_2024\""
  },
  {
    "objectID": "importar_datos.html#lectura-de-datos",
    "href": "importar_datos.html#lectura-de-datos",
    "title": "Importar y exportar datos en R",
    "section": "2.2 Lectura de datos",
    "text": "2.2 Lectura de datos\nCualquier archivo puede ser leído en R. Sólo es cuestión de hacer saber a R en qué formato está codificado el archivo (por ejemplo, qué convenciones se siguieron al generarlo). Los formatos más comunes para almacenar/intercambiar conjuntos de datos como los que solemos manejar en ciencias biológicas son txt, csv y xls/xlsx.\n \nLa función más utilizada para importar datos en R es read.table. De hecho, la documentación de esta función incluye todas las funciones por defecto para introducir datos:\n\n\nCódigo\n?read.table"
  },
  {
    "objectID": "importar_datos.html#leer-archivos-.txt",
    "href": "importar_datos.html#leer-archivos-.txt",
    "title": "Importar y exportar datos en R",
    "section": "2.3 Leer archivos .txt",
    "text": "2.3 Leer archivos .txt\nLos archivos .txt pueden leerse con read.table. Descarguemos primero un conjunto de datos de libre acceso en formato .txt:\n\n\nCódigo\n# definir el directorio de trabajo\nsetwd(\"INGRESE LA DIRECCION DE LA CARPETA DONDE ESTAN LOS DATOS\")\n\ndownload.file(\n  \"https://raw.githubusercontent.com/maRce10/OTS_TBCP_2024/master/data/pantheria_mammals_data.txt\",\n  destfile = \"pantheria_mammals_data.txt\"\n)\n\n\nTambien pueden bajarlos de acá\n \nEl archivo puede introducirse en R de la siguiente forma:\n\n\nCódigo\n# leer datos\ndatos_pantheria &lt;-\n  read.table(\"pantheria_mammals_data.txt\",\n             sep = \"\\t\",\n             header = TRUE)\n\n\n\n\nCódigo\n# explorar estructura\nhead(datos_pantheria)\n\n\n\n\n\n\n\n\n\nMSW93_Order\nMSW93_Family\nMSW93_Genus\nMSW93_Species\nMSW93_Binomial\nX1.1_ActivityCycle\nX5.1_AdultBodyMass_g\nX8.1_AdultForearmLen_mm\nX13.1_AdultHeadBodyLen_mm\nX2.1_AgeatEyeOpening_d\nX3.1_AgeatFirstBirth_d\nX18.1_BasalMetRate_mLO2hr\nX5.2_BasalMetRateMass_g\nX6.1_DietBreadth\nX7.1_DispersalAge_d\nX9.1_GestationLen_d\nX12.1_HabitatBreadth\nX22.1_HomeRange_km2\nX22.2_HomeRange_Indiv_km2\nX14.1_InterBirthInterval_d\nX15.1_LitterSize\nX16.1_LittersPerYear\nX17.1_MaxLongevity_m\nX5.3_NeonateBodyMass_g\nX13.2_NeonateHeadBodyLen_mm\nX21.1_PopulationDensity_n.km2\nX10.1_PopulationGrpSize\nX23.1_SexualMaturityAge_d\nX10.2_SocialGrpSize\nX24.1_TeatNumber\nX12.2_Terrestriality\nX6.2_TrophicLevel\nX25.1_WeaningAge_d\nX5.4_WeaningBodyMass_g\nX13.3_WeaningHeadBodyLen_mm\nReferences\nX5.5_AdultBodyMass_g_EXT\nX16.2_LittersPerYear_EXT\nX5.6_NeonateBodyMass_g_EXT\nX5.7_WeaningBodyMass_g_EXT\nX26.1_GR_Area_km2\nX26.2_GR_MaxLat_dd\nX26.3_GR_MinLat_dd\nX26.4_GR_MRLat_dd\nX26.5_GR_MaxLong_dd\nX26.6_GR_MinLong_dd\nX26.7_GR_MRLong_dd\nX27.1_HuPopDen_Min_n.km2\nX27.2_HuPopDen_Mean_n.km2\nX27.3_HuPopDen_5p_n.km2\nX27.4_HuPopDen_Change\nX28.1_Precip_Mean_mm\nX28.2_Temp_Mean_01degC\nX30.1_AET_Mean_mm\nX30.2_PET_Mean_mm\n\n\n\n\nRodentia\nMuridae\nAbditomys\nlatidens\nAbditomys latidens\n-999\n268\n-999.00\n223.99\n-999\n-999\n-999\n-999\n-999\n-999\n-999.00\n-999\n-999\n-999\n-999\n-999.00\n-999\n-999.0\n-999.0\n-999\n-999\n-999\n-999\n-999\n-999\n-999\n-999\n-999\n-999\n-999\n2152\n-999\n-999\n-999\n-999\n357\n16.94\n16.74\n16.84\n120.97\n120.78\n120.88\n93\n93.00\n93\n0.09\n316.00\n180.00\n1443.00\n1557.0\n\n\nRodentia\nMuridae\nAbrawayaomys\nruschii\nAbrawayaomys ruschii\n-999\n63\n-999.00\n-999.00\n-999\n-999\n-999\n-999\n-999\n-999\n-999.00\n-999\n-999\n-999\n-999\n-999.00\n-999\n-999.0\n-999.0\n-999\n-999\n-999\n-999\n-999\n-999\n-999\n-999\n-999\n-999\n-999\n2655\n-999\n-999\n-999\n-999\n126137\n-15.09\n-19.84\n-17.47\n-40.52\n-43.56\n-42.04\n5\n21.21\n7\n0.06\n107.06\n209.10\n1084.47\n1402.4\n\n\nRodentia\nAbrocomidae\nAbrocoma\nbennettii\nAbrocoma bennettii\n1\n251\n-999.00\n-999.00\n-999\n-999\n-999\n-999\n-999\n-999\n-999.00\n3\n-999\n-999\n-999\n4.86\n-999\n27.6\n-999.0\n-999\n142\n-999\n-999\n-999\n-999\n2\n-999\n-999\n-999\n-999\n543;890;1297;1492;2655\n-999\n-999\n-999\n-999\n54616\n-27.71\n-35.58\n-31.64\n-69.40\n-71.70\n-70.55\n0\n63.15\n1\n0.07\n20.44\n17.66\n213.09\n1073.8\n\n\nRodentia\nAbrocomidae\nAbrocoma\nboliviensis\nAbrocoma boliviensis\n-999\n158\n-999.00\n-999.00\n-999\n-999\n-999\n-999\n-999\n-999\n-999.00\n-999\n-999\n-999\n-999\n-999.00\n-999\n-999.0\n-999.0\n-999\n-999\n-999\n-999\n-999\n-999\n-999\n-999\n-999\n-999\n-999\n2655\n-999\n-999\n-999\n-999\n5774\n-17.44\n-18.05\n-17.75\n-63.49\n-64.51\n-64.00\n5\n29.24\n5\n0.04\n82.63\n175.55\n1171.61\n1487.3\n\n\nRodentia\nAbrocomidae\nAbrocoma\ncinerea\nAbrocoma cinerea\n-999\n194\n-999.00\n-999.00\n-999\n-999\n-999\n-999\n-999\n-999\n109.64\n3\n-999\n-999\n-999\n2.20\n-999\n-999.0\n13.3\n-999\n-999\n-999\n-999\n-999\n-999\n2\n-999\n-999\n-999\n-999\n890;1297;1627;2655\n-999\n-999\n-999\n-999\n381391\n-14.43\n-24.35\n-19.39\n-64.27\n-70.60\n-67.43\n0\n13.72\n0\n0.06\n48.46\n35.22\n515.10\n1257.3\n\n\nChiroptera\nPteropodidae\nAcerodon\ncelebensis\nAcerodon celebensis\n-999\n382\n133.49\n201.55\n-999\n-999\n-999\n-999\n-999\n-999\n-999.00\n1\n-999\n-999\n-999\n0.98\n-999\n-999.0\n-999.0\n-999\n-999\n-999\n-999\n-999\n-999\n2\n-999\n-999\n-999\n-999\n978;1297;1658;2151;2655\n-999\n-999\n-999\n-999\n173613\n3.74\n-6.49\n-1.37\n126.34\n118.75\n122.55\n5\n92.68\n17\n0.07\n229.41\n215.47\n1812.60\n1909.2\n\n\n\n\n\n\n\n\n\n \nEl nombre del archivo está entre comillas y contiene la extensión del archivo.\n \nTenga en cuenta que el valor -999 se utiliza para definir celdas vacías. Podemos leer estos valores como NAs al importar los datos utilizando el argumento ‘na.strings’:\n\n\nCódigo\n# leer datos\ndatos_pantheria &lt;-\n  read.table(\n    \"pantheria_mammals_data.txt\",\n    sep = \"\\t\",\n    header = TRUE,\n    na.strings = \"-999\"\n  )\n\n# explorar estructura\nhead(datos_pantheria)\n\n\n\n\n\n\n\n\n\nMSW93_Order\nMSW93_Family\nMSW93_Genus\nMSW93_Species\nMSW93_Binomial\nX1.1_ActivityCycle\nX5.1_AdultBodyMass_g\nX8.1_AdultForearmLen_mm\nX13.1_AdultHeadBodyLen_mm\nX2.1_AgeatEyeOpening_d\nX3.1_AgeatFirstBirth_d\nX18.1_BasalMetRate_mLO2hr\nX5.2_BasalMetRateMass_g\nX6.1_DietBreadth\nX7.1_DispersalAge_d\nX9.1_GestationLen_d\nX12.1_HabitatBreadth\nX22.1_HomeRange_km2\nX22.2_HomeRange_Indiv_km2\nX14.1_InterBirthInterval_d\nX15.1_LitterSize\nX16.1_LittersPerYear\nX17.1_MaxLongevity_m\nX5.3_NeonateBodyMass_g\nX13.2_NeonateHeadBodyLen_mm\nX21.1_PopulationDensity_n.km2\nX10.1_PopulationGrpSize\nX23.1_SexualMaturityAge_d\nX10.2_SocialGrpSize\nX24.1_TeatNumber\nX12.2_Terrestriality\nX6.2_TrophicLevel\nX25.1_WeaningAge_d\nX5.4_WeaningBodyMass_g\nX13.3_WeaningHeadBodyLen_mm\nReferences\nX5.5_AdultBodyMass_g_EXT\nX16.2_LittersPerYear_EXT\nX5.6_NeonateBodyMass_g_EXT\nX5.7_WeaningBodyMass_g_EXT\nX26.1_GR_Area_km2\nX26.2_GR_MaxLat_dd\nX26.3_GR_MinLat_dd\nX26.4_GR_MRLat_dd\nX26.5_GR_MaxLong_dd\nX26.6_GR_MinLong_dd\nX26.7_GR_MRLong_dd\nX27.1_HuPopDen_Min_n.km2\nX27.2_HuPopDen_Mean_n.km2\nX27.3_HuPopDen_5p_n.km2\nX27.4_HuPopDen_Change\nX28.1_Precip_Mean_mm\nX28.2_Temp_Mean_01degC\nX30.1_AET_Mean_mm\nX30.2_PET_Mean_mm\n\n\n\n\nRodentia\nMuridae\nAbditomys\nlatidens\nAbditomys latidens\nNA\n268\nNA\n223.99\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n2152\nNA\nNA\nNA\nNA\n357\n16.94\n16.74\n16.84\n120.97\n120.78\n120.88\n93\n93.00\n93\n0.09\n316.00\n180.00\n1443.00\n1557.0\n\n\nRodentia\nMuridae\nAbrawayaomys\nruschii\nAbrawayaomys ruschii\nNA\n63\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n2655\nNA\nNA\nNA\nNA\n126137\n-15.09\n-19.84\n-17.47\n-40.52\n-43.56\n-42.04\n5\n21.21\n7\n0.06\n107.06\n209.10\n1084.47\n1402.4\n\n\nRodentia\nAbrocomidae\nAbrocoma\nbennettii\nAbrocoma bennettii\n1\n251\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n3\nNA\nNA\nNA\n4.86\nNA\n27.6\nNA\nNA\n142\nNA\nNA\nNA\nNA\n2\nNA\nNA\nNA\nNA\n543;890;1297;1492;2655\nNA\nNA\nNA\nNA\n54616\n-27.71\n-35.58\n-31.64\n-69.40\n-71.70\n-70.55\n0\n63.15\n1\n0.07\n20.44\n17.66\n213.09\n1073.8\n\n\nRodentia\nAbrocomidae\nAbrocoma\nboliviensis\nAbrocoma boliviensis\nNA\n158\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n2655\nNA\nNA\nNA\nNA\n5774\n-17.44\n-18.05\n-17.75\n-63.49\n-64.51\n-64.00\n5\n29.24\n5\n0.04\n82.63\n175.55\n1171.61\n1487.3\n\n\nRodentia\nAbrocomidae\nAbrocoma\ncinerea\nAbrocoma cinerea\nNA\n194\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n109.64\n3\nNA\nNA\nNA\n2.20\nNA\nNA\n13.3\nNA\nNA\nNA\nNA\nNA\nNA\n2\nNA\nNA\nNA\nNA\n890;1297;1627;2655\nNA\nNA\nNA\nNA\n381391\n-14.43\n-24.35\n-19.39\n-64.27\n-70.60\n-67.43\n0\n13.72\n0\n0.06\n48.46\n35.22\n515.10\n1257.3\n\n\nChiroptera\nPteropodidae\nAcerodon\ncelebensis\nAcerodon celebensis\nNA\n382\n133.49\n201.55\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n1\nNA\nNA\nNA\n0.98\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n2\nNA\nNA\nNA\nNA\n978;1297;1658;2151;2655\nNA\nNA\nNA\nNA\n173613\n3.74\n-6.49\n-1.37\n126.34\n118.75\n122.55\n5\n92.68\n17\n0.07\n229.41\n215.47\n1812.60\n1909.2"
  },
  {
    "objectID": "importar_datos.html#leer-archivos-.csv",
    "href": "importar_datos.html#leer-archivos-.csv",
    "title": "Importar y exportar datos en R",
    "section": "2.4 Leer archivos .csv",
    "text": "2.4 Leer archivos .csv\nDe nuevo, podemos descargar un archivo de ejemplo en línea:\n\n\nCódigo\n# bajar datos en csv\ndownload.file(\"https://github.com/maRce10/OTS_TBCP_2024/raw/master/data/clements_bird_list.csv\", destfile = \"clements_bird_list.csv\")\n\n# leer datos\nclm_lst &lt;- read.csv(\"clements_bird_list.csv\")\n\n# explorarlos\nhead(clm_lst)\n\n\n\n\n\n\n\n\n\nTAXON_ORDER\nCATEGORY\nSPECIES_CODE\nPRIMARY_COM_NAME\nSCI_NAME\nORDER1\nFAMILY\nSPECIES_GROUP\nREPORT_AS\n\n\n\n\n3\nspecies\nostric2\nCommon Ostrich\nStruthio camelus\nStruthioniformes\nStruthionidae (Ostriches)\nOstriches\n\n\n\n5\nspecies\nostric3\nSomali Ostrich\nStruthio molybdophanes\nStruthioniformes\nStruthionidae (Ostriches)\n\n\n\n\n6\nslash\ny00934\nCommon/Somali Ostrich\nStruthio camelus/molybdophanes\nStruthioniformes\nStruthionidae (Ostriches)\n\n\n\n\n7\nspecies\ngrerhe1\nGreater Rhea\nRhea americana\nRheiformes\nRheidae (Rheas)\nRheas\n\n\n\n13\nspecies\nlesrhe2\nLesser Rhea\nRhea pennata\nRheiformes\nRheidae (Rheas)\n\n\n\n\n14\nissf\nlesrhe4\nLesser Rhea (Puna)\nRhea pennata tarapacensis/garleppi\nRheiformes\nRheidae (Rheas)\n\nlesrhe2\n\n\n\n\n\n\n\n\n\nTambién puede descargar manualmente el archivo desde aquí\n \nComo en el ejemplo anterior, podemos decirle a R cómo identificar las celdas vacías utilizando el argumento na.strings:\n\n\nCódigo\n# leer datos\nclm_lst &lt;- read.csv(\"clements_bird_list.csv\", na.strings = \"\")\n\nhead(clm_lst)\n\n\n\n\n\n\n\n\n\nTAXON_ORDER\nCATEGORY\nSPECIES_CODE\nPRIMARY_COM_NAME\nSCI_NAME\nORDER1\nFAMILY\nSPECIES_GROUP\nREPORT_AS\n\n\n\n\n3\nspecies\nostric2\nCommon Ostrich\nStruthio camelus\nStruthioniformes\nStruthionidae (Ostriches)\nOstriches\nNA\n\n\n5\nspecies\nostric3\nSomali Ostrich\nStruthio molybdophanes\nStruthioniformes\nStruthionidae (Ostriches)\nNA\nNA\n\n\n6\nslash\ny00934\nCommon/Somali Ostrich\nStruthio camelus/molybdophanes\nStruthioniformes\nStruthionidae (Ostriches)\nNA\nNA\n\n\n7\nspecies\ngrerhe1\nGreater Rhea\nRhea americana\nRheiformes\nRheidae (Rheas)\nRheas\nNA\n\n\n13\nspecies\nlesrhe2\nLesser Rhea\nRhea pennata\nRheiformes\nRheidae (Rheas)\nNA\nNA\n\n\n14\nissf\nlesrhe4\nLesser Rhea (Puna)\nRhea pennata tarapacensis/garleppi\nRheiformes\nRheidae (Rheas)\nNA\nlesrhe2"
  },
  {
    "objectID": "importar_datos.html#leer-archivos-de-excel",
    "href": "importar_datos.html#leer-archivos-de-excel",
    "title": "Importar y exportar datos en R",
    "section": "2.5 Leer archivos de Excel",
    "text": "2.5 Leer archivos de Excel\nLa mayoría de los investigadores introducen los datos en hojas de cálculo Excel. Así que sería bastante práctico leer los datos directamente desde ahí. Para leer archivos xls y xlsx necesitamos instalar el paquete “readxl” (hay otros paquetes que se pueden utilizar pero todos funcionan de forma similar):\n\n\nCódigo\ninstall.packages(pkgs = \"readxl\")\n\nlibrary(readxl)\n\n\n \nComo hicimos anteriormente, descargue un archivo de ejemplo de un repositorio en línea. En este caso es la misma lista de taxonomía de aves de Clements en formato xlsx:\n\n\nCódigo\ndownload.file(\"https://github.com/maRce10/OTS_TBCP_2024/raw/master/data/clements_bird_list.xlsx\", destfile = \"clements_bird_list.xlsx\")\n\n\nTambién puede descargar manualmente el archivo desde aquí\n \nAhora podemos utilizar la función read_excel() para leer el archivo:\n\n\nCódigo\n# leer archivo\nclm_lst2 &lt;- read_excel(\"clements_bird_list.xlsx\", sheet = 1)\n\nhead(clm_lst2)\n\n\n\n\n\n\n\n\n\nTAXON_ORDER\nCATEGORY\nSPECIES_CODE\nPRIMARY_COM_NAME\nSCI_NAME\nORDER1\nFAMILY\nSPECIES_GROUP\nREPORT_AS\n\n\n\n\n3\nspecies\nostric2\nCommon Ostrich\nStruthio camelus\nStruthioniformes\nStruthionidae (Ostriches)\nOstriches\nNA\n\n\n5\nspecies\nostric3\nSomali Ostrich\nStruthio molybdophanes\nStruthioniformes\nStruthionidae (Ostriches)\nNA\nNA\n\n\n6\nslash\ny00934\nCommon/Somali Ostrich\nStruthio camelus/molybdophanes\nStruthioniformes\nStruthionidae (Ostriches)\nNA\nNA\n\n\n7\nspecies\ngrerhe1\nGreater Rhea\nRhea americana\nRheiformes\nRheidae (Rheas)\nRheas\nNA\n\n\n13\nspecies\nlesrhe2\nLesser Rhea\nRhea pennata\nRheiformes\nRheidae (Rheas)\nNA\nNA\n\n\n14\nissf\nlesrhe4\nLesser Rhea (Puna)\nRhea pennata tarapacensis/garleppi\nRheiformes\nRheidae (Rheas)\nNA\nlesrhe2\n\n\n\n\n\n\n\n\n\n \nYou need to specify the file name (including extension) and the excel sheet (tab) name. read_excel() auto detects the format from the file extension. The functions read_xls() and read_xlsx() can be used to read files without extension.\n \n\n2.5.1 Ejercicio 1\nTodas las funciones predeterminadas para introducir datos en R tienen una contrapartida para exportar el mismo tipo de datos. Los nombres de estas otras funciones son similares a los de las de lectura de datos, aunque suelen empezar por “write” o “save”.\n1.1 ¿Cuáles son los nombres de las funciones por defecto para exportar los formatos de datos que hemos utilizado anteriormente? (pista: prueba apropos para comprobar qué funciones están disponibles)\n1.2 Exportar los datos de mamíferos como archivo .csv.\n1.3 Vuelva a exportar los datos de mamíferos, esta vez excluyendo los nombres de las filas\n1.4 Leer el archivo .csv con read.table.\n1.5 ¿Qué otros paquetes pueden importar archivos excel en R?\n1.6 ¿Se puede exportar un archivo excel o añadir datos a un archivo excel existente desde R?\n1.7 Utilizando el archivo “clements_bird_list.csv”, ¿cómo le diría a R que leyera tanto “Rheiformes” como “Ostriches” como celdas vacías (sin dejar de leer las celdas vacías como celdas vacías)?"
  },
  {
    "objectID": "importar_datos.html#datos-compactos",
    "href": "importar_datos.html#datos-compactos",
    "title": "Importar y exportar datos en R",
    "section": "3.1 Datos compactos",
    "text": "3.1 Datos compactos\nDatos compactos (“tidy data”) es una lógica para organizar conjuntos de datos de forma coherente e intuitiva. Para ejecutar parte del código de abajo necesitarás los paquetes ‘tidyr’ y ‘dplyr’, que se pueden instalar/cargar de la siguiente manera:\n\n\nCódigo\ninstall.packages(pkgs = \"tidyr\")\n\ninstall.packages(pkgs = \"dplyr\")\n\nlibrary(tidyr)\n\nlibrary(dplyr)\n\n\n \nLos mismos datos pueden representarse de muchas maneras. En el ejemplo siguiente, cada conjunto de datos muestra exactamente los mismos valores de cuatro variables país, año, población y casos, pero en cada conjunto de datos los valores están organizados de forma diferente. Los datos muestran el número de casos de tuberculosis en Afganistán, Brasil y China entre 1999 y 2000:\n\n\nCódigo\nas.data.frame(table1)\n\n\n\n\n\n\n\n\n\ncountry\nyear\ncases\npopulation\n\n\n\n\nAfghanistan\n1999\n745\n19987071\n\n\nAfghanistan\n2000\n2666\n20595360\n\n\nBrazil\n1999\n37737\n172006362\n\n\nBrazil\n2000\n80488\n174504898\n\n\nChina\n1999\n212258\n1272915272\n\n\nChina\n2000\n213766\n1280428583\n\n\n\n\n\n\n\n\n\n\n\nCódigo\nas.data.frame(table2)\n\n\n\n\n\n\n\n\n\ncountry\nyear\ntype\ncount\n\n\n\n\nAfghanistan\n1999\ncases\n745\n\n\nAfghanistan\n1999\npopulation\n19987071\n\n\nAfghanistan\n2000\ncases\n2666\n\n\nAfghanistan\n2000\npopulation\n20595360\n\n\nBrazil\n1999\ncases\n37737\n\n\nBrazil\n1999\npopulation\n172006362\n\n\nBrazil\n2000\ncases\n80488\n\n\nBrazil\n2000\npopulation\n174504898\n\n\nChina\n1999\ncases\n212258\n\n\nChina\n1999\npopulation\n1272915272\n\n\nChina\n2000\ncases\n213766\n\n\nChina\n2000\npopulation\n1280428583\n\n\n\n\n\n\n\n\n\n\n\nCódigo\nas.data.frame(table3)\n\n\n\n\n\n\n\n\n\ncountry\nyear\nrate\n\n\n\n\nAfghanistan\n1999\n745/19987071\n\n\nAfghanistan\n2000\n2666/20595360\n\n\nBrazil\n1999\n37737/172006362\n\n\nBrazil\n2000\n80488/174504898\n\n\nChina\n1999\n212258/1272915272\n\n\nChina\n2000\n213766/1280428583\n\n\n\n\n\n\n\n\n\nO incluso repartidos en 2 conjuntos de datos diferentes:\n\n\nCódigo\nas.data.frame(table4a)\n\n\n\n\n\n\n\n\n\ncountry\n1999\n2000\n\n\n\n\nAfghanistan\n745\n2666\n\n\nBrazil\n37737\n80488\n\n\nChina\n212258\n213766\n\n\n\n\n\n\n\n\n\n\n\nCódigo\nas.data.frame(table4b)\n\n\n\n\n\n\n\n\n\ncountry\n1999\n2000\n\n\n\n\nAfghanistan\n19987071\n20595360\n\n\nBrazil\n172006362\n174504898\n\n\nChina\n1272915272\n1280428583\n\n\n\n\n\n\n\n\n\n \nTodos estos conjuntos de datos contenían los mismos datos subyacentes. Sin embargo, no son igual de fáciles de utilizar.\nExisten tres reglas interrelacionadas para ordenar un conjunto de datos:\n\nCada variable debe tener su propia columna\nEach observation must have its own row\nCada valor debe tener su propia celda\n\nEsta figura muestra las reglas visualmente:\n * Modified from R for Data Science  \nEstas tres reglas están interrelacionadas porque es imposible satisfacer sólo dos de las tres. Esa interrelación conduce a un conjunto aún más sencillo de instrucciones prácticas:\n\nPonga cada conjunto de datos en un marco de datos\nPonga cada variable en una columna\n\n \nEn el ejemplo anterior, sólo la tabla1 está ordenada. Es la única representación en la que cada columna es una variable. Formatear los datos de esta manera tiene dos ventajas principales:\n\nSi tienes una estructura de datos consistente, es más fácil aprender las herramientas que trabajan con ella porque tienen una uniformidad subyacente\nColocar las variables en columnas se ajusta bien a la naturaleza vectorial de R. Como hemos visto, las funciones incorporadas en R trabajan con vectores de valores. Esto hace que la transformación de datos ordenados resulte especialmente natural.\n\n \n\n3.1.1 Ejercicio 2\n2.1 Describa cómo están organizadas las variables y observaciones en cada uno de los marcos de datos de muestreo\n2.2 Calcule la tasa de casos por 10000 personas para “tabla1”, “tabla2” y “tabla4a”/“tabla4b”."
  },
  {
    "objectID": "importar_datos.html#recopilación-de-datos",
    "href": "importar_datos.html#recopilación-de-datos",
    "title": "Importar y exportar datos en R",
    "section": "3.2 Recopilación de datos",
    "text": "3.2 Recopilación de datos\nUn problema habitual es un conjunto de datos en el que algunos de los nombres de las columnas no son nombres de variables, sino valores de una variable. Tomemos “tabla4a”: los nombres de las columnas 1999 y 2000 representan valores de la variable año, y cada fila representa dos observaciones, no una:\n\n\nCódigo\nas.data.frame(table4a)\n\n\n\n\n\n\n\n\n\ncountry\n1999\n2000\n\n\n\n\nAfghanistan\n745\n2666\n\n\nBrazil\n37737\n80488\n\n\nChina\n212258\n213766\n\n\n\n\n\n\n\n\n\n \nPara ordenar un conjunto de datos como éste, necesitamos reunir esas columnas en un nuevo par de variables. Para ello necesitamos tres parámetros:\n\nEl conjunto de columnas que representan valores, no variables. En este ejemplo, son las columnas 1999 y 2000.\nEl nombre de la variable cuyos valores forman los nombres de las columnas. En la sintaxis ‘tidyr’ se llama key, que en este caso es year.\nEl nombre de la variable cuyos valores se reparten por las celdas. En la sintaxis ‘tidyr’ que se llama que value, que en este caso es el número de cases\n\nEstos parámetros se pueden utilizar para crear un conjunto de datos ordenados utilizando la función gather():\n\n\nCódigo\ngather(table4a, key = \"year\", value = \"cases\", `1999`, `2000`)\n\n\n\n\n\n\n\n\n\ncountry\nyear\ncases\n\n\n\n\nAfghanistan\n1999\n745\n\n\nBrazil\n1999\n37737\n\n\nChina\n1999\n212258\n\n\nAfghanistan\n2000\n2666\n\n\nBrazil\n2000\n80488\n\n\nChina\n2000\n213766\n\n\n\n\n\n\n\n\n\nPodemos visualizar este formato de la siguiente manera:\n * Modified from R for Data Science  \ngather() también se puede utilizar para ordenar table4b. La única diferencia es la variable almacenada en los valores de las celdas:\n\n\nCódigo\ngather(\n  data = table4b,\n  key = \"year\",\n  value = \"population\",\n  `1999`,\n  `2000`\n)\n\n\n\n\n\n\n\n\n\ncountry\nyear\npopulation\n\n\n\n\nAfghanistan\n1999\n19987071\n\n\nBrazil\n1999\n172006362\n\n\nChina\n1999\n1272915272\n\n\nAfghanistan\n2000\n20595360\n\n\nBrazil\n2000\n174504898\n\n\nChina\n2000\n1280428583\n\n\n\n\n\n\n\n\n\n \nPara combinar las versiones ordenadas de las tablas table4a y table4b en un único marco de datos (o ‘tibble’), podemos utilizar dplyr::left_join() o merge() de la base R:\n\n\nCódigo\ntidy4a &lt;-\n  gather(table4a, key = \"year\", value = \"cases\", `1999`, `2000`)\n\ntidy4b &lt;-\n  gather(table4b, key = \"year\", value = \"population\", `1999`, `2000`)\n\nleft_join(\n  x = tidy4a,\n  y = tidy4b,\n  by = c(\"country\", \"year\")\n)\n\n\n\n\n\n\n\n\n\ncountry\nyear\ncases\npopulation\n\n\n\n\nAfghanistan\n1999\n745\n19987071\n\n\nBrazil\n1999\n37737\n172006362\n\n\nChina\n1999\n212258\n1272915272\n\n\nAfghanistan\n2000\n2666\n20595360\n\n\nBrazil\n2000\n80488\n174504898\n\n\nChina\n2000\n213766\n1280428583\n\n\n\n\n\n\n\n\n\n\n\nCódigo\nmerge(\n  x = tidy4a,\n  y = tidy4b,\n  by = c(\"country\", \"year\")\n)\n\n\n\n\n\n\n\n\n\ncountry\nyear\ncases\npopulation\n\n\n\n\nAfghanistan\n1999\n745\n19987071\n\n\nAfghanistan\n2000\n2666\n20595360\n\n\nBrazil\n1999\n37737\n172006362\n\n\nBrazil\n2000\n80488\n174504898\n\n\nChina\n1999\n212258\n1272915272\n\n\nChina\n2000\n213766\n1280428583"
  },
  {
    "objectID": "importar_datos.html#expandir-spreading",
    "href": "importar_datos.html#expandir-spreading",
    "title": "Importar y exportar datos en R",
    "section": "3.3 Expandir (spreading)",
    "text": "3.3 Expandir (spreading)\nEl expandir es lo contrario de la agrupación. Se utiliza cuando una observación está dispersa en varias filas. Por ejemplo, en la tabla2 una observación es un país en un año, pero cada observación está dispersa en dos filas:\n\n\nCódigo\ntable2\n\n\n\n\n\n\n\n\n\ncountry\nyear\ntype\ncount\n\n\n\n\nAfghanistan\n1999\ncases\n745\n\n\nAfghanistan\n1999\npopulation\n19987071\n\n\nAfghanistan\n2000\ncases\n2666\n\n\nAfghanistan\n2000\npopulation\n20595360\n\n\nBrazil\n1999\ncases\n37737\n\n\nBrazil\n1999\npopulation\n172006362\n\n\nBrazil\n2000\ncases\n80488\n\n\nBrazil\n2000\npopulation\n174504898\n\n\nChina\n1999\ncases\n212258\n\n\nChina\n1999\npopulation\n1272915272\n\n\nChina\n2000\ncases\n213766\n\n\nChina\n2000\npopulation\n1280428583\n\n\n\n\n\n\n\n\n\n \nPara ordenar este conjunto de datos, sólo necesitamos dos parámetros:\n\nLa columna que contiene los nombres de las variables, la columna key. Aquí, es type.\nLa columna que contiene los valores de las variables múltiples, la columna value. la columna value. Aquí es count.\n\n \nPara hacer esto podemos usar spread():\n\n\nCódigo\nspread(table2, key = \"type\", value = \"count\")\n\n\n\n\n\n\n\n\n\ncountry\nyear\ncases\npopulation\n\n\n\n\nAfghanistan\n1999\n745\n19987071\n\n\nAfghanistan\n2000\n2666\n20595360\n\n\nBrazil\n1999\n37737\n172006362\n\n\nBrazil\n2000\n80488\n174504898\n\n\nChina\n1999\n212258\n1272915272\n\n\nChina\n2000\n213766\n1280428583\n\n\n\n\n\n\n\n\n\n \nque puede visualizarse de la siguiente manera:\n * Modified from R for Data Science  \nspread() y gather() son funciones complementarias. gather() hace las tablas anchas más estrechas y largas; spread() hace las tablas largas más cortas y anchas.\n \n\n3.3.1 Ejercicio 3\n3.1 Ordena el siguiente conjunto de datos sobre la altura de los árboles de 2 especies:\n\n\nCódigo\nplnt_sz &lt;- data.frame(\n  forest = c(\"old_growth\", \"disturbed\"),\n  Species_1 = c(154, 160),\n  Species_2 = c(120, 113)\n)"
  },
  {
    "objectID": "importar_datos.html#separar-y-unir",
    "href": "importar_datos.html#separar-y-unir",
    "title": "Importar y exportar datos en R",
    "section": "3.4 Separar y unir",
    "text": "3.4 Separar y unir\nHasta ahora hemos arreglado “tabla2” y “tabla4”, pero no “tabla3”. La “tabla3” tiene un problema diferente: tenemos una columna (tasa) que contiene dos variables (casos y población). Esto puede solucionarse con la función separate() . También veremos su complemento unite(), que se utiliza cuando una sola variable está repartida en varias columnas.\n \n\n3.4.1 Separar\nLa función separate() separa una columna en varias columnas, dividiéndolas donde aparezca un carácter separador. Por ejemplo, table3:\n\n\nCódigo\nas.data.frame(table3)\n\n\n\n\n\n\n\n\n\ncountry\nyear\nrate\n\n\n\n\nAfghanistan\n1999\n745/19987071\n\n\nAfghanistan\n2000\n2666/20595360\n\n\nBrazil\n1999\n37737/172006362\n\n\nBrazil\n2000\n80488/174504898\n\n\nChina\n1999\n212258/1272915272\n\n\nChina\n2000\n213766/1280428583\n\n\n\n\n\n\n\n\n\nVisualmente hace algo así:\n * Modified from R for Data Science\n \nLa columna tasa contiene las variables casos y población, y necesitamos separarla en dos variables. separate() toma el nombre de la columna a separar, y los nombres de las nuevas columnas a crear:\n\n\nCódigo\nseparate(\n  data = table3,\n  col = rate,\n  into = c(\"cases\", \"population\")\n)\n\n\n\n\n\n\n\n\n\ncountry\nyear\ncases\npopulation\n\n\n\n\nAfghanistan\n1999\n745\n19987071\n\n\nAfghanistan\n2000\n2666\n20595360\n\n\nBrazil\n1999\n37737\n172006362\n\n\nBrazil\n2000\n80488\n174504898\n\n\nChina\n1999\n212258\n1272915272\n\n\nChina\n2000\n213766\n1280428583\n\n\n\n\n\n\n\n\n\nDe forma predeterminada, separate() dividirá basándose en cualquier carácter no alfanumérico (es decir, un carácter que no sea un número o una letra). En el código anterior, separate() divide los valores de rate en los caracteres de la barra diagonal. Esto puede indicarse explícitamente (para evitar errores):\n\n\nCódigo\ntb3 &lt;-\n  separate(\n    data = table3,\n    col = rate,\n    into = c(\"cases\", \"population\"),\n    sep = \"/\"\n  )\n\ntb3\n\n\n\n\n\n\n\n\n\ncountry\nyear\ncases\npopulation\n\n\n\n\nAfghanistan\n1999\n745\n19987071\n\n\nAfghanistan\n2000\n2666\n20595360\n\n\nBrazil\n1999\n37737\n172006362\n\n\nBrazil\n2000\n80488\n174504898\n\n\nChina\n1999\n212258\n1272915272\n\n\nChina\n2000\n213766\n1280428583\n\n\n\n\n\n\n\n\n\ntibble [6 × 4] (S3: tbl_df/tbl/data.frame)\n $ country   : chr [1:6] \"Afghanistan\" \"Afghanistan\" \"Brazil\" \"Brazil\" ...\n $ year      : num [1:6] 1999 2000 1999 2000 1999 ...\n $ cases     : chr [1:6] \"745\" \"2666\" \"37737\" \"80488\" ...\n $ population: chr [1:6] \"19987071\" \"20595360\" \"172006362\" \"174504898\" ...\n\n\n \nTenga en cuenta que case y population son columnas de caracteres. Por defecto separate() deja el tipo de las nuevas columnas como en la original. En este caso esto no es lo ideal ya que realmente son números. Podemos pedir a separate() que intente convertir a tipos mejores usando convert = TRUE:\n\n\nCódigo\ntb3 &lt;-\n  separate(\n    data = table3,\n    col = rate,\n    into = c(\"cases\", \"population\"),\n    convert = TRUE\n  )\n\nstr(tb3)\n\n\ntibble [6 × 4] (S3: tbl_df/tbl/data.frame)\n $ country   : chr [1:6] \"Afghanistan\" \"Afghanistan\" \"Brazil\" \"Brazil\" ...\n $ year      : num [1:6] 1999 2000 1999 2000 1999 ...\n $ cases     : int [1:6] 745 2666 37737 80488 212258 213766\n $ population: int [1:6] 19987071 20595360 172006362 174504898 1272915272 1280428583\n\n\n \nTambién puede pasar un vector de enteros a sep, que serán interpretados como posiciones en las que dividir. Los valores positivos comienzan en 1 en el extremo izquierdo de las cadenas; los valores negativos comienzan en -1 en el extremo derecho de las cadenas. Cuando se usan enteros para separar cadenas, la longitud de sep debe ser uno menos que el número de nombres en into. Puede utilizarlo para separar los dos últimos dígitos de cada año:\n\n\nCódigo\nseparate(\n  data = table3,\n  col = year,\n  into = c(\"century\", \"year\"),\n  sep = 2\n)\n\n\n\n\n\n\n\n\n\ncountry\ncentury\nyear\nrate\n\n\n\n\nAfghanistan\n19\n99\n745/19987071\n\n\nAfghanistan\n20\n00\n2666/20595360\n\n\nBrazil\n19\n99\n37737/172006362\n\n\nBrazil\n20\n00\n80488/174504898\n\n\nChina\n19\n99\n212258/1272915272\n\n\nChina\n20\n00\n213766/1280428583\n\n\n\n\n\n\n\n\n\nLa separación de columnas también se puede hacer con la base R, aunque requiere un poco más de codificación:\n\n\nCódigo\ntable3$cases &lt;-\n  sapply(table3$rate, function(x) {\n    try(strsplit(x, \"/\")[[1]][1])\n  },\n  USE.NAMES = FALSE\n  )\n\ntable3$population &lt;-\n  sapply(table3$rate, function(x) {\n    try(strsplit(x, \"/\")[[1]][2])\n  },\n  USE.NAMES = FALSE\n  )\n\ntb3\n\n\n\n\n\n\n\n\n\ncountry\nyear\nrate\ncases\npopulation\n\n\n\n\nAfghanistan\n1999\n745/19987071\n745\n19987071\n\n\nAfghanistan\n2000\n2666/20595360\n2666\n20595360\n\n\nBrazil\n1999\n37737/172006362\n37737\n172006362\n\n\nBrazil\n2000\n80488/174504898\n80488\n174504898\n\n\nChina\n1999\n212258/1272915272\n212258\n1272915272\n\n\nChina\n2000\n213766/1280428583\n213766\n1280428583\n\n\n\n\n\n\n\n\n\ntibble [6 × 4] (S3: tbl_df/tbl/data.frame)\n $ country   : chr [1:6] \"Afghanistan\" \"Afghanistan\" \"Brazil\" \"Brazil\" ...\n $ year      : num [1:6] 1999 2000 1999 2000 1999 ...\n $ cases     : chr [1:6] \"745\" \"2666\" \"37737\" \"80488\" ...\n $ population: chr [1:6] \"19987071\" \"20595360\" \"172006362\" \"174504898\" ..."
  },
  {
    "objectID": "importar_datos.html#unir",
    "href": "importar_datos.html#unir",
    "title": "Importar y exportar datos en R",
    "section": "3.5 Unir",
    "text": "3.5 Unir\nunite() es la inversa de separate(): combinar varias columnas en una sola:\n * Modified from R for Data Science\n \nSin embargo, la necesitará con mucha menos frecuencia que separate().\nPodemos utilizar unite() para volver a unir las columnas century y year que creamos anteriormente:\n\n\nCódigo\nunite(data = table5, col = \"new\", \"century\", \"year\")\n\n\n\n\n\n\n\n\n\ncountry\nnew\nrate\n\n\n\n\nAfghanistan\n19_99\n745/19987071\n\n\nAfghanistan\n20_00\n2666/20595360\n\n\nBrazil\n19_99\n37737/172006362\n\n\nBrazil\n20_00\n80488/174504898\n\n\nChina\n19_99\n212258/1272915272\n\n\nChina\n20_00\n213766/1280428583\n\n\n\n\n\n\n\n\n\nEn esta función también podemos utilizar el argumento sep (aunque en este ejemplo no se ha especificado).\n \n\n3.5.1 Ejercicio 4\n\n4.1 Una century y year de la “tabla5” usando R básico (pista: paste())"
  },
  {
    "objectID": "importar_datos.html#referencias",
    "href": "importar_datos.html#referencias",
    "title": "Importar y exportar datos en R",
    "section": "4.1 Referencias",
    "text": "4.1 Referencias\n\nClements, J. F., T. S. Schulenberg, M. J. Iliff, D. Roberson, T. A. Fredericks, B. L. Sullivan, and C. L. Wood. 2017. The eBird/Clements checklist of birds of the world: v2016.\nJones, Jon Bielby, Marcel Cardillo, Susanne A. Fritz, Justin O’Dell, C. David L. Orme, Kamran Safi, Wes Sechrest, Elizabeth H. Boakes, Chris Carbone, Christina Connolly, Michael J. Cutts, Janine K. Foster, Richard Grenyer, Michael Habib, Christopher A. Plaster, Samantha A. Price, Elizabeth A. Rigby, Janna Rist, Amber Teacher, Olaf R. P. Bininda-Emonds, John L. Gittleman, Georgina M. Mace, and Andy Purvis. 2009. PanTHERIA: a species-level database of life history, ecology, and geography of extant and recently extinct mammals. Ecology 90:2648.\nWickham, Hadley, and Garrett Grolemund. 2016. R for data science: import, tidy, transform, visualize, and model data. website\nData import tutorial- DataCamp"
  },
  {
    "objectID": "importar_datos.html#información-de-la-sesión",
    "href": "importar_datos.html#información-de-la-sesión",
    "title": "Importar y exportar datos en R",
    "section": "Información de la sesión",
    "text": "Información de la sesión\n\n\nR version 4.4.1 (2024-06-14)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 22.04.4 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0 \nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=es_CR.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=es_CR.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=es_CR.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=es_CR.UTF-8 LC_IDENTIFICATION=C       \n\ntime zone: America/Costa_Rica\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] dplyr_1.1.4        tidyr_1.3.1        readxl_1.4.3       kableExtra_1.4.0  \n[5] RColorBrewer_1.1-3 viridis_0.6.5      viridisLite_0.4.2  ggplot2_3.5.1     \n[9] knitr_1.48        \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.5        jsonlite_1.8.9      highr_0.11         \n [4] crayon_1.5.3        compiler_4.4.1      tidyselect_1.2.1   \n [7] xml2_1.3.6          stringr_1.5.1       gridExtra_2.3      \n[10] systemfonts_1.1.0   scales_1.3.0        yaml_2.3.10        \n[13] fastmap_1.2.0       R6_2.5.1            generics_0.1.3     \n[16] htmlwidgets_1.6.4   tibble_3.2.1        munsell_0.5.1      \n[19] xaringanExtra_0.8.0 svglite_2.1.3       pillar_1.9.0       \n[22] rlang_1.1.4         utf8_1.2.4          stringi_1.8.4      \n[25] xfun_0.48           cli_3.6.3           withr_3.0.1        \n[28] magrittr_2.0.3      digest_0.6.37       grid_4.4.1         \n[31] rstudioapi_0.16.0   remotes_2.5.0       packrat_0.9.2      \n[34] lifecycle_1.0.4     vctrs_0.6.5         evaluate_1.0.1     \n[37] glue_1.8.0          cellranger_1.1.0    sketchy_1.0.3      \n[40] fansi_1.0.6         colorspace_2.1-1    purrr_1.0.2        \n[43] rmarkdown_2.28      tools_4.4.1         pkgconfig_2.0.3    \n[46] htmltools_0.5.8.1"
  },
  {
    "objectID": "como_simular_datos.html",
    "href": "como_simular_datos.html",
    "title": "Simulación de datos",
    "section": "",
    "text": "Paquetes a utilizar en este manual:\nCódigo\n# instalar/cargar paquetes\n\nsketchy::load_packages(\n  c(\"ggplot2\", \n    \"viridis\"\n    )\n  )"
  },
  {
    "objectID": "como_simular_datos.html#generación-de-números-aleatorios-en-r",
    "href": "como_simular_datos.html#generación-de-números-aleatorios-en-r",
    "title": "Simulación de datos",
    "section": "1.1 Generación de números aleatorios en R",
    "text": "1.1 Generación de números aleatorios en R\nLa estadística nos permite inferir patrones en los datos. Solemos utilizar conjuntos de datos reales para enseñar estadística. Sin embargo, puede ser circular entender el funcionamiento interno de una herramienta estadística probando su capacidad para inferir un patrón que no estamos seguros de encontrar en los datos (y no tenemos idea del mecanismo que produjo ese patrón). Las simulaciones nos permiten crear escenarios controlados en los que conocemos con seguridad los patrones presentes en los datos y los procesos subyacentes que los han generado.\nR ofrece algunas funciones básicas para la simulación de datos. Las más utilizadas son las funciones generadoras de números aleatorios. Los nombres de estas funciones comienzan con r (r____()). Por ejemplo, runif():\n\n\nCódigo\n# simular variable uniforme\nunif_var &lt;- runif(n = 100, min = 0, max = 10)\n\n\n \nEl resultado es un vector numérico de longitud 100 (n = 100):\n\n\nCódigo\n# imprimir variable\nunif_var\n\n\n  [1] 9.889093 3.977455 1.156978 0.697487 2.437494 7.920104 3.400624 9.720625\n  [9] 1.658555 4.591037 1.717481 2.314771 7.728119 0.963015 4.534478 0.847007\n [17] 5.606659 0.087046 9.857371 3.165848 6.394489 2.952232 9.967037 9.060213\n [25] 9.887391 0.656457 6.270388 4.904750 9.710244 3.622208 6.799935 2.637199\n [33] 1.857143 1.851432 3.792967 8.470244 4.980761 7.905856 8.384639 4.569039\n [41] 7.994758 3.819431 7.597012 4.367756 9.042177 3.195349 0.825691 8.162891\n [49] 8.984762 9.664964 5.730689 7.200795 7.740586 6.277608 7.229893 3.868313\n [57] 1.627908 1.872283 3.912495 2.739012 1.919177 5.043918 7.638404 6.936689\n [65] 5.440542 6.590872 4.687284 4.818055 3.370636 4.245263 2.870151 6.011915\n [73] 8.407423 6.208370 1.345516 5.677224 4.434263 4.379754 6.236172 9.326533\n [81] 8.884926 8.785406 2.421769 7.414538 3.876563 0.789517 0.948356 7.621427\n [89] 3.478940 4.167667 3.440162 0.084109 9.115750 1.822054 7.228034 5.719633\n [97] 5.400364 3.549474 8.240918 1.861368\n\n\n \nPodemos explorar el resultado graficando un histograma:\n\n\nCódigo\n# crear histograma\nggplot(data = data.frame(unif_var), mapping = aes(x = unif_var)) + geom_histogram()\n\n\n\n\n\n\n\n\n\n \nMuestra una distribución uniforme que va de 0 a 10.\nTambién podemos simular números aleatorios procedentes de una distribución normal utilizando rnorm():\n\n\nCódigo\n# crear una variable normal\nnorm_var &lt;- rnorm(n = 1000, mean = 2, sd = 1)\n\n# graficar histograma\nggplot(data = data.frame(norm_var), mapping = aes(x = norm_var)) + geom_histogram() \n\n\n\n\n\n\n\n\n\n \nTenga en cuenta que todas las funciones generadoras de números aleatorios tienen el argumento ‘n’, que determina la longitud del vector generado (es decir, el número de números aleatorios), además de algunos argumentos adicionales relacionados con parámetros específicos de la distribución.\nLas variables continuas (es decir, los vectores numéricos) pueden convertirse en variables discretas (es decir, números enteros) simplemente redondeándolas:\n\n\nCódigo\nv1 &lt;- rnorm(n = 5, mean = 10, sd = 3)\n\nv1\n\n\n[1] 15.4269  8.5450 15.1387  8.9203  9.3693\n\n\nCódigo\nround(x = v1, digits = 0)\n\n\n[1] 15  9 15  9  9\n\n\n \n\nEjercicio 1\n\n¿Qué hacen las funciones rbinom() y rexp()?\nEjecútela y haga histogramas de sus resultados\n¿Qué hacen los argumentos ‘mean’ y ‘sd’ en rnorm()? Juegue con diferentes valores y comprueba el histograma para hacerse una idea de su efecto en la simulación"
  },
  {
    "objectID": "como_simular_datos.html#generación-de-variables-categóricas",
    "href": "como_simular_datos.html#generación-de-variables-categóricas",
    "title": "Simulación de datos",
    "section": "1.2 Generación de variables categóricas",
    "text": "1.2 Generación de variables categóricas\nLa forma más sencilla de generar variables categóricas es utilizar el vector de ejemplo letters' (oLETTERS’) para asignar niveles de categoría. Podemos hacerlo utilizando la función rep(). Por ejemplo, el siguiente código crea un vector categórico (caracteres) con dos niveles, cada uno con 4 observaciones:\n\n\nCódigo\nrep(letters[1:2], each = 4)\n\n\n[1] \"a\" \"a\" \"a\" \"a\" \"b\" \"b\" \"b\" \"b\"\n\n\n \nTambién podemos replicar este patrón utilizando el argumento ‘times’. Este código replica el vector anterior 2 veces:\n\n\nCódigo\nrep(letters[1:2], each = 4, times = 2)\n\n\n [1] \"a\" \"a\" \"a\" \"a\" \"b\" \"b\" \"b\" \"b\" \"a\" \"a\" \"a\" \"a\" \"b\" \"b\" \"b\" \"b\"\n\n\n \nOtra opción es simular una variable a partir de una distribución binomial y luego convertirla en un factor:\n\n\nCódigo\n# correr rbinom\nbinom_var &lt;- rbinom(n = 50, size = 1, prob = 0.5)\n\nbinom_var\n\n\n [1] 1 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0\n[39] 1 1 0 0 0 0 0 1 0 1 1 1\n\n\n\n\nCódigo\n# convertir a factor\ncateg_var &lt;- factor(binom_var, labels = c(\"a\", \"b\"))\n\ncateg_var\n\n\n [1] b a a b b a a a a a a a b b b a a b a b a a a a a a a b a b b b a a a b a a\n[39] b b a a a a a b a b b b\nLevels: a b"
  },
  {
    "objectID": "como_simular_datos.html#muestreo-aleatorio",
    "href": "como_simular_datos.html#muestreo-aleatorio",
    "title": "Simulación de datos",
    "section": "1.3 Muestreo aleatorio",
    "text": "1.3 Muestreo aleatorio\nLa otra herramienta importante de R para jugar con datos simulados es sample(). Esta función permite tomar muestras de tamaños específicos de vectores. Por ejemplo, tomemos el ejemplo del vector ‘letters’:\n\n\nCódigo\nletters\n\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\n\n \nPodemos tomar una muestra de este vector como es:\n\n\nCódigo\n# tomar muestra\nsample(x = letters, size = 10)\n\n\n [1] \"i\" \"r\" \"q\" \"a\" \"c\" \"m\" \"y\" \"z\" \"u\" \"v\"\n\n\n \nEl argumento ‘size’ nos permite determinar el tamaño de la muestra. Tenga en cuenta que obtendremos un error si el tamaño es mayor que el propio vector:\n\n\nCódigo\nsample(x = letters, size = 30)\n\n\nError in sample.int(length(x), size, replace, prob): cannot take a sample larger than the population when 'replace = FALSE'\n\n\n \nEsto sólo puede hacerse cuando el muestreo es con reemplazo (replacement). El muestreo con reemplazo puede aplicarse estableciendo el argumento replace = TRUE:\n\n\nCódigo\nsample(x = letters, size = 30, replace = TRUE)\n\n\n [1] \"j\" \"i\" \"h\" \"i\" \"k\" \"u\" \"n\" \"i\" \"w\" \"k\" \"c\" \"t\" \"m\" \"p\" \"i\" \"h\" \"d\" \"w\" \"c\"\n[20] \"q\" \"j\" \"e\" \"e\" \"j\" \"k\" \"c\" \"l\" \"t\" \"r\" \"h\""
  },
  {
    "objectID": "como_simular_datos.html#iterar-un-proceso",
    "href": "como_simular_datos.html#iterar-un-proceso",
    "title": "Simulación de datos",
    "section": "1.4 Iterar un proceso",
    "text": "1.4 Iterar un proceso\nA menudo, las simulaciones deben repetirse varias veces para descartar resultados espurios debidos al azar o simplemente para probar diferentes parámetros. Las funciones de simulación de datos mencionadas anteriormente pueden ejecutarse varias veces (por ejemplo, iteradas) utilizando la función replicate():\n\n\nCódigo\n# replicar\nrepl_rnorm &lt;- replicate(n = 3, expr = rnorm(2), simplify = FALSE)\n\n# ver clase\nclass(repl_rnorm)\n\n\n[1] \"list\"\n\n\nCódigo\n# imprimir\nrepl_rnorm\n\n\n[[1]]\n[1]  1.19184 -0.33992\n\n[[2]]\n[1]  0.78911 -0.63213\n\n[[3]]\n[1] -1.49312 -0.13441"
  },
  {
    "objectID": "como_simular_datos.html#hacer-que-las-simulaciones-sean-reproducibles",
    "href": "como_simular_datos.html#hacer-que-las-simulaciones-sean-reproducibles",
    "title": "Simulación de datos",
    "section": "1.5 Hacer que las simulaciones sean reproducibles",
    "text": "1.5 Hacer que las simulaciones sean reproducibles\nEl último truco que necesitamos para ejecutar simulaciones en R es la capacidad de reproducir una simulación (es decir, obtener exactamente los mismos datos y resultados simulados). Esto puede ser útil para que otros investigadores puedan ejecutar nuestros análisis exactamente de la misma manera. Esto puede hacerse fácilmente con la función set.seed(). Pruebe a ejecutar el siguiente código. Debería obtener la misma salida:\n\n\nCódigo\n# definir semilla\nset.seed(10)\n\n# crear variable uniforme\nrunif(n = 2)\n\n\n[1] 0.50748 0.30677"
  },
  {
    "objectID": "como_simular_datos.html#juegos-de-datos-con-variables-numéricas-y-categóricas",
    "href": "como_simular_datos.html#juegos-de-datos-con-variables-numéricas-y-categóricas",
    "title": "Simulación de datos",
    "section": "2.1 Juegos de datos con variables numéricas y categóricas",
    "text": "2.1 Juegos de datos con variables numéricas y categóricas\nAhora que sabemos cómo simular variables continuas y categóricas. Podemos juntarlas para crear conjuntos de datos simulados. Esto se puede hacer utilizando la función data.frame():\n\n\nCódigo\n# crear variable categorica\ngrupo &lt;- rep(letters[1:2], each = 3)\n\n# crear variable continuaa\ntamano &lt;- rnorm(n = 6, mean = 5, sd = 1)\n\n# poner juntas en un data frame\ndf &lt;- data.frame(grupo, tamano)\n\n# imprimir\ndf\n\n\n\n\n\n\ngrupo\ntamano\n\n\n\n\na\n4.8158\n\n\na\n3.6287\n\n\na\n4.4008\n\n\nb\n5.2946\n\n\nb\n5.3898\n\n\nb\n3.7919\n\n\n\n\n\n\nPor supuesto, podríamos añadir más variables a este cuadro de datos:\n\n\nCódigo\n# crear variable categorica\ngrupo &lt;- rep(letters[1:2], each = 3)\nindividuo &lt;- rep(LETTERS[1:6])\n\n# crear variables continuas\ntamano &lt;- rnorm(n = 6, mean = 5, sd = 1)\npeso &lt;- rnorm(n = 6, mean = 100, sd = 10)\n\n# poner todo en un data frame\ndf &lt;- data.frame(grupo, individuo, tamano, peso)\n\n# imprimir\ndf\n\n\n\n\n\n\ngrupo\nindividuo\ntamano\npeso\n\n\n\n\na\nA\n4.6363\n109.874\n\n\na\nB\n3.3733\n107.414\n\n\na\nC\n4.7435\n100.893\n\n\nb\nD\n6.1018\n90.451\n\n\nb\nE\n5.7558\n98.049\n\n\nb\nF\n4.7618\n109.255\n\n\n\n\n\n\nY eso es un juego de datos simulados en su forma más básica. Se parece mucho al tipo de datos con los que trabajamos en biología."
  },
  {
    "objectID": "como_simular_datos.html#prueba-de-concepto-el-teorema-del-límite-central",
    "href": "como_simular_datos.html#prueba-de-concepto-el-teorema-del-límite-central",
    "title": "Simulación de datos",
    "section": "3.1 Prueba de concepto: el Teorema del Límite Central",
    "text": "3.1 Prueba de concepto: el Teorema del Límite Central\nEl Teorema del Límite Central afirma que, si tomamos muestras aleatorias de una población, los promedios de esas muestras seguirán una distribución normal, aunque la población no esté distribuida normalmente. Además, la distribución normal resultante debe tener un promedio cercano al promedio de la población. El teorema es un concepto clave para la estadística inferencial, ya que implica que los métodos estadísticos que funcionan para las distribuciones normales pueden ser aplicables a muchos problemas que implican otros tipos de distribuciones. No obstante, el objetivo aquí es sólo mostrar cómo se pueden utilizar las simulaciones para entender el comportamiento de los métodos estadísticos.\nPara comprobar si esas afirmaciones básicas sobre el Teorema del Límite Central son ciertas, podemos utilizar datos simulados en R. Vamos a simular una población de 1000 observaciones con una distribución uniforme:\n\n\nCódigo\n# simular popublacion uniforme\nunif_pop &lt;- runif(1000, min = 0, max = 10)\n\n# ver histograma\nggplot(data = data.frame(unif_pop), mapping = aes(x = unif_pop)) + geom_histogram() \n\n\n\n\n\n\n\n\n\n\n\n \nPodemos tomar muestras aleatorias usando sample() así:\n\n\nCódigo\nsample(x = unif_pop, size = 30)\n\n\n [1] 9.28420 1.02626 2.57517 3.32485 6.89990 2.29404 0.33737 8.21366 3.30364\n[10] 8.03793 2.59174 7.81770 5.65426 0.63831 2.83470 4.20434 4.76330 4.42193\n[19] 6.97830 7.92625 0.68121 3.52323 6.51103 5.38289 7.97210 1.80062 4.21282\n[28] 3.33866 8.91223 4.71163\n\n\n \nEste proceso puede ser replicado varias veces con replicate():\n\n\nCódigo\n# replicar\nsamples &lt;- replicate(n = 100, expr = mean(sample(x = unif_pop, size = 30)))\n\n\n \nEl código anterior toma 100 muestras con 30 valores cada una. Ahora podemos comprobar la distribución de las muestras:\n\n\nCódigo\n# ver distribucion/ histograma\nggplot(data = data.frame(samples), mapping = aes(x = samples)) + geom_histogram() \n\n\n\n\n\n\n\n\n\n\n\n \n… asi como el promedio:\n\n\nCódigo\nmean(samples)\n\n\n[1] 5.0212\n\n\n \nComo era de esperar, las muestras siguen una distribución normal con una media cercana a la media de la población, que es:\n\n\nCódigo\nmean(unif_pop)\n\n\n[1] 5.0527\n\n\n \nProbemos con una distribución más compleja. Por ejemplo, una distribución bimodal:\n\n\nCódigo\n# usar semilla\nset.seed(123)\n\n# simular variables\nnorm1 &lt;- rnorm(n = 1000, mean = 10, sd = 3)\nnorm2 &lt;- rnorm(n = 1000, mean = 20, sd = 3)\n\n# juntar en una sola variable\nbimod_pop &lt;- c(norm1, norm2)\n\n# ver histograma\nggplot(data = data.frame(bimod_pop), mapping = aes(x = bimod_pop)) + geom_histogram() \n\n\n\n\n\n\n\n\n\n\n\n\n\nCódigo\n# replicar muestreo\nsamples &lt;- replicate(200, mean(sample(bimod_pop, 10)))\n\n# ver histograma\nggplot(data = data.frame(samples), mapping = aes(x = samples)) + geom_histogram() \n\n\n\n\n\n\n\n\n\n\n\n\n\nCódigo\n# ver promedios\nmean(samples)\n\n\n[1] 15.231\n\n\nCódigo\nmean(bimod_pop)\n\n\n[1] 15.088\n\n\n \n\nEjercicio 2\n \n\nIntenta explorar el Teorema del Límite Central como en el caso anterior, pero esta vez utilizando:\n\nUna distribución exponencial (rexp())\nUna distribución log-normal (rlnorm())\n\n\n \n\nPara cada distribución: grafique un histograma y compare los promedios de la población y de las muestras"
  },
  {
    "objectID": "como_simular_datos.html#referencias",
    "href": "como_simular_datos.html#referencias",
    "title": "Simulación de datos",
    "section": "3.2 Referencias",
    "text": "3.2 Referencias\n\nR’s rbinom – Simulate Binomial or Bernoulli trials\nR’s rnorm – selecting values from a normal distribution\nR’s exp – Simulating Exponential Distributions\nSimulating data in R"
  },
  {
    "objectID": "ggplot2.html",
    "href": "ggplot2.html",
    "title": "Gráficos con ggplot2",
    "section": "",
    "text": "Comprender la lógica detras de la estructura los gráficos con ggplot2\nSer capaz de intregrar en un gŕafico diferentes fuentes de información que representen la complejidad de los datos"
  },
  {
    "objectID": "ggplot2.html#ggplot2",
    "href": "ggplot2.html#ggplot2",
    "title": "Gráficos con ggplot2",
    "section": "ggplot2",
    "text": "ggplot2\n\nUn paquete de R diseñado específicamente para producir gráficos\nA diferencia de otros paquetes, ggplot2 tiene su propia gramática\nLa gramática se basa en la “Gramática de los gráficos” (Wilkinson 2005)\nMódulos independientes que pueden combinarse de muchas formas\nEsta gramática proporciona una gran flexibilidad"
  },
  {
    "objectID": "ggplot2.html#gramática-de-gráficos",
    "href": "ggplot2.html#gramática-de-gráficos",
    "title": "Gráficos con ggplot2",
    "section": "0.1 Gramática de gráficos",
    "text": "0.1 Gramática de gráficos\nLa idea principal es empezar con una capa base de datos brutos y luego añadir más capas de anotaciones y resúmenes estadísticos. El paquete nos permite producir gráficos utilizando la misma estructura de pensamiento que utilizamos al diseñar un análisis, reduciendo la distancia de cómo visualizamos un gráfico en la cabeza y el producto final.\nAprender la gramática no sólo es útil para producir un gráfico de interés, sino también para pensar en otros gráficos más complejos. La ventaja de esta gramática es la posibilidad de crear nuevos gráficos compuestos por nuevas combinaciones de elementos."
  },
  {
    "objectID": "ggplot2.html#componentes-del-gráfico",
    "href": "ggplot2.html#componentes-del-gráfico",
    "title": "Gráficos con ggplot2",
    "section": "0.2 Componentes del gráfico",
    "text": "0.2 Componentes del gráfico\nTodos los gráficos ggplot2 contienen los siguientes componentes:\n\nDatos - El objeto R con la información que necesita ser trazada\nCapas - Los datos específicos que serán graficados (ej. ‘x’ & ‘y’)\nEscala - Rango de datos a incluir\nCoordenadas - Sistema de coordenadas (no se utiliza muy a menudo)\nParcelas (facets) - Determina cómo dividir los datos en subparcelas en un multipanel\nTema - Controla el estilo del gráfico\n\n \nEstos componentes se juntan utilizando “+”.\nLa sintaxis más habitual incluye los datos dentro de la llamada “ggplot” y una capa “geom_”.\n \nPrimero instala/carga el paquete:\n\n\nCódigo\n# install\ninstall.packages(\"ggplot2\")\n\n# load library\nlibrary(ggplot2)"
  },
  {
    "objectID": "ggplot2.html#gráficos-de-dispersión",
    "href": "ggplot2.html#gráficos-de-dispersión",
    "title": "Gráficos con ggplot2",
    "section": "0.3 Gráficos de dispersión",
    "text": "0.3 Gráficos de dispersión\nUtilicemos el conjunto de datos “iris” para crear gráficos de dispersión:\n\n\nCódigo\nggplot(data = iris, mapping = aes(x = Sepal.Length, y = Petal.Length)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n \nThis plot is defined by 3 components: 1. “data”- iris 1. “aes” - Sepal.length vs Petal.length 1. “layer” - Points (geom)"
  },
  {
    "objectID": "ggplot2.html#atributos-estéticos",
    "href": "ggplot2.html#atributos-estéticos",
    "title": "Gráficos con ggplot2",
    "section": "0.4 Atributos estéticos",
    "text": "0.4 Atributos estéticos\nTambién podemos añadir otros atributos estéticos como el color, la forma y el tamaño. Estos atributos se pueden incluir dentro de aes():\n\n\nCódigo\n# color by species\nggplot(\n  data = iris,\n  mapping = aes(x = Sepal.Length, y = Petal.Length, color = Species)\n) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nCódigo\n# color and shape by species\nggplot(\n  data = iris,\n  mapping = aes(\n    x = Sepal.Length,\n    y = Petal.Length,\n    color = Species,\n    shape = Species\n  )\n) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n \nTenga en cuenta que los argumentos estéticos también pueden incluirse en la capa “geom”:\n\n\nCódigo\nggplot(\n  data = iris,\n  mapping = aes(x = Sepal.Length, y = Petal.Length)\n) +\n  geom_point(aes(color = Species, shape = Species))\n\n\n\n\n\n\n\n\n\n \nTambién podemos incluir un valor fijo:\n\n\nCódigo\nggplot(\n  data = iris,\n  mapping = aes(x = Sepal.Length, y = Petal.Length)\n) +\n  geom_point(color = \"red2\")\n\n\n\n\n\n\n\n\n\n \nAlgunos atributos funcionan mejor con algunos tipos de datos:\n\nColor y forma: variables categóricas\nTamaño: variables continuas\n\n \n\n\nEjercicio 1\nUtilizando el conjunto de datos “hylaeformis”:\n\n\nCódigo\n# lear desde el sitio del curso\nhylaeformis_data &lt;- read.csv(\n  paste0(\"https://raw.githubusercontent.com/maRce10/\", \"r_avanzado_2023/master/data/hylaeformis_data.csv\")\n)\n\n# or bajar manualmente y leer copia local\nhylaeformis_data &lt;- read.csv(\"hylaeformis_data.csv\", stringsAsFactors = FALSE)\n\nhead(hylaeformis_data, 20)\n\n\n\n1.1 Create a scatter plot of vs “meanfreq” (mean frequency)\n1.1 Crear un gráfico de dispersión de duración (“duration”) vs frecuencia promedio (“meanfreq”)\n\n1.2 Añadir un atributo estético para mostrar un color diferente para cada localidad\n\n1.3 Añade otro atributo estético para mostrar el rango de frecuencia dominante (“dfrange”) como tamaño los símbolos"
  },
  {
    "objectID": "ggplot2.html#gráficos-multipanel-facetting",
    "href": "ggplot2.html#gráficos-multipanel-facetting",
    "title": "Gráficos con ggplot2",
    "section": "0.5 Gráficos multipanel (Facetting)",
    "text": "0.5 Gráficos multipanel (Facetting)\n\nOtra forma de visualizar variables categóricas\nPermite crear gráficos multipanel para cada nivel de la variable\n2 tipos: “grid” & “wrap”\n\n\n\nCódigo\nggplot(iris, aes(Sepal.Length, Petal.Length)) +\n  geom_point() +\n  facet_wrap(~Species)\n\n\n\n\n\n\n\n\n\nCódigo\n# o\n\nggplot(iris, aes(Sepal.Length, Petal.Length)) +\n  geom_point() +\n  facet_grid(~Species)\n\n\n\n\n\n\n\n\n\n \nLa escala puede ser fija o libre para los ejes x e y, y puede modificarse el número de columnas y filas:\n\n\nCódigo\n# free x\nggplot(iris, aes(Sepal.Length, Petal.Length)) +\n  geom_point() +\n  facet_wrap(~Species, scales = \"free_x\")\n\n\n\n\n\n\n\n\n\n\n\nCódigo\n# free x and 3 rows\nggplot(iris, aes(Sepal.Length, Petal.Length)) +\n  geom_point() +\n  facet_wrap(~Species, scales = \"free_y\", nrow = 3)\n\n\n\n\n\n\n\n\n\n\n\nCódigo\n# both free and 2 rows\nggplot(iris, aes(Sepal.Length, Petal.Length)) +\n  geom_point() +\n  facet_wrap(~Species, scales = \"free\", nrow = 2)\n\n\n\n\n\n\n\n\n\n \nTenga en cuenta que también podemos guardar el componente básico como un objeto R y añadir otros componentes más adelante en el código:\n\n\nCódigo\np &lt;- ggplot(iris, aes(Sepal.Length, Petal.Length)) +\n  geom_point()\n\np + facet_wrap(~Species, scales = \"free_x\", nrow = 3)"
  },
  {
    "objectID": "ggplot2.html#geoms-adicionales",
    "href": "ggplot2.html#geoms-adicionales",
    "title": "Gráficos con ggplot2",
    "section": "0.6 “geoms” adicionales",
    "text": "0.6 “geoms” adicionales\n\ngeom_smooth() - añade las líneas de mejor ajuste (incluyendo CI)\ngeom_boxplot() - Distribución de frecuencias\ngeom_histogram() & geom_freqpoly() - distribuciones de frecuencia\ngeom_bar() - distribución de frecuencias de variables categóricas\ngeom_path() & geom_line() - añade líneas a los gráficos de dispersión\n\n \n\n0.6.1 geom_smooth()\nLas líneas de regresión de mejor ajuste pueden añadirse con geom_smooth():\n\n\nCódigo\n# con CI\nggplot(iris, aes(Sepal.Length, Petal.Length)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  facet_wrap(~Species, scales = \"free\", nrow = 3)\n\n\n\n\n\n\n\n\n\nCódigo\n# sin CI\nggplot(iris, aes(Sepal.Length, Petal.Length)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~Species, scales = \"free\", nrow = 3)\n\n\n\n\n\n\n\n\n\n \n\nEjercicio 2\nUtilizando el conjunto de datos de ejemplo “msleep”:\n\n2.1 Crear un gráfico de dispersión de peso corporal (“bodywt”) frente a peso cerebral (“brainwt”)\n\n2.2 Añadir orden (“order”) como estética del color\n\n2.3 Añadir un componente “faceta” para dividir los gráficos por orden utilizando escalas libres\n\n2.4 Elimine los órdenes con menos de 4 especies en el conjunto de datos y haga un gráfico similar al 2.3\n\n2.5 Añadir una línea de mejor ajuste a cada gráfico del panel\n\n\n\n\n0.6.2 Boxplots\nDe nuevo, sólo se necesita un nuevo componente “geom” para crear un boxplot:\n\n\nCódigo\nggplot(iris, aes(Species, Petal.Length)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nUna alternativa interesante son los gráficos de violines:\n\n\nCódigo\nggplot(iris, aes(Species, Petal.Length)) +\n  geom_violin()\n\n\n\n\n\n\n\n\n\n \n\n\n0.6.3 Histogramas\nLo mismo ocurre con los histrogramas y los gráficos de frecuencias:\n\n\nCódigo\nggplot(iris, aes(Petal.Length)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\n\nCódigo\nggplot(iris, aes(Petal.Length)) +\n  geom_freqpoly()\n\n\n\n\n\n\n\n\n\nCódigo\nggplot(iris, aes(Petal.Length)) +\n  geom_histogram() +\n  geom_freqpoly()\n\n\n\n\n\n\n\n\n\n \nPodemos controlar la anchura de las barras:\n\n\nCódigo\nggplot(iris, aes(Petal.Length)) +\n  geom_histogram(binwidth = 1, fill = adjustcolor(\"red2\", alpha.f = 0.3))\n\n\nWarning: Duplicated aesthetics after name standardisation: fill\n\n\n\n\n\n\n\n\n\n \nY compara la distribución de los distintos grupos dentro del mismo histograma:\n\n\nCódigo\nggplot(iris, aes(Petal.Length, fill = Species)) +\n  geom_histogram(binwidth = 0.4)\n\n\n\n\n\n\n\n\n\n \n\n\n0.6.4 Gráfico de barras\nMuestran la distribución de variables discretas (categóricas):\n\n\nCódigo\ntab &lt;- table(msleep$order)\n\ndf &lt;-\n  as.data.frame(table(msleep$order[msleep$order %in% names(tab)[tab &gt; 3]]))\n\nggplot(df, aes(Var1, Freq)) +\n  geom_bar(stat = \"identity\")"
  },
  {
    "objectID": "ggplot2.html#personalización-de-ggplots",
    "href": "ggplot2.html#personalización-de-ggplots",
    "title": "Gráficos con ggplot2",
    "section": "0.7 Personalización de ggplots",
    "text": "0.7 Personalización de ggplots\nAdemás de las funciones básicas (por ejemplo, componentes) descritas anteriormente, ggplot tiene muchas otras herramientas (tanto argumentos como funciones adicionales) para personalizar aún más los gráficos. Prácticamente todo se puede modificar. Aquí vemos algunas de las herramientas más comunes.\n \n\n0.7.1 Temas\nggplot2 viene con algunos temas por defecto que se pueden aplicar fácilmente para modificar el aspecto de nuestros gráficos:\n\n\nCódigo\np &lt;- ggplot(iris, aes(Sepal.Length, Petal.Length)) +\n  geom_point()\n\np + theme_classic()\n\n\n\n\n\n\n\n\n\nCódigo\np + theme_bw()\n\n\n\n\n\n\n\n\n\nCódigo\np + theme_minimal()\n\n\n\n\n\n\n\n\n\n \nLa mayoría de los temas difieren en el uso de cuadrículas, líneas de borde y patrones de etiquetado de ejes.\n \n\n\n0.7.2 Personalización de ejes\nLos límites de los ejes pueden modificarse como sigue:\n\n\nCódigo\nggplot(iris, aes(Sepal.Length, Petal.Length)) +\n  geom_point() +\n  xlim(c(0, 10))\n\n\n\n\n\n\n\n\n\nCódigo\nggplot(iris, aes(Sepal.Length, Petal.Length, col = Species)) +\n  geom_point() +\n  xlim(c(0, 10)) +\n  ylim(c(0, 9))\n\n\n\n\n\n\n\n\n\n \nLos ejes también pueden transformarse:\n\n\nCódigo\nggplot(iris, aes(Sepal.Length, Petal.Length, col = Species)) +\n  geom_point() +\n  scale_x_continuous(trans = \"log\") +\n  scale_y_continuous(trans = \"log2\")\n\n\n\n\n\n\n\n\n\n \no invertidos:\n\n\nCódigo\nggplot(iris, aes(Sepal.Length, Petal.Length, col = Species)) +\n  geom_point() +\n  scale_y_reverse()\n\n\n\n\n\n\n\n\n\n\n\n0.7.3 Guardar ggplots\nLos ggplots pueden exportarse como archivos de imagen utilizando la función ggsave:\n\n\nCódigo\nggplot(\n  data = msleep[msleep$order %in% names(tab)[tab &gt; 5], ],\n  mapping = aes(x = bodywt, y = brainwt)\n) +\n  geom_point() +\n  facet_wrap(~order, scales = \"free\")\n\n\nWarning: Removed 21 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nCódigo\n# Export\nggsave(\"plot.png\", width = 5, height = 5)\n\n\nWarning: Removed 21 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n \nEl tipo de archivo de imagen se identificará por la extensión en el nombre del archivo\n \nPersonalización adicional del eje:\n\n\nCódigo\n# Log2 scaling of the y axis (with visually-equal spacing)\nrequire(scales)\n\np + scale_y_continuous(trans = log2_trans())\n\n\n\n\n\n\n\n\n\nCódigo\n# show exponents\np + scale_y_continuous(\n  trans = log2_trans(),\n  breaks = trans_breaks(\"log2\", function(x) 2^x),\n  labels = trans_format(\"log2\", math_format(2^.x))\n)\n\n\n\n\n\n\n\n\n\nCódigo\n# Percent\np + scale_y_continuous(labels = percent)\n\n\n\n\n\n\n\n\n\nCódigo\n# dollar\np + scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\nCódigo\n# scientific\np + scale_y_continuous(labels = scientific)\n\n\n\n\n\n\n\n\n\nCódigo\n### Agregar \"tick marks\" ###\n\n# Cargar librerías\nlibrary(MASS)\n\ndata(Animals)\n\n# x and y axis are transformed and formatted\np2 &lt;- ggplot(Animals, aes(x = body, y = brain)) +\n  geom_point(size = 4) +\n  scale_x_log10(\n    breaks = trans_breaks(\"log10\", function(x) 10^x),\n    labels = trans_format(\"log10\", math_format(10^.x))\n  ) +\n  scale_y_log10(\n    breaks = trans_breaks(\"log10\", function(x) 10^x),\n    labels = trans_format(\"log10\", math_format(10^.x))\n  ) +\n  theme_bw()\n\n# log-log plot without log tick marks\np2\n\n\n\n\n\n\n\n\n\nCódigo\n# Show log tick marks\np2 + annotation_logticks()\n\n\n\n\n\n\n\n\n\nCódigo\n# # Log ticks on left and right\np2 + annotation_logticks(sides = \"lr\")\n\n\n\n\n\n\n\n\n\nCódigo\n# All sides\np2 + annotation_logticks(sides = \"trbl\")\n\n\n\n\n\n\n\n\n\n \n\n\n0.7.4 Otros gráficos\nSe pueden generar muchos otros tipos de gráficos. Aquí muestro un ejemplo de gráficos de contorno y de “mapa de calor”:\n\n\nCódigo\nhead(faithful)\n\n\n\n\n\n\neruptions\nwaiting\n\n\n\n\n3.600\n79\n\n\n1.800\n54\n\n\n3.333\n74\n\n\n2.283\n62\n\n\n4.533\n85\n\n\n2.883\n55\n\n\n\n\n\n\nCódigo\nggplot(faithfuld, aes(eruptions, waiting)) +\n  geom_contour(aes(z = density, colour = after_stat(level)))\n\n\n\n\n\n\n\n\n\nCódigo\nggplot(faithfuld, aes(eruptions, waiting)) +\n  geom_raster(aes(fill = density))"
  },
  {
    "objectID": "ggplot2.html#otros-paquetes-de-gráficos-en-r",
    "href": "ggplot2.html#otros-paquetes-de-gráficos-en-r",
    "title": "Gráficos con ggplot2",
    "section": "0.8 Otros paquetes de gráficos en R",
    "text": "0.8 Otros paquetes de gráficos en R\n\nggvis (ggplots interactivos)\nvcd (Warnes 2015)\nplotrix (Lemon et al. 2006)\ngplots (Warnes 2015)\n\nConsulte la CRAN Graphics Task View para obtener una lista más completa de herramientas gráficas en R."
  },
  {
    "objectID": "ggplot2.html#references",
    "href": "ggplot2.html#references",
    "title": "Gráficos con ggplot2",
    "section": "0.9 References",
    "text": "0.9 References\n\nLemon J (2006) Plotrix: a package in the red light district of R. R-News 6(4):8–12\nWarnes GR, Bolker B, Bonebakker L, Gentleman R, Liaw WHA, Lumley T, Maechler M, Magnusson A, Moeller S, Schwartz M, Venables B (2015) gplots: various R programming tools for plotting data. R package version 2.17.0. https://CRAN.R-project.org/package=gplots\nWickham H (2010) A layered grammar of graphics. J Comput Graph Stat 19(1):3–28\nWilkinson L (2005) The grammar of graphics. Statistics and computing, 2nd edn. Springer, New York\n\n\n\nInformación de la sesión\n\n\nR version 4.4.1 (2024-06-14)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 22.04.4 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0 \nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=es_CR.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=es_CR.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=es_CR.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=es_CR.UTF-8 LC_IDENTIFICATION=C       \n\ntime zone: America/Costa_Rica\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] scales_1.3.0       viridis_0.6.5      viridisLite_0.4.2  MASS_7.3-61       \n[5] RColorBrewer_1.1-3 ggplot2_3.5.1      knitr_1.48         kableExtra_1.4.0  \n\nloaded via a namespace (and not attached):\n [1] Matrix_1.7-0      gtable_0.3.5      jsonlite_1.8.9    dplyr_1.1.4      \n [5] compiler_4.4.1    tidyselect_1.2.1  xml2_1.3.6        stringr_1.5.1    \n [9] gridExtra_2.3     textshaping_0.4.0 splines_4.4.1     systemfonts_1.1.0\n[13] yaml_2.3.10       fastmap_1.2.0     lattice_0.22-6    R6_2.5.1         \n[17] labeling_0.4.3    generics_0.1.3    isoband_0.2.7     htmlwidgets_1.6.4\n[21] tibble_3.2.1      munsell_0.5.1     svglite_2.1.3     pillar_1.9.0     \n[25] rlang_1.1.4       utf8_1.2.4        stringi_1.8.4     xfun_0.48        \n[29] cli_3.6.3         mgcv_1.9-1        withr_3.0.1       magrittr_2.0.3   \n[33] digest_0.6.37     grid_4.4.1        rstudioapi_0.16.0 nlme_3.1-165     \n[37] lifecycle_1.0.4   vctrs_0.6.5       evaluate_1.0.1    glue_1.8.0       \n[41] farver_2.1.2      ragg_1.3.2        fansi_1.0.6       colorspace_2.1-1 \n[45] rmarkdown_2.28    tools_4.4.1       pkgconfig_2.0.3   htmltools_0.5.8.1"
  },
  {
    "objectID": "modelos_de_arboles.html#visualización-del-error",
    "href": "modelos_de_arboles.html#visualización-del-error",
    "title": "Modelos basados en árboles",
    "section": "3.11 Visualización del error",
    "text": "3.11 Visualización del error\nA continuación, mostramos cómo podemos analizar el comportamiento del error durante el entrenamiento:\n\n\nCódigo\n# Error de validación cruzada\nprint(modelo_xgb_caret)\n\n\neXtreme Gradient Boosting \n\n297 samples\n 13 predictor\n  2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 267, 268, 267, 267, 268, 268, ... \nResampling results across tuning parameters:\n\n  eta  max_depth  colsample_bytree  subsample  nrounds  Accuracy  Kappa  \n  0.3   1         0.6               0.50000     50      0.82437   0.64592\n  0.3   1         0.6               0.50000    100      0.82759   0.65244\n  0.3   1         0.6               0.50000    150      0.80080   0.59777\n  0.3   1         0.6               0.50000    200      0.80092   0.59755\n  0.3   1         0.6               0.50000    250      0.81103   0.61897\n  0.3   1         0.6               0.50000    300      0.80069   0.59905\n  0.3   1         0.6               0.50000    350      0.80080   0.59761\n  0.3   1         0.6               0.50000    400      0.80414   0.60428\n  0.3   1         0.6               0.50000    450      0.80092   0.59882\n  0.3   1         0.6               0.50000    500      0.80069   0.59831\n  0.3   1         0.6               0.55556     50      0.82103   0.64059\n  0.3   1         0.6               0.55556    100      0.82414   0.64639\n  0.3   1         0.6               0.55556    150      0.81080   0.62022\n  0.3   1         0.6               0.55556    200      0.79724   0.59093\n  0.3   1         0.6               0.55556    250      0.79713   0.59134\n  0.3   1         0.6               0.55556    300      0.80057   0.59817\n  0.3   1         0.6               0.55556    350      0.80759   0.61212\n  0.3   1         0.6               0.55556    400      0.81092   0.61914\n  0.3   1         0.6               0.55556    450      0.80414   0.60496\n  0.3   1         0.6               0.55556    500      0.81092   0.61813\n  0.3   1         0.6               0.61111     50      0.83448   0.66770\n  0.3   1         0.6               0.61111    100      0.82770   0.65350\n  0.3   1         0.6               0.61111    150      0.81414   0.62653\n  0.3   1         0.6               0.61111    200      0.82425   0.64538\n  0.3   1         0.6               0.61111    250      0.80724   0.61136\n  0.3   1         0.6               0.61111    300      0.80736   0.61258\n  0.3   1         0.6               0.61111    350      0.81069   0.61794\n  0.3   1         0.6               0.61111    400      0.80402   0.60436\n  0.3   1         0.6               0.61111    450      0.81736   0.63139\n  0.3   1         0.6               0.61111    500      0.79724   0.59180\n  0.3   1         0.6               0.66667     50      0.82437   0.64737\n  0.3   1         0.6               0.66667    100      0.82092   0.63896\n  0.3   1         0.6               0.66667    150      0.80402   0.60511\n  0.3   1         0.6               0.66667    200      0.80402   0.60419\n  0.3   1         0.6               0.66667    250      0.80402   0.60454\n  0.3   1         0.6               0.66667    300      0.80402   0.60502\n  0.3   1         0.6               0.66667    350      0.79724   0.59150\n  0.3   1         0.6               0.66667    400      0.80736   0.61209\n  0.3   1         0.6               0.66667    450      0.79736   0.59198\n  0.3   1         0.6               0.66667    500      0.79391   0.58503\n  0.3   1         0.6               0.72222     50      0.83425   0.66658\n  0.3   1         0.6               0.72222    100      0.84448   0.68745\n  0.3   1         0.6               0.72222    150      0.83425   0.66539\n  0.3   1         0.6               0.72222    200      0.81092   0.62012\n  0.3   1         0.6               0.72222    250      0.79379   0.58456\n  0.3   1         0.6               0.72222    300      0.80057   0.59769\n  0.3   1         0.6               0.72222    350      0.81069   0.61739\n  0.3   1         0.6               0.72222    400      0.81414   0.62514\n  0.3   1         0.6               0.72222    450      0.80736   0.61168\n  0.3   1         0.6               0.72222    500      0.80402   0.60519\n  0.3   1         0.6               0.77778     50      0.83414   0.66688\n  0.3   1         0.6               0.77778    100      0.82759   0.65149\n  0.3   1         0.6               0.77778    150      0.83425   0.66491\n  0.3   1         0.6               0.77778    200      0.82747   0.65210\n  0.3   1         0.6               0.77778    250      0.81747   0.63137\n  0.3   1         0.6               0.77778    300      0.81080   0.61731\n  0.3   1         0.6               0.77778    350      0.81069   0.61818\n  0.3   1         0.6               0.77778    400      0.81736   0.63193\n  0.3   1         0.6               0.77778    450      0.82080   0.63901\n  0.3   1         0.6               0.77778    500      0.81069   0.61998\n  0.3   1         0.6               0.83333     50      0.83080   0.66027\n  0.3   1         0.6               0.83333    100      0.84103   0.67945\n  0.3   1         0.6               0.83333    150      0.82425   0.64521\n  0.3   1         0.6               0.83333    200      0.81391   0.62517\n  0.3   1         0.6               0.83333    250      0.82747   0.65155\n  0.3   1         0.6               0.83333    300      0.82092   0.63869\n  0.3   1         0.6               0.83333    350      0.82414   0.64526\n  0.3   1         0.6               0.83333    400      0.81069   0.61867\n  0.3   1         0.6               0.83333    450      0.81402   0.62558\n  0.3   1         0.6               0.83333    500      0.81069   0.61848\n  0.3   1         0.6               0.88889     50      0.83793   0.67375\n  0.3   1         0.6               0.88889    100      0.82092   0.63920\n  0.3   1         0.6               0.88889    150      0.82759   0.65327\n  0.3   1         0.6               0.88889    200      0.81736   0.63221\n  0.3   1         0.6               0.88889    250      0.81736   0.63151\n  0.3   1         0.6               0.88889    300      0.80736   0.61109\n  0.3   1         0.6               0.88889    350      0.81736   0.63109\n  0.3   1         0.6               0.88889    400      0.81414   0.62434\n  0.3   1         0.6               0.88889    450      0.81414   0.62465\n  0.3   1         0.6               0.88889    500      0.81425   0.62526\n  0.3   1         0.6               0.94444     50      0.83092   0.65986\n  0.3   1         0.6               0.94444    100      0.82759   0.65242\n  0.3   1         0.6               0.94444    150      0.83103   0.65937\n  0.3   1         0.6               0.94444    200      0.82092   0.63864\n  0.3   1         0.6               0.94444    250      0.82747   0.65223\n  0.3   1         0.6               0.94444    300      0.82080   0.63871\n  0.3   1         0.6               0.94444    350      0.81080   0.61804\n  0.3   1         0.6               0.94444    400      0.80747   0.61113\n  0.3   1         0.6               0.94444    450      0.81736   0.63139\n  0.3   1         0.6               0.94444    500      0.81402   0.62417\n  0.3   1         0.6               1.00000     50      0.83437   0.66697\n  0.3   1         0.6               1.00000    100      0.83437   0.66636\n  0.3   1         0.6               1.00000    150      0.83437   0.66630\n  0.3   1         0.6               1.00000    200      0.83103   0.66013\n  0.3   1         0.6               1.00000    250      0.82759   0.65296\n  0.3   1         0.6               1.00000    300      0.81759   0.63204\n  0.3   1         0.6               1.00000    350      0.81080   0.61773\n  0.3   1         0.6               1.00000    400      0.80747   0.61113\n  0.3   1         0.6               1.00000    450      0.80747   0.61088\n  0.3   1         0.6               1.00000    500      0.80747   0.61088\n  0.3   1         0.8               0.50000     50      0.83425   0.66650\n  0.3   1         0.8               0.50000    100      0.79736   0.59257\n  0.3   1         0.8               0.50000    150      0.80069   0.59805\n  0.3   1         0.8               0.50000    200      0.79414   0.58484\n  0.3   1         0.8               0.50000    250      0.80080   0.59888\n  0.3   1         0.8               0.50000    300      0.80414   0.60640\n  0.3   1         0.8               0.50000    350      0.80069   0.59926\n  0.3   1         0.8               0.50000    400      0.79414   0.58552\n  0.3   1         0.8               0.50000    450      0.80069   0.59945\n  0.3   1         0.8               0.50000    500      0.79402   0.58618\n  0.3   1         0.8               0.55556     50      0.81425   0.62609\n  0.3   1         0.8               0.55556    100      0.82080   0.63862\n  0.3   1         0.8               0.55556    150      0.80747   0.61258\n  0.3   1         0.8               0.55556    200      0.79736   0.59271\n  0.3   1         0.8               0.55556    250      0.80402   0.60599\n  0.3   1         0.8               0.55556    300      0.80391   0.60569\n  0.3   1         0.8               0.55556    350      0.79402   0.58493\n  0.3   1         0.8               0.55556    400      0.80414   0.60645\n  0.3   1         0.8               0.55556    450      0.81092   0.62019\n  0.3   1         0.8               0.55556    500      0.80080   0.60088\n  0.3   1         0.8               0.61111     50      0.83747   0.67230\n  0.3   1         0.8               0.61111    100      0.83092   0.65954\n  0.3   1         0.8               0.61111    150      0.82770   0.65282\n  0.3   1         0.8               0.61111    200      0.83115   0.65928\n  0.3   1         0.8               0.61111    250      0.82115   0.63834\n  0.3   1         0.8               0.61111    300      0.81759   0.63142\n  0.3   1         0.8               0.61111    350      0.81414   0.62532\n  0.3   1         0.8               0.61111    400      0.80069   0.59846\n  0.3   1         0.8               0.61111    450      0.79736   0.59209\n  0.3   1         0.8               0.61111    500      0.80414   0.60562\n  0.3   1         0.8               0.66667     50      0.83126   0.66013\n  0.3   1         0.8               0.66667    100      0.82069   0.63895\n  0.3   1         0.8               0.66667    150      0.82080   0.64028\n  0.3   1         0.8               0.66667    200      0.81759   0.63136\n  0.3   1         0.8               0.66667    250      0.81080   0.61893\n  0.3   1         0.8               0.66667    300      0.80080   0.59756\n  0.3   1         0.8               0.66667    350      0.80747   0.61150\n  0.3   1         0.8               0.66667    400      0.80759   0.61124\n  0.3   1         0.8               0.66667    450      0.79414   0.58460\n  0.3   1         0.8               0.66667    500      0.79092   0.57810\n  0.3   1         0.8               0.72222     50      0.84805   0.69379\n  0.3   1         0.8               0.72222    100      0.80747   0.61339\n  0.3   1         0.8               0.72222    150      0.80736   0.61168\n  0.3   1         0.8               0.72222    200      0.80391   0.60525\n  0.3   1         0.8               0.72222    250      0.82747   0.65198\n  0.3   1         0.8               0.72222    300      0.82402   0.64512\n  0.3   1         0.8               0.72222    350      0.81080   0.61883\n  0.3   1         0.8               0.72222    400      0.81425   0.62493\n  0.3   1         0.8               0.72222    450      0.82092   0.63863\n  0.3   1         0.8               0.72222    500      0.80402   0.60572\n  0.3   1         0.8               0.77778     50      0.82425   0.64623\n  0.3   1         0.8               0.77778    100      0.82437   0.64555\n  0.3   1         0.8               0.77778    150      0.81402   0.62455\n  0.3   1         0.8               0.77778    200      0.80736   0.61066\n  0.3   1         0.8               0.77778    250      0.80747   0.61192\n  0.3   1         0.8               0.77778    300      0.81402   0.62496\n  0.3   1         0.8               0.77778    350      0.80402   0.60502\n  0.3   1         0.8               0.77778    400      0.81069   0.61836\n  0.3   1         0.8               0.77778    450      0.81747   0.63222\n  0.3   1         0.8               0.77778    500      0.82080   0.63907\n  0.3   1         0.8               0.83333     50      0.83759   0.67338\n  0.3   1         0.8               0.83333    100      0.84138   0.67987\n  0.3   1         0.8               0.83333    150      0.81402   0.62394\n  0.3   1         0.8               0.83333    200      0.82747   0.65168\n  0.3   1         0.8               0.83333    250      0.82069   0.63788\n  0.3   1         0.8               0.83333    300      0.81747   0.63070\n  0.3   1         0.8               0.83333    350      0.82092   0.63827\n  0.3   1         0.8               0.83333    400      0.81402   0.62442\n  0.3   1         0.8               0.83333    450      0.81747   0.63095\n  0.3   1         0.8               0.83333    500      0.81402   0.62460\n  0.3   1         0.8               0.88889     50      0.83793   0.67303\n  0.3   1         0.8               0.88889    100      0.82759   0.65211\n  0.3   1         0.8               0.88889    150      0.82069   0.63758\n  0.3   1         0.8               0.88889    200      0.82402   0.64448\n  0.3   1         0.8               0.88889    250      0.82736   0.65133\n  0.3   1         0.8               0.88889    300      0.82069   0.63781\n  0.3   1         0.8               0.88889    350      0.83080   0.65821\n  0.3   1         0.8               0.88889    400      0.83080   0.65821\n  0.3   1         0.8               0.88889    450      0.83080   0.65821\n  0.3   1         0.8               0.88889    500      0.82747   0.65167\n  0.3   1         0.8               0.94444     50      0.83092   0.65986\n  0.3   1         0.8               0.94444    100      0.83115   0.65930\n  0.3   1         0.8               0.94444    150      0.83092   0.65877\n  0.3   1         0.8               0.94444    200      0.82080   0.63786\n  0.3   1         0.8               0.94444    250      0.81069   0.61702\n  0.3   1         0.8               0.94444    300      0.81402   0.62454\n  0.3   1         0.8               0.94444    350      0.81747   0.63198\n  0.3   1         0.8               0.94444    400      0.81069   0.61733\n  0.3   1         0.8               0.94444    450      0.81069   0.61751\n  0.3   1         0.8               0.94444    500      0.81402   0.62442\n  0.3   1         0.8               1.00000     50      0.84448   0.68771\n  0.3   1         0.8               1.00000    100      0.83092   0.65913\n  0.3   1         0.8               1.00000    150      0.82770   0.65315\n  0.3   1         0.8               1.00000    200      0.82770   0.65358\n  0.3   1         0.8               1.00000    250      0.82759   0.65296\n  0.3   1         0.8               1.00000    300      0.81747   0.63186\n  0.3   1         0.8               1.00000    350      0.81080   0.61785\n  0.3   1         0.8               1.00000    400      0.81080   0.61785\n  0.3   1         0.8               1.00000    450      0.81080   0.61779\n  0.3   1         0.8               1.00000    500      0.80747   0.61088\n  0.3   2         0.6               0.50000     50      0.79391   0.58438\n  0.3   2         0.6               0.50000    100      0.79034   0.57918\n  0.3   2         0.6               0.50000    150      0.79046   0.57849\n  0.3   2         0.6               0.50000    200      0.78034   0.55918\n  0.3   2         0.6               0.50000    250      0.78034   0.55869\n  0.3   2         0.6               0.50000    300      0.77034   0.53876\n  0.3   2         0.6               0.50000    350      0.77034   0.53888\n  0.3   2         0.6               0.50000    400      0.77379   0.54596\n  0.3   2         0.6               0.50000    450      0.77046   0.53953\n  0.3   2         0.6               0.50000    500      0.77713   0.55329\n  0.3   2         0.6               0.55556     50      0.82425   0.64695\n  0.3   2         0.6               0.55556    100      0.79379   0.58681\n  0.3   2         0.6               0.55556    150      0.79391   0.58605\n  0.3   2         0.6               0.55556    200      0.78356   0.56575\n  0.3   2         0.6               0.55556    250      0.78046   0.55950\n  0.3   2         0.6               0.55556    300      0.78713   0.57313\n  0.3   2         0.6               0.55556    350      0.77379   0.54677\n  0.3   2         0.6               0.55556    400      0.77713   0.55368\n  0.3   2         0.6               0.55556    450      0.77379   0.54683\n  0.3   2         0.6               0.55556    500      0.76713   0.53314\n  0.3   2         0.6               0.61111     50      0.79747   0.59186\n  0.3   2         0.6               0.61111    100      0.79402   0.58672\n  0.3   2         0.6               0.61111    150      0.79046   0.57862\n  0.3   2         0.6               0.61111    200      0.78379   0.56529\n  0.3   2         0.6               0.61111    250      0.79057   0.57952\n  0.3   2         0.6               0.61111    300      0.78724   0.57261\n  0.3   2         0.6               0.61111    350      0.78046   0.55934\n  0.3   2         0.6               0.61111    400      0.78391   0.56569\n  0.3   2         0.6               0.61111    450      0.78057   0.55909\n  0.3   2         0.6               0.61111    500      0.78724   0.57284\n  0.3   2         0.6               0.66667     50      0.81425   0.62511\n  0.3   2         0.6               0.66667    100      0.78379   0.56279\n  0.3   2         0.6               0.66667    150      0.77046   0.53842\n  0.3   2         0.6               0.66667    200      0.76701   0.53140\n  0.3   2         0.6               0.66667    250      0.76034   0.51704\n  0.3   2         0.6               0.66667    300      0.77368   0.54413\n  0.3   2         0.6               0.66667    350      0.76701   0.53080\n  0.3   2         0.6               0.66667    400      0.76034   0.51716\n  0.3   2         0.6               0.66667    450      0.75701   0.51055\n  0.3   2         0.6               0.66667    500      0.77391   0.54474\n  0.3   2         0.6               0.72222     50      0.80437   0.60637\n  0.3   2         0.6               0.72222    100      0.77379   0.54481\n  0.3   2         0.6               0.72222    150      0.75690   0.51185\n  0.3   2         0.6               0.72222    200      0.76713   0.53243\n  0.3   2         0.6               0.72222    250      0.76046   0.51946\n  0.3   2         0.6               0.72222    300      0.78379   0.56654\n  0.3   2         0.6               0.72222    350      0.77379   0.54629\n  0.3   2         0.6               0.72222    400      0.77034   0.54019\n  0.3   2         0.6               0.72222    450      0.77034   0.54031\n  0.3   2         0.6               0.72222    500      0.76368   0.52704\n  0.3   2         0.6               0.77778     50      0.81759   0.63221\n  0.3   2         0.6               0.77778    100      0.81057   0.61781\n  0.3   2         0.6               0.77778    150      0.79724   0.59151\n  0.3   2         0.6               0.77778    200      0.78713   0.57177\n  0.3   2         0.6               0.77778    250      0.78057   0.55862\n  0.3   2         0.6               0.77778    300      0.78057   0.55848\n  0.3   2         0.6               0.77778    350      0.78713   0.57255\n  0.3   2         0.6               0.77778    400      0.79057   0.57848\n  0.3   2         0.6               0.77778    450      0.78724   0.57253\n  0.3   2         0.6               0.77778    500      0.78391   0.56593\n  0.3   2         0.6               0.83333     50      0.82092   0.64006\n  0.3   2         0.6               0.83333    100      0.80736   0.61281\n  0.3   2         0.6               0.83333    150      0.79046   0.57891\n  0.3   2         0.6               0.83333    200      0.80069   0.59940\n  0.3   2         0.6               0.83333    250      0.78402   0.56644\n  0.3   2         0.6               0.83333    300      0.78069   0.55923\n  0.3   2         0.6               0.83333    350      0.78069   0.55898\n  0.3   2         0.6               0.83333    400      0.77724   0.55164\n  0.3   2         0.6               0.83333    450      0.76713   0.53195\n  0.3   2         0.6               0.83333    500      0.78057   0.55945\n  0.3   2         0.6               0.88889     50      0.81092   0.61909\n  0.3   2         0.6               0.88889    100      0.80414   0.60541\n  0.3   2         0.6               0.88889    150      0.79391   0.58559\n  0.3   2         0.6               0.88889    200      0.78391   0.56498\n  0.3   2         0.6               0.88889    250      0.78713   0.57224\n  0.3   2         0.6               0.88889    300      0.78391   0.56624\n  0.3   2         0.6               0.88889    350      0.78057   0.55921\n  0.3   2         0.6               0.88889    400      0.78057   0.55921\n  0.3   2         0.6               0.88889    450      0.78057   0.55981\n  0.3   2         0.6               0.88889    500      0.77391   0.54660\n  0.3   2         0.6               0.94444     50      0.81425   0.62557\n  0.3   2         0.6               0.94444    100      0.81425   0.62550\n  0.3   2         0.6               0.94444    150      0.78046   0.55789\n  0.3   2         0.6               0.94444    200      0.78057   0.55825\n  0.3   2         0.6               0.94444    250      0.78057   0.55825\n  0.3   2         0.6               0.94444    300      0.77391   0.54480\n  0.3   2         0.6               0.94444    350      0.77736   0.55255\n  0.3   2         0.6               0.94444    400      0.77046   0.53887\n  0.3   2         0.6               0.94444    450      0.77391   0.54620\n  0.3   2         0.6               0.94444    500      0.77057   0.53977\n  0.3   2         0.6               1.00000     50      0.80736   0.61339\n  0.3   2         0.6               1.00000    100      0.79402   0.58577\n  0.3   2         0.6               1.00000    150      0.79391   0.58570\n  0.3   2         0.6               1.00000    200      0.77701   0.55262\n  0.3   2         0.6               1.00000    250      0.78046   0.56031\n  0.3   2         0.6               1.00000    300      0.78069   0.55916\n  0.3   2         0.6               1.00000    350      0.77391   0.54638\n  0.3   2         0.6               1.00000    400      0.77057   0.54063\n  0.3   2         0.6               1.00000    450      0.76713   0.53341\n  0.3   2         0.6               1.00000    500      0.76713   0.53341\n  0.3   2         0.8               0.50000     50      0.81759   0.63338\n  0.3   2         0.8               0.50000    100      0.79414   0.58587\n  0.3   2         0.8               0.50000    150      0.80080   0.59878\n  0.3   2         0.8               0.50000    200      0.80080   0.59886\n  0.3   2         0.8               0.50000    250      0.79391   0.58521\n  0.3   2         0.8               0.50000    300      0.78402   0.56617\n  0.3   2         0.8               0.50000    350      0.79080   0.57951\n  0.3   2         0.8               0.50000    400      0.79425   0.58672\n  0.3   2         0.8               0.50000    450      0.77724   0.55337\n  0.3   2         0.8               0.50000    500      0.78069   0.55981\n  0.3   2         0.8               0.55556     50      0.79402   0.58479\n  0.3   2         0.8               0.55556    100      0.78747   0.56970\n  0.3   2         0.8               0.55556    150      0.78046   0.55717\n  0.3   2         0.8               0.55556    200      0.76368   0.52343\n  0.3   2         0.8               0.55556    250      0.76356   0.52379\n  0.3   2         0.8               0.55556    300      0.77023   0.53730\n  0.3   2         0.8               0.55556    350      0.76356   0.52410\n  0.3   2         0.8               0.55556    400      0.77034   0.53924\n  0.3   2         0.8               0.55556    450      0.77046   0.53947\n  0.3   2         0.8               0.55556    500      0.76713   0.53237\n  0.3   2         0.8               0.61111     50      0.81414   0.62567\n  0.3   2         0.8               0.61111    100      0.79747   0.59192\n  0.3   2         0.8               0.61111    150      0.79057   0.57789\n  0.3   2         0.8               0.61111    200      0.78379   0.56625\n  0.3   2         0.8               0.61111    250      0.77701   0.55393\n  0.3   2         0.8               0.61111    300      0.78034   0.56024\n  0.3   2         0.8               0.61111    350      0.76356   0.52686\n  0.3   2         0.8               0.61111    400      0.77690   0.55304\n  0.3   2         0.8               0.61111    450      0.76690   0.53370\n  0.3   2         0.8               0.61111    500      0.77034   0.54077\n  0.3   2         0.8               0.66667     50      0.80414   0.60454\n  0.3   2         0.8               0.66667    100      0.78391   0.56539\n  0.3   2         0.8               0.66667    150      0.77368   0.54512\n  0.3   2         0.8               0.66667    200      0.78402   0.56558\n  0.3   2         0.8               0.66667    250      0.77724   0.55346\n  0.3   2         0.8               0.66667    300      0.77046   0.54019\n  0.3   2         0.8               0.66667    350      0.77379   0.54680\n  0.3   2         0.8               0.66667    400      0.76713   0.53335\n  0.3   2         0.8               0.66667    450      0.77046   0.54001\n  0.3   2         0.8               0.66667    500      0.77046   0.54025\n  0.3   2         0.8               0.72222     50      0.81092   0.61955\n  0.3   2         0.8               0.72222    100      0.79092   0.57880\n  0.3   2         0.8               0.72222    150      0.79402   0.58582\n  0.3   2         0.8               0.72222    200      0.78724   0.57320\n  0.3   2         0.8               0.72222    250      0.78046   0.55989\n  0.3   2         0.8               0.72222    300      0.78713   0.57338\n  0.3   2         0.8               0.72222    350      0.78379   0.56653\n  0.3   2         0.8               0.72222    400      0.77713   0.55362\n  0.3   2         0.8               0.72222    450      0.77046   0.53975\n  0.3   2         0.8               0.72222    500      0.77046   0.53992\n  0.3   2         0.8               0.77778     50      0.80759   0.61101\n  0.3   2         0.8               0.77778    100      0.79713   0.59263\n  0.3   2         0.8               0.77778    150      0.78034   0.55840\n  0.3   2         0.8               0.77778    200      0.78724   0.57165\n  0.3   2         0.8               0.77778    250      0.78724   0.57183\n  0.3   2         0.8               0.77778    300      0.78057   0.55844\n  0.3   2         0.8               0.77778    350      0.79057   0.57909\n  0.3   2         0.8               0.77778    400      0.78391   0.56492\n  0.3   2         0.8               0.77778    450      0.77713   0.55194\n  0.3   2         0.8               0.77778    500      0.77379   0.54474\n  0.3   2         0.8               0.83333     50      0.81402   0.62497\n  0.3   2         0.8               0.83333    100      0.79391   0.58421\n  0.3   2         0.8               0.83333    150      0.79379   0.58468\n  0.3   2         0.8               0.83333    200      0.78046   0.55881\n  0.3   2         0.8               0.83333    250      0.77379   0.54450\n  0.3   2         0.8               0.83333    300      0.78069   0.55946\n  0.3   2         0.8               0.83333    350      0.77391   0.54612\n  0.3   2         0.8               0.83333    400      0.77057   0.53927\n  0.3   2         0.8               0.83333    450      0.77057   0.53927\n  0.3   2         0.8               0.83333    500      0.77391   0.54570\n  0.3   2         0.8               0.88889     50      0.80391   0.60474\n  0.3   2         0.8               0.88889    100      0.78368   0.56547\n  0.3   2         0.8               0.88889    150      0.78379   0.56503\n  0.3   2         0.8               0.88889    200      0.77011   0.53991\n  0.3   2         0.8               0.88889    250      0.77356   0.54625\n  0.3   2         0.8               0.88889    300      0.77713   0.55328\n  0.3   2         0.8               0.88889    350      0.77713   0.55376\n  0.3   2         0.8               0.88889    400      0.77724   0.55333\n  0.3   2         0.8               0.88889    450      0.77391   0.54684\n  0.3   2         0.8               0.88889    500      0.77057   0.53993\n  0.3   2         0.8               0.94444     50      0.81414   0.62486\n  0.3   2         0.8               0.94444    100      0.80747   0.61172\n  0.3   2         0.8               0.94444    150      0.78368   0.56494\n  0.3   2         0.8               0.94444    200      0.78356   0.56529\n  0.3   2         0.8               0.94444    250      0.78713   0.57245\n  0.3   2         0.8               0.94444    300      0.78379   0.56492\n  0.3   2         0.8               0.94444    350      0.77713   0.55233\n  0.3   2         0.8               0.94444    400      0.78057   0.55965\n  0.3   2         0.8               0.94444    450      0.78736   0.57243\n  0.3   2         0.8               0.94444    500      0.78736   0.57243\n  0.3   2         0.8               1.00000     50      0.80736   0.61165\n  0.3   2         0.8               1.00000    100      0.78713   0.57184\n  0.3   2         0.8               1.00000    150      0.78368   0.56553\n  0.3   2         0.8               1.00000    200      0.78046   0.55842\n  0.3   2         0.8               1.00000    250      0.78391   0.56628\n  0.3   2         0.8               1.00000    300      0.77391   0.54669\n  0.3   2         0.8               1.00000    350      0.78069   0.55953\n  0.3   2         0.8               1.00000    400      0.77747   0.55197\n  0.3   2         0.8               1.00000    450      0.77391   0.54492\n  0.3   2         0.8               1.00000    500      0.77736   0.55136\n  0.3   3         0.6               0.50000     50      0.81092   0.61878\n  0.3   3         0.6               0.50000    100      0.80069   0.59879\n  0.3   3         0.6               0.50000    150      0.79736   0.59219\n  0.3   3         0.6               0.50000    200      0.80069   0.59922\n  0.3   3         0.6               0.50000    250      0.79402   0.58521\n  0.3   3         0.6               0.50000    300      0.80402   0.60613\n  0.3   3         0.6               0.50000    350      0.80069   0.59916\n  0.3   3         0.6               0.50000    400      0.79736   0.59262\n  0.3   3         0.6               0.50000    450      0.79736   0.59256\n  0.3   3         0.6               0.50000    500      0.79736   0.59256\n  0.3   3         0.6               0.55556     50      0.80391   0.60623\n  0.3   3         0.6               0.55556    100      0.79736   0.59226\n  0.3   3         0.6               0.55556    150      0.78724   0.57196\n  0.3   3         0.6               0.55556    200      0.78379   0.56534\n  0.3   3         0.6               0.55556    250      0.78391   0.56612\n  0.3   3         0.6               0.55556    300      0.77368   0.54608\n  0.3   3         0.6               0.55556    350      0.78379   0.56533\n  0.3   3         0.6               0.55556    400      0.78724   0.57272\n  0.3   3         0.6               0.55556    450      0.78057   0.55945\n  0.3   3         0.6               0.55556    500      0.78391   0.56636\n  0.3   3         0.6               0.61111     50      0.80782   0.61221\n  0.3   3         0.6               0.61111    100      0.80080   0.59885\n  0.3   3         0.6               0.61111    150      0.79402   0.58436\n  0.3   3         0.6               0.61111    200      0.78402   0.56510\n  0.3   3         0.6               0.61111    250      0.79069   0.57976\n  0.3   3         0.6               0.61111    300      0.79414   0.58630\n  0.3   3         0.6               0.61111    350      0.78724   0.57244\n  0.3   3         0.6               0.61111    400      0.79069   0.57897\n  0.3   3         0.6               0.61111    450      0.77724   0.55273\n  0.3   3         0.6               0.61111    500      0.77724   0.55273\n  0.3   3         0.6               0.66667     50      0.81115   0.62163\n  0.3   3         0.6               0.66667    100      0.78724   0.57171\n  0.3   3         0.6               0.66667    150      0.79414   0.58537\n  0.3   3         0.6               0.66667    200      0.79736   0.59272\n  0.3   3         0.6               0.66667    250      0.79402   0.58635\n  0.3   3         0.6               0.66667    300      0.80069   0.59969\n  0.3   3         0.6               0.66667    350      0.78736   0.57266\n  0.3   3         0.6               0.66667    400      0.78402   0.56593\n  0.3   3         0.6               0.66667    450      0.79069   0.57944\n  0.3   3         0.6               0.66667    500      0.78736   0.57284\n  0.3   3         0.6               0.72222     50      0.81770   0.63200\n  0.3   3         0.6               0.72222    100      0.80425   0.60535\n  0.3   3         0.6               0.72222    150      0.79080   0.57861\n  0.3   3         0.6               0.72222    200      0.79414   0.58516\n  0.3   3         0.6               0.72222    250      0.79080   0.57873\n  0.3   3         0.6               0.72222    300      0.79069   0.57875\n  0.3   3         0.6               0.72222    350      0.78402   0.56524\n  0.3   3         0.6               0.72222    400      0.78402   0.56560\n  0.3   3         0.6               0.72222    450      0.78402   0.56590\n  0.3   3         0.6               0.72222    500      0.78069   0.55900\n  0.3   3         0.6               0.77778     50      0.78713   0.57426\n  0.3   3         0.6               0.77778    100      0.79080   0.57939\n  0.3   3         0.6               0.77778    150      0.78046   0.55797\n  0.3   3         0.6               0.77778    200      0.77701   0.55160\n  0.3   3         0.6               0.77778    250      0.77701   0.55209\n  0.3   3         0.6               0.77778    300      0.78046   0.55843\n  0.3   3         0.6               0.77778    350      0.78046   0.55930\n  0.3   3         0.6               0.77778    400      0.78402   0.56625\n  0.3   3         0.6               0.77778    450      0.78402   0.56589\n  0.3   3         0.6               0.77778    500      0.78402   0.56589\n  0.3   3         0.6               0.83333     50      0.80103   0.59908\n  0.3   3         0.6               0.83333    100      0.79747   0.59117\n  0.3   3         0.6               0.83333    150      0.79080   0.57883\n  0.3   3         0.6               0.83333    200      0.78414   0.56595\n  0.3   3         0.6               0.83333    250      0.79080   0.57971\n  0.3   3         0.6               0.83333    300      0.79414   0.58631\n  0.3   3         0.6               0.83333    350      0.79414   0.58631\n  0.3   3         0.6               0.83333    400      0.79414   0.58631\n  0.3   3         0.6               0.83333    450      0.79414   0.58631\n  0.3   3         0.6               0.83333    500      0.79069   0.57909\n  0.3   3         0.6               0.88889     50      0.78724   0.57334\n  0.3   3         0.6               0.88889    100      0.78057   0.55850\n  0.3   3         0.6               0.88889    150      0.78391   0.56567\n  0.3   3         0.6               0.88889    200      0.78391   0.56635\n  0.3   3         0.6               0.88889    250      0.77057   0.53981\n  0.3   3         0.6               0.88889    300      0.78069   0.56059\n  0.3   3         0.6               0.88889    350      0.77736   0.55392\n  0.3   3         0.6               0.88889    400      0.78069   0.56059\n  0.3   3         0.6               0.88889    450      0.78069   0.56059\n  0.3   3         0.6               0.88889    500      0.77736   0.55392\n  0.3   3         0.6               0.94444     50      0.80057   0.59842\n  0.3   3         0.6               0.94444    100      0.78368   0.56517\n  0.3   3         0.6               0.94444    150      0.78713   0.57152\n  0.3   3         0.6               0.94444    200      0.78368   0.56505\n  0.3   3         0.6               0.94444    250      0.78713   0.57282\n  0.3   3         0.6               0.94444    300      0.78391   0.56632\n  0.3   3         0.6               0.94444    350      0.78046   0.55910\n  0.3   3         0.6               0.94444    400      0.78391   0.56632\n  0.3   3         0.6               0.94444    450      0.78046   0.55910\n  0.3   3         0.6               0.94444    500      0.78046   0.55910\n  0.3   3         0.6               1.00000     50      0.80425   0.60623\n  0.3   3         0.6               1.00000    100      0.78747   0.57313\n  0.3   3         0.6               1.00000    150      0.79080   0.57991\n  0.3   3         0.6               1.00000    200      0.79080   0.58028\n  0.3   3         0.6               1.00000    250      0.79080   0.58003\n  0.3   3         0.6               1.00000    300      0.79414   0.58688\n  0.3   3         0.6               1.00000    350      0.79069   0.58054\n  0.3   3         0.6               1.00000    400      0.79414   0.58688\n  0.3   3         0.6               1.00000    450      0.79414   0.58688\n  0.3   3         0.6               1.00000    500      0.79747   0.59343\n  0.3   3         0.8               0.50000     50      0.80425   0.60403\n  0.3   3         0.8               0.50000    100      0.80080   0.59794\n  0.3   3         0.8               0.50000    150      0.79080   0.57740\n  0.3   3         0.8               0.50000    200      0.79759   0.59253\n  0.3   3         0.8               0.50000    250      0.79080   0.57837\n  0.3   3         0.8               0.50000    300      0.78069   0.55911\n  0.3   3         0.8               0.50000    350      0.78747   0.57207\n  0.3   3         0.8               0.50000    400      0.78414   0.56504\n  0.3   3         0.8               0.50000    450      0.77747   0.55243\n  0.3   3         0.8               0.50000    500      0.78080   0.55922\n  0.3   3         0.8               0.55556     50      0.80425   0.60471\n  0.3   3         0.8               0.55556    100      0.79046   0.57855\n  0.3   3         0.8               0.55556    150      0.79736   0.59309\n  0.3   3         0.8               0.55556    200      0.79391   0.58680\n  0.3   3         0.8               0.55556    250      0.79402   0.58661\n  0.3   3         0.8               0.55556    300      0.78402   0.56661\n  0.3   3         0.8               0.55556    350      0.78402   0.56720\n  0.3   3         0.8               0.55556    400      0.78402   0.56667\n  0.3   3         0.8               0.55556    450      0.78402   0.56655\n  0.3   3         0.8               0.55556    500      0.78402   0.56721\n  0.3   3         0.8               0.61111     50      0.81103   0.61971\n  0.3   3         0.8               0.61111    100      0.81437   0.62535\n  0.3   3         0.8               0.61111    150      0.80092   0.59854\n  0.3   3         0.8               0.61111    200      0.79747   0.59292\n  0.3   3         0.8               0.61111    250      0.79747   0.59292\n  0.3   3         0.8               0.61111    300      0.79747   0.59292\n  0.3   3         0.8               0.61111    350      0.79759   0.59304\n  0.3   3         0.8               0.61111    400      0.79414   0.58632\n  0.3   3         0.8               0.61111    450      0.78747   0.57311\n  0.3   3         0.8               0.61111    500      0.79080   0.57995\n  0.3   3         0.8               0.66667     50      0.80414   0.60536\n  0.3   3         0.8               0.66667    100      0.78724   0.57207\n  0.3   3         0.8               0.66667    150      0.78379   0.56530\n  0.3   3         0.8               0.66667    200      0.79736   0.59284\n  0.3   3         0.8               0.66667    250      0.79402   0.58635\n  0.3   3         0.8               0.66667    300      0.78724   0.57355\n  0.3   3         0.8               0.66667    350      0.78724   0.57355\n  0.3   3         0.8               0.66667    400      0.79069   0.57998\n  0.3   3         0.8               0.66667    450      0.79069   0.57998\n  0.3   3         0.8               0.66667    500      0.78736   0.57314\n  0.3   3         0.8               0.72222     50      0.79437   0.58477\n  0.3   3         0.8               0.72222    100      0.79402   0.58539\n  0.3   3         0.8               0.72222    150      0.77724   0.55110\n  0.3   3         0.8               0.72222    200      0.78391   0.56510\n  0.3   3         0.8               0.72222    250      0.78379   0.56473\n  0.3   3         0.8               0.72222    300      0.78057   0.55896\n  0.3   3         0.8               0.72222    350      0.78736   0.57231\n  0.3   3         0.8               0.72222    400      0.78057   0.55897\n  0.3   3         0.8               0.72222    450      0.77713   0.55261\n  0.3   3         0.8               0.72222    500      0.77379   0.54570\n  0.3   3         0.8               0.77778     50      0.80425   0.60473\n  0.3   3         0.8               0.77778    100      0.79080   0.57962\n  0.3   3         0.8               0.77778    150      0.79069   0.57917\n  0.3   3         0.8               0.77778    200      0.78736   0.57274\n  0.3   3         0.8               0.77778    250      0.78736   0.57286\n  0.3   3         0.8               0.77778    300      0.78391   0.56643\n  0.3   3         0.8               0.77778    350      0.78057   0.55976\n  0.3   3         0.8               0.77778    400      0.77736   0.55299\n  0.3   3         0.8               0.77778    450      0.77391   0.54577\n  0.3   3         0.8               0.77778    500      0.77391   0.54577\n  0.3   3         0.8               0.83333     50      0.79747   0.59187\n  0.3   3         0.8               0.83333    100      0.79747   0.59333\n  0.3   3         0.8               0.83333    150      0.79425   0.58657\n  0.3   3         0.8               0.83333    200      0.79080   0.57989\n  0.3   3         0.8               0.83333    250      0.78747   0.57286\n  0.3   3         0.8               0.83333    300      0.78069   0.56026\n  0.3   3         0.8               0.83333    350      0.78414   0.56661\n  0.3   3         0.8               0.83333    400      0.78080   0.55934\n  0.3   3         0.8               0.83333    450      0.78080   0.56012\n  0.3   3         0.8               0.83333    500      0.77391   0.54655\n  0.3   3         0.8               0.88889     50      0.80414   0.60637\n  0.3   3         0.8               0.88889    100      0.78391   0.56625\n  0.3   3         0.8               0.88889    150      0.78391   0.56699\n  0.3   3         0.8               0.88889    200      0.78724   0.57383\n  0.3   3         0.8               0.88889    250      0.79080   0.58039\n  0.3   3         0.8               0.88889    300      0.79080   0.58039\n  0.3   3         0.8               0.88889    350      0.79080   0.58039\n  0.3   3         0.8               0.88889    400      0.79080   0.58039\n  0.3   3         0.8               0.88889    450      0.79414   0.58730\n  0.3   3         0.8               0.88889    500      0.79069   0.58095\n  0.3   3         0.8               0.94444     50      0.78391   0.56527\n  0.3   3         0.8               0.94444    100      0.80402   0.60638\n  0.3   3         0.8               0.94444    150      0.79069   0.57905\n  0.3   3         0.8               0.94444    200      0.79069   0.57941\n  0.3   3         0.8               0.94444    250      0.79402   0.58596\n  0.3   3         0.8               0.94444    300      0.79069   0.57941\n  0.3   3         0.8               0.94444    350      0.79069   0.57953\n  0.3   3         0.8               0.94444    400      0.79414   0.58665\n  0.3   3         0.8               0.94444    450      0.79080   0.58010\n  0.3   3         0.8               0.94444    500      0.79414   0.58671\n  0.3   3         0.8               1.00000     50      0.81080   0.61921\n  0.3   3         0.8               1.00000    100      0.80092   0.59960\n  0.3   3         0.8               1.00000    150      0.79425   0.58670\n  0.3   3         0.8               1.00000    200      0.80092   0.60052\n  0.3   3         0.8               1.00000    250      0.79759   0.59284\n  0.3   3         0.8               1.00000    300      0.79759   0.59284\n  0.3   3         0.8               1.00000    350      0.79759   0.59284\n  0.3   3         0.8               1.00000    400      0.79759   0.59284\n  0.3   3         0.8               1.00000    450      0.79425   0.58581\n  0.3   3         0.8               1.00000    500      0.79092   0.57914\n  0.3   4         0.6               0.50000     50      0.81425   0.62756\n  0.3   4         0.6               0.50000    100      0.79368   0.58607\n  0.3   4         0.6               0.50000    150      0.78368   0.56552\n  0.3   4         0.6               0.50000    200      0.79069   0.57990\n  0.3   4         0.6               0.50000    250      0.79402   0.58747\n  0.3   4         0.6               0.50000    300      0.77713   0.55247\n  0.3   4         0.6               0.50000    350      0.78736   0.57285\n  0.3   4         0.6               0.50000    400      0.78057   0.55921\n  0.3   4         0.6               0.50000    450      0.78057   0.55958\n  0.3   4         0.6               0.50000    500      0.77713   0.55225\n  0.3   4         0.6               0.55556     50      0.81460   0.62649\n  0.3   4         0.6               0.55556    100      0.81092   0.62159\n  0.3   4         0.6               0.55556    150      0.79736   0.59261\n  0.3   4         0.6               0.55556    200      0.80414   0.60542\n  0.3   4         0.6               0.55556    250      0.79080   0.57916\n  0.3   4         0.6               0.55556    300      0.81092   0.61941\n  0.3   4         0.6               0.55556    350      0.79736   0.59377\n  0.3   4         0.6               0.55556    400      0.80069   0.60056\n  0.3   4         0.6               0.55556    450      0.78402   0.56657\n  0.3   4         0.6               0.55556    500      0.78736   0.57359\n  0.3   4         0.6               0.61111     50      0.79747   0.59179\n  0.3   4         0.6               0.61111    100      0.79736   0.59220\n  0.3   4         0.6               0.61111    150      0.80080   0.59979\n  0.3   4         0.6               0.61111    200      0.79080   0.57814\n  0.3   4         0.6               0.61111    250      0.79414   0.58523\n  0.3   4         0.6               0.61111    300      0.79747   0.59207\n  0.3   4         0.6               0.61111    350      0.80080   0.59892\n  0.3   4         0.6               0.61111    400      0.79747   0.59208\n  0.3   4         0.6               0.61111    450      0.79747   0.59208\n  0.3   4         0.6               0.61111    500      0.79069   0.57874\n  0.3   4         0.6               0.66667     50      0.79391   0.58580\n  0.3   4         0.6               0.66667    100      0.79724   0.59157\n  0.3   4         0.6               0.66667    150      0.78713   0.57207\n  0.3   4         0.6               0.66667    200      0.78046   0.55953\n  0.3   4         0.6               0.66667    250      0.78046   0.55953\n  0.3   4         0.6               0.66667    300      0.77379   0.54650\n  0.3   4         0.6               0.66667    350      0.77379   0.54662\n  0.3   4         0.6               0.66667    400      0.76713   0.53340\n  0.3   4         0.6               0.66667    450      0.77057   0.53981\n  0.3   4         0.6               0.66667    500      0.76391   0.52648\n  0.3   4         0.6               0.72222     50      0.81437   0.62636\n  0.3   4         0.6               0.72222    100      0.80747   0.61276\n  0.3   4         0.6               0.72222    150      0.80080   0.59963\n  0.3   4         0.6               0.72222    200      0.79736   0.59288\n  0.3   4         0.6               0.72222    250      0.79046   0.58000\n  0.3   4         0.6               0.72222    300      0.79046   0.58012\n  0.3   4         0.6               0.72222    350      0.78368   0.56627\n  0.3   4         0.6               0.72222    400      0.78368   0.56627\n  0.3   4         0.6               0.72222    450      0.78034   0.55967\n  0.3   4         0.6               0.72222    500      0.78701   0.57324\n  0.3   4         0.6               0.77778     50      0.79747   0.59103\n  0.3   4         0.6               0.77778    100      0.79069   0.57729\n  0.3   4         0.6               0.77778    150      0.79069   0.57753\n  0.3   4         0.6               0.77778    200      0.78736   0.57075\n  0.3   4         0.6               0.77778    250      0.79080   0.57851\n  0.3   4         0.6               0.77778    300      0.79080   0.57851\n  0.3   4         0.6               0.77778    350      0.79069   0.57874\n  0.3   4         0.6               0.77778    400      0.78736   0.57208\n  0.3   4         0.6               0.77778    450      0.78736   0.57208\n  0.3   4         0.6               0.77778    500      0.78736   0.57208\n  0.3   4         0.6               0.83333     50      0.80069   0.59962\n  0.3   4         0.6               0.83333    100      0.80402   0.60611\n  0.3   4         0.6               0.83333    150      0.80069   0.59981\n  0.3   4         0.6               0.83333    200      0.79402   0.58506\n  0.3   4         0.6               0.83333    250      0.79747   0.59227\n  0.3   4         0.6               0.83333    300      0.80425   0.60573\n  0.3   4         0.6               0.83333    350      0.80425   0.60585\n  0.3   4         0.6               0.83333    400      0.80414   0.60626\n  0.3   4         0.6               0.83333    450      0.80414   0.60566\n  0.3   4         0.6               0.83333    500      0.79747   0.59239\n  0.3   4         0.6               0.88889     50      0.80391   0.60570\n  0.3   4         0.6               0.88889    100      0.79069   0.57844\n  0.3   4         0.6               0.88889    150      0.79057   0.57874\n  0.3   4         0.6               0.88889    200      0.78724   0.57190\n  0.3   4         0.6               0.88889    250      0.79391   0.58565\n  0.3   4         0.6               0.88889    300      0.79057   0.57923\n  0.3   4         0.6               0.88889    350      0.79057   0.57923\n  0.3   4         0.6               0.88889    400      0.79736   0.59286\n  0.3   4         0.6               0.88889    450      0.79069   0.57977\n  0.3   4         0.6               0.88889    500      0.79069   0.57977\n  0.3   4         0.6               0.94444     50      0.81437   0.62604\n  0.3   4         0.6               0.94444    100      0.80759   0.61290\n  0.3   4         0.6               0.94444    150      0.80759   0.61254\n  0.3   4         0.6               0.94444    200      0.80414   0.60623\n  0.3   4         0.6               0.94444    250      0.80414   0.60623\n  0.3   4         0.6               0.94444    300      0.80414   0.60623\n  0.3   4         0.6               0.94444    350      0.79747   0.59290\n  0.3   4         0.6               0.94444    400      0.79747   0.59290\n  0.3   4         0.6               0.94444    450      0.79747   0.59290\n  0.3   4         0.6               0.94444    500      0.79747   0.59290\n  0.3   4         0.6               1.00000     50      0.77724   0.55155\n  0.3   4         0.6               1.00000    100      0.77414   0.54484\n  0.3   4         0.6               1.00000    150      0.77080   0.53726\n  0.3   4         0.6               1.00000    200      0.77402   0.54476\n  0.3   4         0.6               1.00000    250      0.78069   0.55834\n  0.3   4         0.6               1.00000    300      0.78057   0.55891\n  0.3   4         0.6               1.00000    350      0.78402   0.56525\n  0.3   4         0.6               1.00000    400      0.78402   0.56525\n  0.3   4         0.6               1.00000    450      0.77724   0.55236\n  0.3   4         0.6               1.00000    500      0.77736   0.55204\n  0.3   4         0.8               0.50000     50      0.80092   0.59886\n  0.3   4         0.8               0.50000    100      0.80747   0.61227\n  0.3   4         0.8               0.50000    150      0.81080   0.61923\n  0.3   4         0.8               0.50000    200      0.80414   0.60560\n  0.3   4         0.8               0.50000    250      0.79414   0.58542\n  0.3   4         0.8               0.50000    300      0.79057   0.57843\n  0.3   4         0.8               0.50000    350      0.80092   0.59879\n  0.3   4         0.8               0.50000    400      0.80080   0.59893\n  0.3   4         0.8               0.50000    450      0.80080   0.59923\n  0.3   4         0.8               0.50000    500      0.80747   0.61256\n  0.3   4         0.8               0.55556     50      0.80115   0.59734\n  0.3   4         0.8               0.55556    100      0.78414   0.56401\n  0.3   4         0.8               0.55556    150      0.78747   0.57091\n  0.3   4         0.8               0.55556    200      0.78425   0.56399\n  0.3   4         0.8               0.55556    250      0.78759   0.57011\n  0.3   4         0.8               0.55556    300      0.78425   0.56438\n  0.3   4         0.8               0.55556    350      0.78437   0.56406\n  0.3   4         0.8               0.55556    400      0.77402   0.54425\n  0.3   4         0.8               0.55556    450      0.78069   0.55764\n  0.3   4         0.8               0.55556    500      0.78092   0.55789\n  0.3   4         0.8               0.61111     50      0.81115   0.61834\n  0.3   4         0.8               0.61111    100      0.81425   0.62580\n  0.3   4         0.8               0.61111    150      0.80747   0.61194\n  0.3   4         0.8               0.61111    200      0.81092   0.61875\n  0.3   4         0.8               0.61111    250      0.80080   0.59910\n  0.3   4         0.8               0.61111    300      0.81092   0.61992\n  0.3   4         0.8               0.61111    350      0.81080   0.61952\n  0.3   4         0.8               0.61111    400      0.80747   0.61279\n  0.3   4         0.8               0.61111    450      0.80747   0.61297\n  0.3   4         0.8               0.61111    500      0.80069   0.60001\n  0.3   4         0.8               0.66667     50      0.80759   0.61097\n  0.3   4         0.8               0.66667    100      0.80425   0.60600\n  0.3   4         0.8               0.66667    150      0.79747   0.59197\n  0.3   4         0.8               0.66667    200      0.79747   0.59245\n  0.3   4         0.8               0.66667    250      0.79414   0.58620\n  0.3   4         0.8               0.66667    300      0.79080   0.57850\n  0.3   4         0.8               0.66667    350      0.79747   0.59208\n  0.3   4         0.8               0.66667    400      0.78414   0.56486\n  0.3   4         0.8               0.66667    450      0.79069   0.57946\n  0.3   4         0.8               0.66667    500      0.78736   0.57255\n  0.3   4         0.8               0.72222     50      0.78759   0.57170\n  0.3   4         0.8               0.72222    100      0.79080   0.57766\n  0.3   4         0.8               0.72222    150      0.78402   0.56354\n  0.3   4         0.8               0.72222    200      0.78092   0.55834\n  0.3   4         0.8               0.72222    250      0.78437   0.56527\n  0.3   4         0.8               0.72222    300      0.78103   0.55891\n  0.3   4         0.8               0.72222    350      0.77770   0.55260\n  0.3   4         0.8               0.72222    400      0.77437   0.54611\n  0.3   4         0.8               0.72222    450      0.77437   0.54611\n  0.3   4         0.8               0.72222    500      0.77437   0.54611\n  0.3   4         0.8               0.77778     50      0.81425   0.62587\n  0.3   4         0.8               0.77778    100      0.80069   0.59780\n  0.3   4         0.8               0.77778    150      0.79080   0.57824\n  0.3   4         0.8               0.77778    200      0.80092   0.59881\n  0.3   4         0.8               0.77778    250      0.80080   0.59982\n  0.3   4         0.8               0.77778    300      0.79080   0.57947\n  0.3   4         0.8               0.77778    350      0.78414   0.56620\n  0.3   4         0.8               0.77778    400      0.78414   0.56667\n  0.3   4         0.8               0.77778    450      0.77747   0.55298\n  0.3   4         0.8               0.77778    500      0.78080   0.55989\n  0.3   4         0.8               0.83333     50      0.81103   0.61852\n  0.3   4         0.8               0.83333    100      0.80759   0.61109\n  0.3   4         0.8               0.83333    150      0.80425   0.60437\n  0.3   4         0.8               0.83333    200      0.79759   0.59116\n  0.3   4         0.8               0.83333    250      0.79414   0.58488\n  0.3   4         0.8               0.83333    300      0.79414   0.58488\n  0.3   4         0.8               0.83333    350      0.79080   0.57821\n  0.3   4         0.8               0.83333    400      0.79080   0.57821\n  0.3   4         0.8               0.83333    450      0.79080   0.57821\n  0.3   4         0.8               0.83333    500      0.79080   0.57821\n  0.3   4         0.8               0.88889     50      0.80747   0.61355\n  0.3   4         0.8               0.88889    100      0.80414   0.60664\n  0.3   4         0.8               0.88889    150      0.79747   0.59312\n  0.3   4         0.8               0.88889    200      0.79747   0.59312\n  0.3   4         0.8               0.88889    250      0.79414   0.58675\n  0.3   4         0.8               0.88889    300      0.80425   0.60772\n  0.3   4         0.8               0.88889    350      0.80092   0.60087\n  0.3   4         0.8               0.88889    400      0.80425   0.60772\n  0.3   4         0.8               0.88889    450      0.80092   0.60087\n  0.3   4         0.8               0.88889    500      0.80425   0.60772\n  0.3   4         0.8               0.94444     50      0.81425   0.62531\n  0.3   4         0.8               0.94444    100      0.80092   0.59889\n  0.3   4         0.8               0.94444    150      0.78747   0.57175\n  0.3   4         0.8               0.94444    200      0.78402   0.56486\n  0.3   4         0.8               0.94444    250      0.78414   0.56529\n  0.3   4         0.8               0.94444    300      0.78402   0.56497\n  0.3   4         0.8               0.94444    350      0.78736   0.57134\n  0.3   4         0.8               0.94444    400      0.78736   0.57134\n  0.3   4         0.8               0.94444    450      0.78736   0.57117\n  0.3   4         0.8               0.94444    500      0.78736   0.57117\n  0.3   4         0.8               1.00000     50      0.80414   0.60580\n  0.3   4         0.8               1.00000    100      0.78736   0.57258\n  0.3   4         0.8               1.00000    150      0.78402   0.56549\n  0.3   4         0.8               1.00000    200      0.78069   0.55907\n  0.3   4         0.8               1.00000    250      0.78069   0.55907\n  0.3   4         0.8               1.00000    300      0.78069   0.55907\n  0.3   4         0.8               1.00000    350      0.77736   0.55241\n  0.3   4         0.8               1.00000    400      0.77736   0.55241\n  0.3   4         0.8               1.00000    450      0.77736   0.55241\n  0.3   4         0.8               1.00000    500      0.77402   0.54604\n  0.3   5         0.6               0.50000     50      0.80115   0.60113\n  0.3   5         0.6               0.50000    100      0.80437   0.60710\n  0.3   5         0.6               0.50000    150      0.79759   0.59146\n  0.3   5         0.6               0.50000    200      0.79747   0.59187\n  0.3   5         0.6               0.50000    250      0.79057   0.57903\n  0.3   5         0.6               0.50000    300      0.78402   0.56552\n  0.3   5         0.6               0.50000    350      0.79069   0.57769\n  0.3   5         0.6               0.50000    400      0.77724   0.55237\n  0.3   5         0.6               0.50000    450      0.78391   0.56430\n  0.3   5         0.6               0.50000    500      0.78736   0.57305\n  0.3   5         0.6               0.55556     50      0.79057   0.57844\n  0.3   5         0.6               0.55556    100      0.78724   0.57261\n  0.3   5         0.6               0.55556    150      0.79724   0.59326\n  0.3   5         0.6               0.55556    200      0.80057   0.60048\n  0.3   5         0.6               0.55556    250      0.79391   0.58688\n  0.3   5         0.6               0.55556    300      0.79736   0.59437\n  0.3   5         0.6               0.55556    350      0.79046   0.58007\n  0.3   5         0.6               0.55556    400      0.78713   0.57346\n  0.3   5         0.6               0.55556    450      0.79046   0.58007\n  0.3   5         0.6               0.55556    500      0.79057   0.57981\n  0.3   5         0.6               0.61111     50      0.79736   0.59269\n  0.3   5         0.6               0.61111    100      0.79747   0.59276\n  0.3   5         0.6               0.61111    150      0.79414   0.58538\n  0.3   5         0.6               0.61111    200      0.80080   0.59883\n  0.3   5         0.6               0.61111    250      0.79747   0.59260\n  0.3   5         0.6               0.61111    300      0.79402   0.58556\n  0.3   5         0.6               0.61111    350      0.78402   0.56522\n  0.3   5         0.6               0.61111    400      0.77724   0.55183\n  0.3   5         0.6               0.61111    450      0.78414   0.56528\n  0.3   5         0.6               0.61111    500      0.78069   0.55746\n  0.3   5         0.6               0.66667     50      0.79391   0.58511\n  0.3   5         0.6               0.66667    100      0.78391   0.56468\n  0.3   5         0.6               0.66667    150      0.78046   0.55825\n  0.3   5         0.6               0.66667    200      0.78046   0.55909\n  0.3   5         0.6               0.66667    250      0.77713   0.55206\n  0.3   5         0.6               0.66667    300      0.78057   0.55969\n  0.3   5         0.6               0.66667    350      0.78057   0.55998\n  0.3   5         0.6               0.66667    400      0.78057   0.55998\n  0.3   5         0.6               0.66667    450      0.78391   0.56629\n  0.3   5         0.6               0.66667    500      0.77391   0.54635\n  0.3   5         0.6               0.72222     50      0.78391   0.56643\n  0.3   5         0.6               0.72222    100      0.78046   0.55882\n  0.3   5         0.6               0.72222    150      0.78724   0.57146\n  0.3   5         0.6               0.72222    200      0.79069   0.57789\n  0.3   5         0.6               0.72222    250      0.77736   0.55160\n  0.3   5         0.6               0.72222    300      0.77724   0.55201\n  0.3   5         0.6               0.72222    350      0.77724   0.55214\n  0.3   5         0.6               0.72222    400      0.77391   0.54505\n  0.3   5         0.6               0.72222    450      0.76724   0.53153\n  0.3   5         0.6               0.72222    500      0.77046   0.53919\n  0.3   5         0.6               0.77778     50      0.80080   0.59837\n  0.3   5         0.6               0.77778    100      0.80414   0.60387\n  0.3   5         0.6               0.77778    150      0.80069   0.59751\n  0.3   5         0.6               0.77778    200      0.79391   0.58376\n  0.3   5         0.6               0.77778    250      0.80069   0.59751\n  0.3   5         0.6               0.77778    300      0.80069   0.59751\n  0.3   5         0.6               0.77778    350      0.80069   0.59751\n  0.3   5         0.6               0.77778    400      0.79736   0.59096\n  0.3   5         0.6               0.77778    450      0.80069   0.59787\n  0.3   5         0.6               0.77778    500      0.79402   0.58508\n  0.3   5         0.6               0.83333     50      0.80747   0.61350\n  0.3   5         0.6               0.83333    100      0.79736   0.59212\n  0.3   5         0.6               0.83333    150      0.79046   0.57935\n  0.3   5         0.6               0.83333    200      0.79391   0.58600\n  0.3   5         0.6               0.83333    250      0.79057   0.57957\n  0.3   5         0.6               0.83333    300      0.79402   0.58641\n  0.3   5         0.6               0.83333    350      0.79057   0.57939\n  0.3   5         0.6               0.83333    400      0.79402   0.58641\n  0.3   5         0.6               0.83333    450      0.78724   0.57284\n  0.3   5         0.6               0.83333    500      0.78736   0.57344\n  0.3   5         0.6               0.88889     50      0.81103   0.61901\n  0.3   5         0.6               0.88889    100      0.80437   0.60381\n  0.3   5         0.6               0.88889    150      0.79437   0.58411\n  0.3   5         0.6               0.88889    200      0.79092   0.57758\n  0.3   5         0.6               0.88889    250      0.79425   0.58449\n  0.3   5         0.6               0.88889    300      0.79425   0.58449\n  0.3   5         0.6               0.88889    350      0.79425   0.58491\n  0.3   5         0.6               0.88889    400      0.79414   0.58545\n  0.3   5         0.6               0.88889    450      0.79080   0.57890\n  0.3   5         0.6               0.88889    500      0.79425   0.58534\n  0.3   5         0.6               0.94444     50      0.79069   0.57777\n  0.3   5         0.6               0.94444    100      0.78747   0.57115\n  0.3   5         0.6               0.94444    150      0.78414   0.56430\n  0.3   5         0.6               0.94444    200      0.78080   0.55715\n  0.3   5         0.6               0.94444    250      0.78080   0.55733\n  0.3   5         0.6               0.94444    300      0.78414   0.56388\n  0.3   5         0.6               0.94444    350      0.78747   0.57072\n  0.3   5         0.6               0.94444    400      0.78080   0.55697\n  0.3   5         0.6               0.94444    450      0.78425   0.56419\n  0.3   5         0.6               0.94444    500      0.78425   0.56419\n  0.3   5         0.6               1.00000     50      0.78713   0.57289\n  0.3   5         0.6               1.00000    100      0.78437   0.56529\n  0.3   5         0.6               1.00000    150      0.79092   0.57966\n  0.3   5         0.6               1.00000    200      0.79092   0.57966\n  0.3   5         0.6               1.00000    250      0.79092   0.57966\n  0.3   5         0.6               1.00000    300      0.79092   0.57966\n  0.3   5         0.6               1.00000    350      0.79092   0.57966\n  0.3   5         0.6               1.00000    400      0.78759   0.57268\n  0.3   5         0.6               1.00000    450      0.79092   0.57924\n  0.3   5         0.6               1.00000    500      0.79425   0.58608\n  0.3   5         0.8               0.50000     50      0.80092   0.59752\n  0.3   5         0.8               0.50000    100      0.79782   0.59078\n  0.3   5         0.8               0.50000    150      0.80437   0.60457\n  0.3   5         0.8               0.50000    200      0.79414   0.58505\n  0.3   5         0.8               0.50000    250      0.78759   0.57061\n  0.3   5         0.8               0.50000    300      0.79759   0.59135\n  0.3   5         0.8               0.50000    350      0.79080   0.57772\n  0.3   5         0.8               0.50000    400      0.79080   0.57821\n  0.3   5         0.8               0.50000    450      0.79425   0.58474\n  0.3   5         0.8               0.50000    500      0.79414   0.58474\n  0.3   5         0.8               0.55556     50      0.79759   0.59187\n  0.3   5         0.8               0.55556    100      0.80069   0.59818\n  0.3   5         0.8               0.55556    150      0.79402   0.58438\n  0.3   5         0.8               0.55556    200      0.79736   0.59264\n  0.3   5         0.8               0.55556    250      0.80414   0.60617\n  0.3   5         0.8               0.55556    300      0.79080   0.57901\n  0.3   5         0.8               0.55556    350      0.79747   0.59241\n  0.3   5         0.8               0.55556    400      0.79080   0.57973\n  0.3   5         0.8               0.55556    450      0.79080   0.58058\n  0.3   5         0.8               0.55556    500      0.77747   0.55300\n  0.3   5         0.8               0.61111     50      0.80069   0.59910\n  0.3   5         0.8               0.61111    100      0.79736   0.59249\n  0.3   5         0.8               0.61111    150      0.79069   0.57917\n  0.3   5         0.8               0.61111    200      0.78402   0.56584\n  0.3   5         0.8               0.61111    250      0.79069   0.57917\n  0.3   5         0.8               0.61111    300      0.79069   0.57984\n  0.3   5         0.8               0.61111    350      0.79414   0.58685\n  0.3   5         0.8               0.61111    400      0.79747   0.59346\n  0.3   5         0.8               0.61111    450      0.78747   0.57430\n  0.3   5         0.8               0.61111    500      0.78402   0.56751\n  0.3   5         0.8               0.66667     50      0.79414   0.58713\n  0.3   5         0.8               0.66667    100      0.80759   0.61353\n  0.3   5         0.8               0.66667    150      0.78759   0.57426\n  0.3   5         0.8               0.66667    200      0.78747   0.57457\n  0.3   5         0.8               0.66667    250      0.78069   0.56019\n  0.3   5         0.8               0.66667    300      0.78736   0.57381\n  0.3   5         0.8               0.66667    350      0.79069   0.58041\n  0.3   5         0.8               0.66667    400      0.78736   0.57363\n  0.3   5         0.8               0.66667    450      0.78402   0.56766\n  0.3   5         0.8               0.66667    500      0.78069   0.56105\n  0.3   5         0.8               0.72222     50      0.80759   0.61222\n  0.3   5         0.8               0.72222    100      0.80092   0.59876\n  0.3   5         0.8               0.72222    150      0.79425   0.58531\n  0.3   5         0.8               0.72222    200      0.79425   0.58600\n  0.3   5         0.8               0.72222    250      0.79414   0.58496\n  0.3   5         0.8               0.72222    300      0.78736   0.57147\n  0.3   5         0.8               0.72222    350      0.78747   0.57214\n  0.3   5         0.8               0.72222    400      0.79080   0.57917\n  0.3   5         0.8               0.72222    450      0.78069   0.55909\n  0.3   5         0.8               0.72222    500      0.78402   0.56594\n  0.3   5         0.8               0.77778     50      0.82103   0.63880\n  0.3   5         0.8               0.77778    100      0.80080   0.59892\n  0.3   5         0.8               0.77778    150      0.81103   0.61924\n  0.3   5         0.8               0.77778    200      0.80759   0.61212\n  0.3   5         0.8               0.77778    250      0.81092   0.61936\n  0.3   5         0.8               0.77778    300      0.80759   0.61281\n  0.3   5         0.8               0.77778    350      0.80747   0.61293\n  0.3   5         0.8               0.77778    400      0.81092   0.61936\n  0.3   5         0.8               0.77778    450      0.80759   0.61299\n  0.3   5         0.8               0.77778    500      0.80759   0.61299\n  0.3   5         0.8               0.83333     50      0.80414   0.60503\n  0.3   5         0.8               0.83333    100      0.80425   0.60516\n  0.3   5         0.8               0.83333    150      0.79747   0.59209\n  0.3   5         0.8               0.83333    200      0.78747   0.57203\n  0.3   5         0.8               0.83333    250      0.79069   0.57844\n  0.3   5         0.8               0.83333    300      0.78402   0.56560\n  0.3   5         0.8               0.83333    350      0.78391   0.56541\n  0.3   5         0.8               0.83333    400      0.78402   0.56560\n  0.3   5         0.8               0.83333    450      0.78414   0.56645\n  0.3   5         0.8               0.83333    500      0.78402   0.56667\n  0.3   5         0.8               0.88889     50      0.79092   0.57853\n  0.3   5         0.8               0.88889    100      0.79414   0.58565\n  0.3   5         0.8               0.88889    150      0.78747   0.57268\n  0.3   5         0.8               0.88889    200      0.79414   0.58608\n  0.3   5         0.8               0.88889    250      0.79080   0.57941\n  0.3   5         0.8               0.88889    300      0.79414   0.58703\n  0.3   5         0.8               0.88889    350      0.79080   0.58001\n  0.3   5         0.8               0.88889    400      0.79080   0.58001\n  0.3   5         0.8               0.88889    450      0.79080   0.58001\n  0.3   5         0.8               0.88889    500      0.79080   0.58001\n  0.3   5         0.8               0.94444     50      0.79736   0.59258\n  0.3   5         0.8               0.94444    100      0.78414   0.56654\n  0.3   5         0.8               0.94444    150      0.78736   0.57177\n  0.3   5         0.8               0.94444    200      0.78402   0.56577\n  0.3   5         0.8               0.94444    250      0.79080   0.57935\n  0.3   5         0.8               0.94444    300      0.79747   0.59287\n  0.3   5         0.8               0.94444    350      0.79414   0.58626\n  0.3   5         0.8               0.94444    400      0.79425   0.58642\n  0.3   5         0.8               0.94444    450      0.79092   0.58017\n  0.3   5         0.8               0.94444    500      0.78759   0.57321\n  0.3   5         0.8               1.00000     50      0.80770   0.61343\n  0.3   5         0.8               1.00000    100      0.80092   0.60051\n  0.3   5         0.8               1.00000    150      0.80425   0.60729\n  0.3   5         0.8               1.00000    200      0.80425   0.60746\n  0.3   5         0.8               1.00000    250      0.80759   0.61337\n  0.3   5         0.8               1.00000    300      0.79425   0.58712\n  0.3   5         0.8               1.00000    350      0.79414   0.58696\n  0.3   5         0.8               1.00000    400      0.78736   0.57357\n  0.3   5         0.8               1.00000    450      0.78736   0.57357\n  0.3   5         0.8               1.00000    500      0.78736   0.57357\n  0.3   6         0.6               0.50000     50      0.81770   0.63348\n  0.3   6         0.6               0.50000    100      0.81782   0.63264\n  0.3   6         0.6               0.50000    150      0.80092   0.59938\n  0.3   6         0.6               0.50000    200      0.80414   0.60521\n  0.3   6         0.6               0.50000    250      0.81425   0.62598\n  0.3   6         0.6               0.50000    300      0.81080   0.62012\n  0.3   6         0.6               0.50000    350      0.80414   0.60659\n  0.3   6         0.6               0.50000    400      0.80069   0.59917\n  0.3   6         0.6               0.50000    450      0.80736   0.61261\n  0.3   6         0.6               0.50000    500      0.79402   0.58582\n  0.3   6         0.6               0.55556     50      0.80736   0.61303\n  0.3   6         0.6               0.55556    100      0.81103   0.61892\n  0.3   6         0.6               0.55556    150      0.80080   0.59904\n  0.3   6         0.6               0.55556    200      0.80080   0.59848\n  0.3   6         0.6               0.55556    250      0.80402   0.60541\n  0.3   6         0.6               0.55556    300      0.80402   0.60528\n  0.3   6         0.6               0.55556    350      0.79736   0.59064\n  0.3   6         0.6               0.55556    400      0.80080   0.59736\n  0.3   6         0.6               0.55556    450      0.79414   0.58495\n  0.3   6         0.6               0.55556    500      0.78724   0.57262\n  0.3   6         0.6               0.61111     50      0.78368   0.56425\n  0.3   6         0.6               0.61111    100      0.80069   0.59943\n  0.3   6         0.6               0.61111    150      0.79391   0.58575\n  0.3   6         0.6               0.61111    200      0.79046   0.57837\n  0.3   6         0.6               0.61111    250      0.79724   0.59272\n  0.3   6         0.6               0.61111    300      0.78724   0.57278\n  0.3   6         0.6               0.61111    350      0.78391   0.56629\n  0.3   6         0.6               0.61111    400      0.78057   0.55974\n  0.3   6         0.6               0.61111    450      0.78057   0.55974\n  0.3   6         0.6               0.61111    500      0.77724   0.55272\n  0.3   6         0.6               0.66667     50      0.79057   0.57889\n  0.3   6         0.6               0.66667    100      0.77713   0.55134\n  0.3   6         0.6               0.66667    150      0.79057   0.57884\n  0.3   6         0.6               0.66667    200      0.79046   0.57946\n  0.3   6         0.6               0.66667    250      0.78046   0.55987\n  0.3   6         0.6               0.66667    300      0.77713   0.55267\n  0.3   6         0.6               0.66667    350      0.77379   0.54612\n  0.3   6         0.6               0.66667    400      0.77379   0.54678\n  0.3   6         0.6               0.66667    450      0.77713   0.55309\n  0.3   6         0.6               0.66667    500      0.77713   0.55309\n  0.3   6         0.6               0.72222     50      0.80425   0.60556\n  0.3   6         0.6               0.72222    100      0.80092   0.59896\n  0.3   6         0.6               0.72222    150      0.80770   0.61207\n  0.3   6         0.6               0.72222    200      0.79414   0.58474\n  0.3   6         0.6               0.72222    250      0.79080   0.57832\n  0.3   6         0.6               0.72222    300      0.78747   0.57177\n  0.3   6         0.6               0.72222    350      0.79092   0.57821\n  0.3   6         0.6               0.72222    400      0.79080   0.57819\n  0.3   6         0.6               0.72222    450      0.79080   0.57819\n  0.3   6         0.6               0.72222    500      0.79080   0.57819\n  0.3   6         0.6               0.77778     50      0.80770   0.61240\n  0.3   6         0.6               0.77778    100      0.78414   0.56488\n  0.3   6         0.6               0.77778    150      0.78414   0.56454\n  0.3   6         0.6               0.77778    200      0.78069   0.55723\n  0.3   6         0.6               0.77778    250      0.77736   0.55063\n  0.3   6         0.6               0.77778    300      0.78069   0.55723\n  0.3   6         0.6               0.77778    350      0.78069   0.55723\n  0.3   6         0.6               0.77778    400      0.78069   0.55760\n  0.3   6         0.6               0.77778    450      0.77402   0.54439\n  0.3   6         0.6               0.77778    500      0.77736   0.55105\n  0.3   6         0.6               0.83333     50      0.78759   0.57204\n  0.3   6         0.6               0.83333    100      0.80080   0.59914\n  0.3   6         0.6               0.83333    150      0.79080   0.57885\n  0.3   6         0.6               0.83333    200      0.78069   0.55959\n  0.3   6         0.6               0.83333    250      0.77736   0.55214\n  0.3   6         0.6               0.83333    300      0.78069   0.55917\n  0.3   6         0.6               0.83333    350      0.78069   0.55917\n  0.3   6         0.6               0.83333    400      0.77724   0.55195\n  0.3   6         0.6               0.83333    450      0.77724   0.55195\n  0.3   6         0.6               0.83333    500      0.78069   0.55916\n  0.3   6         0.6               0.88889     50      0.80069   0.59957\n  0.3   6         0.6               0.88889    100      0.79414   0.58649\n  0.3   6         0.6               0.88889    150      0.78736   0.57133\n  0.3   6         0.6               0.88889    200      0.78402   0.56479\n  0.3   6         0.6               0.88889    250      0.78057   0.55759\n  0.3   6         0.6               0.88889    300      0.78402   0.56479\n  0.3   6         0.6               0.88889    350      0.78402   0.56479\n  0.3   6         0.6               0.88889    400      0.78391   0.56419\n  0.3   6         0.6               0.88889    450      0.78057   0.55759\n  0.3   6         0.6               0.88889    500      0.77724   0.55122\n  0.3   6         0.6               0.94444     50      0.79747   0.59153\n  0.3   6         0.6               0.94444    100      0.78391   0.56388\n  0.3   6         0.6               0.94444    150      0.78057   0.55788\n  0.3   6         0.6               0.94444    200      0.77391   0.54419\n  0.3   6         0.6               0.94444    250      0.78736   0.57208\n  0.3   6         0.6               0.94444    300      0.78736   0.57208\n  0.3   6         0.6               0.94444    350      0.78736   0.57208\n  0.3   6         0.6               0.94444    400      0.79069   0.57857\n  0.3   6         0.6               0.94444    450      0.78736   0.57148\n  0.3   6         0.6               0.94444    500      0.78736   0.57148\n  0.3   6         0.6               1.00000     50      0.80092   0.59843\n  0.3   6         0.6               1.00000    100      0.79759   0.59110\n  0.3   6         0.6               1.00000    150      0.79759   0.59110\n  0.3   6         0.6               1.00000    200      0.79414   0.58388\n  0.3   6         0.6               1.00000    250      0.79092   0.57795\n  0.3   6         0.6               1.00000    300      0.78057   0.55777\n  0.3   6         0.6               1.00000    350      0.78724   0.57092\n  0.3   6         0.6               1.00000    400      0.78724   0.57128\n  0.3   6         0.6               1.00000    450      0.78724   0.57128\n  0.3   6         0.6               1.00000    500      0.78724   0.57128\n  0.3   6         0.8               0.50000     50      0.81414   0.62734\n  0.3   6         0.8               0.50000    100      0.79057   0.57744\n  0.3   6         0.8               0.50000    150      0.80069   0.59818\n  0.3   6         0.8               0.50000    200      0.79069   0.57764\n  0.3   6         0.8               0.50000    250      0.79391   0.58485\n  0.3   6         0.8               0.50000    300      0.79402   0.58468\n  0.3   6         0.8               0.50000    350      0.79724   0.59087\n  0.3   6         0.8               0.50000    400      0.79736   0.59171\n  0.3   6         0.8               0.50000    450      0.79402   0.58529\n  0.3   6         0.8               0.50000    500      0.78724   0.57224\n  0.3   6         0.8               0.55556     50      0.79103   0.58098\n  0.3   6         0.8               0.55556    100      0.78092   0.56154\n  0.3   6         0.8               0.55556    150      0.77092   0.54052\n  0.3   6         0.8               0.55556    200      0.77770   0.55383\n  0.3   6         0.8               0.55556    250      0.78092   0.55980\n  0.3   6         0.8               0.55556    300      0.78425   0.56694\n  0.3   6         0.8               0.55556    350      0.77425   0.54671\n  0.3   6         0.8               0.55556    400      0.78092   0.55981\n  0.3   6         0.8               0.55556    450      0.78092   0.55981\n  0.3   6         0.8               0.55556    500      0.77759   0.55320\n  0.3   6         0.8               0.61111     50      0.80115   0.59928\n  0.3   6         0.8               0.61111    100      0.81115   0.61836\n  0.3   6         0.8               0.61111    150      0.80092   0.59805\n  0.3   6         0.8               0.61111    200      0.80770   0.61268\n  0.3   6         0.8               0.61111    250      0.81103   0.62033\n  0.3   6         0.8               0.61111    300      0.81103   0.62028\n  0.3   6         0.8               0.61111    350      0.80425   0.60677\n  0.3   6         0.8               0.61111    400      0.80115   0.60096\n  0.3   6         0.8               0.61111    450      0.80103   0.60046\n  0.3   6         0.8               0.61111    500      0.79448   0.58734\n  0.3   6         0.8               0.66667     50      0.79414   0.58526\n  0.3   6         0.8               0.66667    100      0.79759   0.59333\n  0.3   6         0.8               0.66667    150      0.78414   0.56661\n  0.3   6         0.8               0.66667    200      0.79402   0.58577\n  0.3   6         0.8               0.66667    250      0.79391   0.58546\n  0.3   6         0.8               0.66667    300      0.78736   0.57187\n  0.3   6         0.8               0.66667    350      0.77713   0.55289\n  0.3   6         0.8               0.66667    400      0.78057   0.55933\n  0.3   6         0.8               0.66667    450      0.78057   0.55902\n  0.3   6         0.8               0.66667    500      0.78057   0.55873\n  0.3   6         0.8               0.72222     50      0.82115   0.63996\n  0.3   6         0.8               0.72222    100      0.79736   0.59202\n  0.3   6         0.8               0.72222    150      0.79057   0.57916\n  0.3   6         0.8               0.72222    200      0.79736   0.59328\n  0.3   6         0.8               0.72222    250      0.80080   0.59962\n  0.3   6         0.8               0.72222    300      0.79736   0.59250\n  0.3   6         0.8               0.72222    350      0.79736   0.59328\n  0.3   6         0.8               0.72222    400      0.79736   0.59328\n  0.3   6         0.8               0.72222    450      0.79736   0.59328\n  0.3   6         0.8               0.72222    500      0.79736   0.59357\n  0.3   6         0.8               0.77778     50      0.81103   0.61900\n  0.3   6         0.8               0.77778    100      0.78759   0.57152\n  0.3   6         0.8               0.77778    150      0.79069   0.57929\n  0.3   6         0.8               0.77778    200      0.79080   0.57897\n  0.3   6         0.8               0.77778    250      0.79069   0.57905\n  0.3   6         0.8               0.77778    300      0.78425   0.56659\n  0.3   6         0.8               0.77778    350      0.79092   0.58011\n  0.3   6         0.8               0.77778    400      0.78759   0.57345\n  0.3   6         0.8               0.77778    450      0.79092   0.58047\n  0.3   6         0.8               0.77778    500      0.78759   0.57345\n  0.3   6         0.8               0.83333     50      0.79425   0.58661\n  0.3   6         0.8               0.83333    100      0.78069   0.55814\n  0.3   6         0.8               0.83333    150      0.78402   0.56541\n  0.3   6         0.8               0.83333    200      0.78069   0.55863\n  0.3   6         0.8               0.83333    250      0.77724   0.55159\n  0.3   6         0.8               0.83333    300      0.77724   0.55160\n  0.3   6         0.8               0.83333    350      0.77391   0.54475\n  0.3   6         0.8               0.83333    400      0.77391   0.54475\n  0.3   6         0.8               0.83333    450      0.77736   0.55196\n  0.3   6         0.8               0.83333    500      0.77736   0.55196\n  0.3   6         0.8               0.88889     50      0.79770   0.59243\n  0.3   6         0.8               0.88889    100      0.79770   0.59388\n  0.3   6         0.8               0.88889    150      0.79092   0.57922\n  0.3   6         0.8               0.88889    200      0.79437   0.58589\n  0.3   6         0.8               0.88889    250      0.79103   0.57868\n  0.3   6         0.8               0.88889    300      0.78425   0.56506\n  0.3   6         0.8               0.88889    350      0.79092   0.57857\n  0.3   6         0.8               0.88889    400      0.78092   0.55839\n  0.3   6         0.8               0.88889    450      0.78425   0.56548\n  0.3   6         0.8               0.88889    500      0.78092   0.55839\n  0.3   6         0.8               0.94444     50      0.79736   0.59183\n  0.3   6         0.8               0.94444    100      0.79414   0.58536\n  0.3   6         0.8               0.94444    150      0.78402   0.56547\n  0.3   6         0.8               0.94444    200      0.79069   0.57904\n  0.3   6         0.8               0.94444    250      0.79402   0.58559\n  0.3   6         0.8               0.94444    300      0.79747   0.59279\n  0.3   6         0.8               0.94444    350      0.79414   0.58619\n  0.3   6         0.8               0.94444    400      0.79080   0.57952\n  0.3   6         0.8               0.94444    450      0.79080   0.57952\n  0.3   6         0.8               0.94444    500      0.79080   0.57952\n  0.3   6         0.8               1.00000     50      0.79759   0.59325\n  0.3   6         0.8               1.00000    100      0.80437   0.60612\n  0.3   6         0.8               1.00000    150      0.79747   0.59196\n  0.3   6         0.8               1.00000    200      0.80080   0.59856\n  0.3   6         0.8               1.00000    250      0.80414   0.60547\n  0.3   6         0.8               1.00000    300      0.79747   0.59213\n  0.3   6         0.8               1.00000    350      0.79759   0.59259\n  0.3   6         0.8               1.00000    400      0.80092   0.59950\n  0.3   6         0.8               1.00000    450      0.80092   0.59920\n  0.3   6         0.8               1.00000    500      0.80092   0.59920\n  0.3   7         0.6               0.50000     50      0.80759   0.61281\n  0.3   7         0.6               0.50000    100      0.79391   0.58504\n  0.3   7         0.6               0.50000    150      0.81092   0.61927\n  0.3   7         0.6               0.50000    200      0.81092   0.62011\n  0.3   7         0.6               0.50000    250      0.81080   0.61943\n  0.3   7         0.6               0.50000    300      0.80080   0.59904\n  0.3   7         0.6               0.50000    350      0.79069   0.57878\n  0.3   7         0.6               0.50000    400      0.79770   0.59190\n  0.3   7         0.6               0.50000    450      0.79080   0.57772\n  0.3   7         0.6               0.50000    500      0.79092   0.57845\n  0.3   7         0.6               0.55556     50      0.80437   0.60685\n  0.3   7         0.6               0.55556    100      0.79425   0.58561\n  0.3   7         0.6               0.55556    150      0.80092   0.59845\n  0.3   7         0.6               0.55556    200      0.79092   0.57821\n  0.3   7         0.6               0.55556    250      0.79414   0.58423\n  0.3   7         0.6               0.55556    300      0.78747   0.57090\n  0.3   7         0.6               0.55556    350      0.78747   0.57164\n  0.3   7         0.6               0.55556    400      0.78080   0.55843\n  0.3   7         0.6               0.55556    450      0.78080   0.55831\n  0.3   7         0.6               0.55556    500      0.77747   0.55146\n  0.3   7         0.6               0.61111     50      0.81437   0.62569\n  0.3   7         0.6               0.61111    100      0.80759   0.61287\n  0.3   7         0.6               0.61111    150      0.80080   0.59880\n  0.3   7         0.6               0.61111    200      0.79747   0.59262\n  0.3   7         0.6               0.61111    250      0.79069   0.57867\n  0.3   7         0.6               0.61111    300      0.79736   0.59243\n  0.3   7         0.6               0.61111    350      0.79736   0.59243\n  0.3   7         0.6               0.61111    400      0.79402   0.58588\n  0.3   7         0.6               0.61111    450      0.79402   0.58588\n  0.3   7         0.6               0.61111    500      0.79069   0.57933\n  0.3   7         0.6               0.66667     50      0.78448   0.56440\n  0.3   7         0.6               0.66667    100      0.78736   0.57158\n  0.3   7         0.6               0.66667    150      0.79057   0.57801\n  0.3   7         0.6               0.66667    200      0.78724   0.57194\n  0.3   7         0.6               0.66667    250      0.78391   0.56533\n  0.3   7         0.6               0.66667    300      0.78391   0.56503\n  0.3   7         0.6               0.66667    350      0.77713   0.55208\n  0.3   7         0.6               0.66667    400      0.78069   0.55862\n  0.3   7         0.6               0.66667    450      0.78057   0.55970\n  0.3   7         0.6               0.66667    500      0.78069   0.55944\n  0.3   7         0.6               0.72222     50      0.80069   0.59813\n  0.3   7         0.6               0.72222    100      0.80092   0.59779\n  0.3   7         0.6               0.72222    150      0.80103   0.59814\n  0.3   7         0.6               0.72222    200      0.79092   0.57806\n  0.3   7         0.6               0.72222    250      0.80437   0.60505\n  0.3   7         0.6               0.72222    300      0.80092   0.59843\n  0.3   7         0.6               0.72222    350      0.79747   0.59110\n  0.3   7         0.6               0.72222    400      0.79092   0.57778\n  0.3   7         0.6               0.72222    450      0.79414   0.58431\n  0.3   7         0.6               0.72222    500      0.78736   0.57111\n  0.3   7         0.6               0.77778     50      0.82115   0.63961\n  0.3   7         0.6               0.77778    100      0.81770   0.63294\n  0.3   7         0.6               0.77778    150      0.81770   0.63267\n  0.3   7         0.6               0.77778    200      0.81080   0.61907\n  0.3   7         0.6               0.77778    250      0.81080   0.61907\n  0.3   7         0.6               0.77778    300      0.80414   0.60604\n  0.3   7         0.6               0.77778    350      0.80080   0.59920\n  0.3   7         0.6               0.77778    400      0.80759   0.61270\n  0.3   7         0.6               0.77778    450      0.80092   0.59986\n  0.3   7         0.6               0.77778    500      0.79414   0.58640\n  0.3   7         0.6               0.83333     50      0.80759   0.61296\n  0.3   7         0.6               0.83333    100      0.80425   0.60641\n  0.3   7         0.6               0.83333    150      0.79736   0.59259\n  0.3   7         0.6               0.83333    200      0.81069   0.61991\n  0.3   7         0.6               0.83333    250      0.79736   0.59295\n  0.3   7         0.6               0.83333    300      0.79391   0.58611\n  0.3   7         0.6               0.83333    350      0.79724   0.59271\n  0.3   7         0.6               0.83333    400      0.78402   0.56642\n  0.3   7         0.6               0.83333    450      0.79057   0.57962\n  0.3   7         0.6               0.83333    500      0.79057   0.57967\n  0.3   7         0.6               0.88889     50      0.79046   0.57815\n  0.3   7         0.6               0.88889    100      0.79724   0.59170\n  0.3   7         0.6               0.88889    150      0.79391   0.58474\n  0.3   7         0.6               0.88889    200      0.79057   0.57770\n  0.3   7         0.6               0.88889    250      0.78724   0.57188\n  0.3   7         0.6               0.88889    300      0.78724   0.57128\n  0.3   7         0.6               0.88889    350      0.78736   0.57087\n  0.3   7         0.6               0.88889    400      0.78736   0.57165\n  0.3   7         0.6               0.88889    450      0.79080   0.57886\n  0.3   7         0.6               0.88889    500      0.79080   0.57886\n  0.3   7         0.6               0.94444     50      0.82115   0.64003\n  0.3   7         0.6               0.94444    100      0.81115   0.61931\n  0.3   7         0.6               0.94444    150      0.80448   0.60604\n  0.3   7         0.6               0.94444    200      0.80448   0.60669\n  0.3   7         0.6               0.94444    250      0.80793   0.61322\n  0.3   7         0.6               0.94444    300      0.80103   0.60016\n  0.3   7         0.6               0.94444    350      0.80115   0.59991\n  0.3   7         0.6               0.94444    400      0.80460   0.60703\n  0.3   7         0.6               0.94444    450      0.80115   0.59991\n  0.3   7         0.6               0.94444    500      0.80460   0.60703\n  0.3   7         0.6               1.00000     50      0.78391   0.56551\n  0.3   7         0.6               1.00000    100      0.78057   0.55862\n  0.3   7         0.6               1.00000    150      0.78057   0.55862\n  0.3   7         0.6               1.00000    200      0.77713   0.55140\n  0.3   7         0.6               1.00000    250      0.78046   0.55849\n  0.3   7         0.6               1.00000    300      0.78402   0.56616\n  0.3   7         0.6               1.00000    350      0.78057   0.55905\n  0.3   7         0.6               1.00000    400      0.78402   0.56616\n  0.3   7         0.6               1.00000    450      0.77724   0.55196\n  0.3   7         0.6               1.00000    500      0.78069   0.55932\n  0.3   7         0.8               0.50000     50      0.81092   0.61843\n  0.3   7         0.8               0.50000    100      0.78402   0.56514\n  0.3   7         0.8               0.50000    150      0.77069   0.53766\n  0.3   7         0.8               0.50000    200      0.76391   0.52510\n  0.3   7         0.8               0.50000    250      0.77069   0.53874\n  0.3   7         0.8               0.50000    300      0.78080   0.55925\n  0.3   7         0.8               0.50000    350      0.77069   0.53760\n  0.3   7         0.8               0.50000    400      0.77414   0.54547\n  0.3   7         0.8               0.50000    450      0.77080   0.53934\n  0.3   7         0.8               0.50000    500      0.77747   0.55309\n  0.3   7         0.8               0.55556     50      0.80437   0.60652\n  0.3   7         0.8               0.55556    100      0.80115   0.59944\n  0.3   7         0.8               0.55556    150      0.80092   0.59999\n  0.3   7         0.8               0.55556    200      0.80092   0.59993\n  0.3   7         0.8               0.55556    250      0.79759   0.59302\n  0.3   7         0.8               0.55556    300      0.79092   0.57920\n  0.3   7         0.8               0.55556    350      0.79759   0.59284\n  0.3   7         0.8               0.55556    400      0.78414   0.56634\n  0.3   7         0.8               0.55556    450      0.78414   0.56652\n  0.3   7         0.8               0.55556    500      0.78414   0.56706\n  0.3   7         0.8               0.61111     50      0.79747   0.59194\n  0.3   7         0.8               0.61111    100      0.79736   0.59254\n  0.3   7         0.8               0.61111    150      0.78759   0.57204\n  0.3   7         0.8               0.61111    200      0.78747   0.57212\n  0.3   7         0.8               0.61111    250      0.78069   0.55906\n  0.3   7         0.8               0.61111    300      0.79402   0.58673\n  0.3   7         0.8               0.61111    350      0.78402   0.56575\n  0.3   7         0.8               0.61111    400      0.79080   0.57923\n  0.3   7         0.8               0.61111    450      0.78747   0.57214\n  0.3   7         0.8               0.61111    500      0.78414   0.56535\n  0.3   7         0.8               0.66667     50      0.78402   0.56595\n  0.3   7         0.8               0.66667    100      0.78069   0.55923\n  0.3   7         0.8               0.66667    150      0.79057   0.58007\n  0.3   7         0.8               0.66667    200      0.79080   0.58088\n  0.3   7         0.8               0.66667    250      0.78747   0.57317\n  0.3   7         0.8               0.66667    300      0.79437   0.58700\n  0.3   7         0.8               0.66667    350      0.78747   0.57326\n  0.3   7         0.8               0.66667    400      0.78069   0.55964\n  0.3   7         0.8               0.66667    450      0.78069   0.55964\n  0.3   7         0.8               0.66667    500      0.78069   0.55964\n  0.3   7         0.8               0.72222     50      0.80069   0.59938\n  0.3   7         0.8               0.72222    100      0.79402   0.58605\n  0.3   7         0.8               0.72222    150      0.80069   0.60038\n  0.3   7         0.8               0.72222    200      0.80736   0.61383\n  0.3   7         0.8               0.72222    250      0.79724   0.59442\n  0.3   7         0.8               0.72222    300      0.79057   0.58062\n  0.3   7         0.8               0.72222    350      0.79724   0.59442\n  0.3   7         0.8               0.72222    400      0.79046   0.58036\n  0.3   7         0.8               0.72222    450      0.79391   0.58758\n  0.3   7         0.8               0.72222    500      0.79391   0.58758\n  0.3   7         0.8               0.77778     50      0.79414   0.58378\n  0.3   7         0.8               0.77778    100      0.79080   0.57730\n  0.3   7         0.8               0.77778    150      0.79092   0.57854\n  0.3   7         0.8               0.77778    200      0.79770   0.59267\n  0.3   7         0.8               0.77778    250      0.79437   0.58576\n  0.3   7         0.8               0.77778    300      0.79103   0.57969\n  0.3   7         0.8               0.77778    350      0.78770   0.57315\n  0.3   7         0.8               0.77778    400      0.79437   0.58624\n  0.3   7         0.8               0.77778    450      0.79103   0.57969\n  0.3   7         0.8               0.77778    500      0.78770   0.57261\n  0.3   7         0.8               0.83333     50      0.80782   0.61238\n  0.3   7         0.8               0.83333    100      0.80759   0.61176\n  0.3   7         0.8               0.83333    150      0.81103   0.61888\n  0.3   7         0.8               0.83333    200      0.80080   0.59861\n  0.3   7         0.8               0.83333    250      0.79391   0.58467\n  0.3   7         0.8               0.83333    300      0.79402   0.58562\n  0.3   7         0.8               0.83333    350      0.78724   0.57146\n  0.3   7         0.8               0.83333    400      0.78724   0.57146\n  0.3   7         0.8               0.83333    450      0.78724   0.57146\n  0.3   7         0.8               0.83333    500      0.78391   0.56521\n  0.3   7         0.8               0.88889     50      0.80402   0.60524\n  0.3   7         0.8               0.88889    100      0.81092   0.61810\n  0.3   7         0.8               0.88889    150      0.79724   0.59195\n  0.3   7         0.8               0.88889    200      0.79724   0.59195\n  0.3   7         0.8               0.88889    250      0.79391   0.58492\n  0.3   7         0.8               0.88889    300      0.79057   0.57843\n  0.3   7         0.8               0.88889    350      0.79402   0.58594\n  0.3   7         0.8               0.88889    400      0.78724   0.57231\n  0.3   7         0.8               0.88889    450      0.79069   0.57952\n  0.3   7         0.8               0.88889    500      0.79069   0.57952\n  0.3   7         0.8               0.94444     50      0.80759   0.61284\n  0.3   7         0.8               0.94444    100      0.80402   0.60608\n  0.3   7         0.8               0.94444    150      0.79747   0.59284\n  0.3   7         0.8               0.94444    200      0.79747   0.59320\n  0.3   7         0.8               0.94444    250      0.79414   0.58642\n  0.3   7         0.8               0.94444    300      0.79069   0.58028\n  0.3   7         0.8               0.94444    350      0.79069   0.58028\n  0.3   7         0.8               0.94444    400      0.79069   0.58028\n  0.3   7         0.8               0.94444    450      0.78402   0.56706\n  0.3   7         0.8               0.94444    500      0.78069   0.56075\n  0.3   7         0.8               1.00000     50      0.79759   0.59221\n  0.3   7         0.8               1.00000    100      0.79080   0.57980\n  0.3   7         0.8               1.00000    150      0.79425   0.58623\n  0.3   7         0.8               1.00000    200      0.79425   0.58647\n  0.3   7         0.8               1.00000    250      0.79425   0.58647\n  0.3   7         0.8               1.00000    300      0.79092   0.57968\n  0.3   7         0.8               1.00000    350      0.79425   0.58647\n  0.3   7         0.8               1.00000    400      0.79759   0.59284\n  0.3   7         0.8               1.00000    450      0.79747   0.59263\n  0.3   7         0.8               1.00000    500      0.79759   0.59308\n  0.3   8         0.6               0.50000     50      0.81425   0.62514\n  0.3   8         0.6               0.50000    100      0.80080   0.59802\n  0.3   8         0.6               0.50000    150      0.79736   0.59176\n  0.3   8         0.6               0.50000    200      0.79069   0.57978\n  0.3   8         0.6               0.50000    250      0.79402   0.58609\n  0.3   8         0.6               0.50000    300      0.77736   0.55269\n  0.3   8         0.6               0.50000    350      0.78425   0.56555\n  0.3   8         0.6               0.50000    400      0.77391   0.54620\n  0.3   8         0.6               0.50000    450      0.78069   0.55935\n  0.3   8         0.6               0.50000    500      0.78057   0.55990\n  0.3   8         0.6               0.55556     50      0.81092   0.61957\n  0.3   8         0.6               0.55556    100      0.79736   0.59241\n  0.3   8         0.6               0.55556    150      0.79402   0.58562\n  0.3   8         0.6               0.55556    200      0.79080   0.57980\n  0.3   8         0.6               0.55556    250      0.78747   0.57314\n  0.3   8         0.6               0.55556    300      0.78402   0.56583\n  0.3   8         0.6               0.55556    350      0.78402   0.56583\n  0.3   8         0.6               0.55556    400      0.78736   0.57226\n  0.3   8         0.6               0.55556    450      0.78747   0.57314\n  0.3   8         0.6               0.55556    500      0.78736   0.57250\n  0.3   8         0.6               0.61111     50      0.79425   0.58401\n  0.3   8         0.6               0.61111    100      0.77713   0.55205\n  0.3   8         0.6               0.61111    150      0.77724   0.55182\n  0.3   8         0.6               0.61111    200      0.76701   0.53038\n  0.3   8         0.6               0.61111    250      0.76713   0.53062\n  0.3   8         0.6               0.61111    300      0.76379   0.52407\n  0.3   8         0.6               0.61111    350      0.76379   0.52431\n  0.3   8         0.6               0.61111    400      0.77046   0.53734\n  0.3   8         0.6               0.61111    450      0.77046   0.53734\n  0.3   8         0.6               0.61111    500      0.77046   0.53734\n  0.3   8         0.6               0.66667     50      0.82770   0.65221\n  0.3   8         0.6               0.66667    100      0.82770   0.65300\n  0.3   8         0.6               0.66667    150      0.81770   0.63326\n  0.3   8         0.6               0.66667    200      0.81103   0.61969\n  0.3   8         0.6               0.66667    250      0.80759   0.61337\n  0.3   8         0.6               0.66667    300      0.80425   0.60671\n  0.3   8         0.6               0.66667    350      0.80425   0.60671\n  0.3   8         0.6               0.66667    400      0.80092   0.60011\n  0.3   8         0.6               0.66667    450      0.80770   0.61324\n  0.3   8         0.6               0.66667    500      0.80414   0.60626\n  0.3   8         0.6               0.72222     50      0.79770   0.59255\n  0.3   8         0.6               0.72222    100      0.77391   0.54447\n  0.3   8         0.6               0.72222    150      0.78057   0.55823\n  0.3   8         0.6               0.72222    200      0.78069   0.55855\n  0.3   8         0.6               0.72222    250      0.78080   0.55887\n  0.3   8         0.6               0.72222    300      0.78069   0.55922\n  0.3   8         0.6               0.72222    350      0.78057   0.55860\n  0.3   8         0.6               0.72222    400      0.77713   0.55267\n  0.3   8         0.6               0.72222    450      0.77713   0.55225\n  0.3   8         0.6               0.72222    500      0.77724   0.55305\n  0.3   8         0.6               0.77778     50      0.78724   0.57179\n  0.3   8         0.6               0.77778    100      0.78713   0.57157\n  0.3   8         0.6               0.77778    150      0.77701   0.55049\n  0.3   8         0.6               0.77778    200      0.78724   0.57234\n  0.3   8         0.6               0.77778    250      0.78034   0.55770\n  0.3   8         0.6               0.77778    300      0.76356   0.52444\n  0.3   8         0.6               0.77778    350      0.76701   0.53250\n  0.3   8         0.6               0.77778    400      0.76701   0.53250\n  0.3   8         0.6               0.77778    450      0.76701   0.53250\n  0.3   8         0.6               0.77778    500      0.77046   0.54034\n  0.3   8         0.6               0.83333     50      0.80736   0.61291\n  0.3   8         0.6               0.83333    100      0.81414   0.62602\n  0.3   8         0.6               0.83333    150      0.80402   0.60547\n  0.3   8         0.6               0.83333    200      0.80069   0.59832\n  0.3   8         0.6               0.83333    250      0.79402   0.58456\n  0.3   8         0.6               0.83333    300      0.79057   0.57861\n  0.3   8         0.6               0.83333    350      0.79402   0.58505\n  0.3   8         0.6               0.83333    400      0.78736   0.57237\n  0.3   8         0.6               0.83333    450      0.78402   0.56546\n  0.3   8         0.6               0.83333    500      0.78736   0.57219\n  0.3   8         0.6               0.88889     50      0.79736   0.59402\n  0.3   8         0.6               0.88889    100      0.80414   0.60730\n  0.3   8         0.6               0.88889    150      0.80092   0.60020\n  0.3   8         0.6               0.88889    200      0.80437   0.60741\n  0.3   8         0.6               0.88889    250      0.79759   0.59327\n  0.3   8         0.6               0.88889    300      0.79425   0.58672\n  0.3   8         0.6               0.88889    350      0.79759   0.59363\n  0.3   8         0.6               0.88889    400      0.79759   0.59363\n  0.3   8         0.6               0.88889    450      0.79759   0.59363\n  0.3   8         0.6               0.88889    500      0.79425   0.58703\n  0.3   8         0.6               0.94444     50      0.80057   0.59916\n  0.3   8         0.6               0.94444    100      0.79724   0.59273\n  0.3   8         0.6               0.94444    150      0.79724   0.59255\n  0.3   8         0.6               0.94444    200      0.79391   0.58594\n  0.3   8         0.6               0.94444    250      0.79391   0.58594\n  0.3   8         0.6               0.94444    300      0.79391   0.58594\n  0.3   8         0.6               0.94444    350      0.79057   0.57928\n  0.3   8         0.6               0.94444    400      0.79057   0.57928\n  0.3   8         0.6               0.94444    450      0.79057   0.57928\n  0.3   8         0.6               0.94444    500      0.79057   0.57928\n  0.3   8         0.6               1.00000     50      0.81115   0.61807\n  0.3   8         0.6               1.00000    100      0.81782   0.63293\n  0.3   8         0.6               1.00000    150      0.81103   0.61875\n  0.3   8         0.6               1.00000    200      0.80092   0.59883\n  0.3   8         0.6               1.00000    250      0.80425   0.60538\n  0.3   8         0.6               1.00000    300      0.79759   0.59241\n  0.3   8         0.6               1.00000    350      0.79759   0.59241\n  0.3   8         0.6               1.00000    400      0.79425   0.58538\n  0.3   8         0.6               1.00000    450      0.79425   0.58538\n  0.3   8         0.6               1.00000    500      0.80092   0.59920\n  0.3   8         0.8               0.50000     50      0.80747   0.61155\n  0.3   8         0.8               0.50000    100      0.79046   0.57766\n  0.3   8         0.8               0.50000    150      0.78402   0.56524\n  0.3   8         0.8               0.50000    200      0.79414   0.58637\n  0.3   8         0.8               0.50000    250      0.78414   0.56582\n  0.3   8         0.8               0.50000    300      0.79425   0.58569\n  0.3   8         0.8               0.50000    350      0.78747   0.57274\n  0.3   8         0.8               0.50000    400      0.78747   0.57231\n  0.3   8         0.8               0.50000    450      0.79069   0.57887\n  0.3   8         0.8               0.50000    500      0.78069   0.55881\n  0.3   8         0.8               0.55556     50      0.79414   0.58422\n  0.3   8         0.8               0.55556    100      0.79069   0.57723\n  0.3   8         0.8               0.55556    150      0.79391   0.58504\n  0.3   8         0.8               0.55556    200      0.78747   0.57253\n  0.3   8         0.8               0.55556    250      0.79425   0.58639\n  0.3   8         0.8               0.55556    300      0.78747   0.57211\n  0.3   8         0.8               0.55556    350      0.78402   0.56499\n  0.3   8         0.8               0.55556    400      0.78402   0.56541\n  0.3   8         0.8               0.55556    450      0.77391   0.54474\n  0.3   8         0.8               0.55556    500      0.77391   0.54516\n  0.3   8         0.8               0.61111     50      0.80437   0.60677\n  0.3   8         0.8               0.61111    100      0.79425   0.58562\n  0.3   8         0.8               0.61111    150      0.80092   0.59901\n  0.3   8         0.8               0.61111    200      0.79759   0.59283\n  0.3   8         0.8               0.61111    250      0.79425   0.58670\n  0.3   8         0.8               0.61111    300      0.79425   0.58712\n  0.3   8         0.8               0.61111    350      0.78759   0.57360\n  0.3   8         0.8               0.61111    400      0.78092   0.55974\n  0.3   8         0.8               0.61111    450      0.78759   0.57367\n  0.3   8         0.8               0.61111    500      0.78759   0.57367\n  0.3   8         0.8               0.66667     50      0.79069   0.57873\n  0.3   8         0.8               0.66667    100      0.78736   0.57319\n  0.3   8         0.8               0.66667    150      0.78736   0.57249\n  0.3   8         0.8               0.66667    200      0.77391   0.54619\n  0.3   8         0.8               0.66667    250      0.78391   0.56577\n  0.3   8         0.8               0.66667    300      0.78402   0.56628\n  0.3   8         0.8               0.66667    350      0.78069   0.55997\n  0.3   8         0.8               0.66667    400      0.78069   0.55997\n  0.3   8         0.8               0.66667    450      0.77069   0.54009\n  0.3   8         0.8               0.66667    500      0.77069   0.54009\n  0.3   8         0.8               0.72222     50      0.81414   0.62524\n  0.3   8         0.8               0.72222    100      0.79402   0.58529\n  0.3   8         0.8               0.72222    150      0.79724   0.59146\n  0.3   8         0.8               0.72222    200      0.79046   0.57855\n  0.3   8         0.8               0.72222    250      0.79724   0.59225\n  0.3   8         0.8               0.72222    300      0.78724   0.57248\n  0.3   8         0.8               0.72222    350      0.78713   0.57212\n  0.3   8         0.8               0.72222    400      0.79391   0.58575\n  0.3   8         0.8               0.72222    450      0.79057   0.57933\n  0.3   8         0.8               0.72222    500      0.79057   0.57933\n  0.3   8         0.8               0.77778     50      0.80080   0.59857\n  0.3   8         0.8               0.77778    100      0.78402   0.56547\n  0.3   8         0.8               0.77778    150      0.78747   0.57259\n  0.3   8         0.8               0.77778    200      0.79080   0.57956\n  0.3   8         0.8               0.77778    250      0.78414   0.56598\n  0.3   8         0.8               0.77778    300      0.78736   0.57244\n  0.3   8         0.8               0.77778    350      0.78069   0.55911\n  0.3   8         0.8               0.77778    400      0.78069   0.55911\n  0.3   8         0.8               0.77778    450      0.78736   0.57244\n  0.3   8         0.8               0.77778    500      0.78402   0.56577\n  0.3   8         0.8               0.83333     50      0.81425   0.62616\n  0.3   8         0.8               0.83333    100      0.80069   0.60011\n  0.3   8         0.8               0.83333    150      0.79736   0.59299\n  0.3   8         0.8               0.83333    200      0.80414   0.60637\n  0.3   8         0.8               0.83333    250      0.79402   0.58633\n  0.3   8         0.8               0.83333    300      0.79414   0.58559\n  0.3   8         0.8               0.83333    350      0.79402   0.58633\n  0.3   8         0.8               0.83333    400      0.79080   0.57898\n  0.3   8         0.8               0.83333    450      0.79069   0.57972\n  0.3   8         0.8               0.83333    500      0.78736   0.57263\n  0.3   8         0.8               0.88889     50      0.79414   0.58699\n  0.3   8         0.8               0.88889    100      0.79080   0.57967\n  0.3   8         0.8               0.88889    150      0.79437   0.58609\n  0.3   8         0.8               0.88889    200      0.79425   0.58706\n  0.3   8         0.8               0.88889    250      0.79092   0.58040\n  0.3   8         0.8               0.88889    300      0.79437   0.58693\n  0.3   8         0.8               0.88889    350      0.79103   0.57997\n  0.3   8         0.8               0.88889    400      0.79092   0.57970\n  0.3   8         0.8               0.88889    450      0.79092   0.58016\n  0.3   8         0.8               0.88889    500      0.78759   0.57338\n  0.3   8         0.8               0.94444     50      0.79092   0.57954\n  0.3   8         0.8               0.94444    100      0.78759   0.57235\n  0.3   8         0.8               0.94444    150      0.77736   0.55318\n  0.3   8         0.8               0.94444    200      0.78069   0.55961\n  0.3   8         0.8               0.94444    250      0.77736   0.55264\n  0.3   8         0.8               0.94444    300      0.79080   0.57956\n  0.3   8         0.8               0.94444    350      0.78736   0.57307\n  0.3   8         0.8               0.94444    400      0.78747   0.57253\n  0.3   8         0.8               0.94444    450      0.78402   0.56670\n  0.3   8         0.8               0.94444    500      0.78414   0.56647\n  0.3   8         0.8               1.00000     50      0.78057   0.55982\n  0.3   8         0.8               1.00000    100      0.78414   0.56587\n  0.3   8         0.8               1.00000    150      0.78080   0.55944\n  0.3   8         0.8               1.00000    200      0.78414   0.56629\n  0.3   8         0.8               1.00000    250      0.78414   0.56629\n  0.3   8         0.8               1.00000    300      0.78747   0.57290\n  0.3   8         0.8               1.00000    350      0.78747   0.57290\n  0.3   8         0.8               1.00000    400      0.79080   0.57939\n  0.3   8         0.8               1.00000    450      0.79080   0.57939\n  0.3   8         0.8               1.00000    500      0.79080   0.57939\n  0.3   9         0.6               0.50000     50      0.79057   0.57734\n  0.3   9         0.6               0.50000    100      0.78368   0.56335\n  0.3   9         0.6               0.50000    150      0.77736   0.55129\n  0.3   9         0.6               0.50000    200      0.78402   0.56493\n  0.3   9         0.6               0.50000    250      0.78736   0.57177\n  0.3   9         0.6               0.50000    300      0.77391   0.54462\n  0.3   9         0.6               0.50000    350      0.77724   0.55127\n  0.3   9         0.6               0.50000    400      0.77724   0.55187\n  0.3   9         0.6               0.50000    450      0.78425   0.56559\n  0.3   9         0.6               0.50000    500      0.77736   0.55261\n  0.3   9         0.6               0.55556     50      0.80069   0.59780\n  0.3   9         0.6               0.55556    100      0.80391   0.60620\n  0.3   9         0.6               0.55556    150      0.77713   0.55109\n  0.3   9         0.6               0.55556    200      0.78724   0.57225\n  0.3   9         0.6               0.55556    250      0.78379   0.56583\n  0.3   9         0.6               0.55556    300      0.77713   0.55182\n  0.3   9         0.6               0.55556    350      0.78379   0.56583\n  0.3   9         0.6               0.55556    400      0.78046   0.55892\n  0.3   9         0.6               0.55556    450      0.78046   0.55892\n  0.3   9         0.6               0.55556    500      0.78046   0.55928\n  0.3   9         0.6               0.61111     50      0.81448   0.62626\n  0.3   9         0.6               0.61111    100      0.79092   0.57916\n  0.3   9         0.6               0.61111    150      0.79770   0.59338\n  0.3   9         0.6               0.61111    200      0.79759   0.59188\n  0.3   9         0.6               0.61111    250      0.80092   0.59848\n  0.3   9         0.6               0.61111    300      0.80770   0.61291\n  0.3   9         0.6               0.61111    350      0.79747   0.59236\n  0.3   9         0.6               0.61111    400      0.79425   0.58655\n  0.3   9         0.6               0.61111    450      0.80437   0.60553\n  0.3   9         0.6               0.61111    500      0.79759   0.59392\n  0.3   9         0.6               0.66667     50      0.80425   0.60563\n  0.3   9         0.6               0.66667    100      0.77713   0.55134\n  0.3   9         0.6               0.66667    150      0.78379   0.56497\n  0.3   9         0.6               0.66667    200      0.77713   0.55205\n  0.3   9         0.6               0.66667    250      0.77034   0.53782\n  0.3   9         0.6               0.66667    300      0.77034   0.53770\n  0.3   9         0.6               0.66667    350      0.77379   0.54413\n  0.3   9         0.6               0.66667    400      0.78057   0.55926\n  0.3   9         0.6               0.66667    450      0.78046   0.55891\n  0.3   9         0.6               0.66667    500      0.77713   0.55188\n  0.3   9         0.6               0.72222     50      0.79759   0.59300\n  0.3   9         0.6               0.72222    100      0.81115   0.61860\n  0.3   9         0.6               0.72222    150      0.80425   0.60616\n  0.3   9         0.6               0.72222    200      0.80080   0.59979\n  0.3   9         0.6               0.72222    250      0.80080   0.59973\n  0.3   9         0.6               0.72222    300      0.79080   0.57955\n  0.3   9         0.6               0.72222    350      0.78747   0.57300\n  0.3   9         0.6               0.72222    400      0.79759   0.59296\n  0.3   9         0.6               0.72222    450      0.79759   0.59296\n  0.3   9         0.6               0.72222    500      0.79759   0.59253\n  0.3   9         0.6               0.77778     50      0.79069   0.57869\n  0.3   9         0.6               0.77778    100      0.78057   0.55712\n  0.3   9         0.6               0.77778    150      0.77701   0.55109\n  0.3   9         0.6               0.77778    200      0.79034   0.57777\n  0.3   9         0.6               0.77778    250      0.78368   0.56437\n  0.3   9         0.6               0.77778    300      0.78713   0.57117\n  0.3   9         0.6               0.77778    350      0.79046   0.57849\n  0.3   9         0.6               0.77778    400      0.79046   0.57897\n  0.3   9         0.6               0.77778    450      0.78713   0.57152\n  0.3   9         0.6               0.77778    500      0.79379   0.58557\n  0.3   9         0.6               0.83333     50      0.80425   0.60572\n  0.3   9         0.6               0.83333    100      0.80092   0.59940\n  0.3   9         0.6               0.83333    150      0.79080   0.57918\n  0.3   9         0.6               0.83333    200      0.79759   0.59262\n  0.3   9         0.6               0.83333    250      0.79425   0.58571\n  0.3   9         0.6               0.83333    300      0.79080   0.57924\n  0.3   9         0.6               0.83333    350      0.79414   0.58608\n  0.3   9         0.6               0.83333    400      0.80092   0.60005\n  0.3   9         0.6               0.83333    450      0.80437   0.60658\n  0.3   9         0.6               0.83333    500      0.79425   0.58684\n  0.3   9         0.6               0.88889     50      0.79425   0.58682\n  0.3   9         0.6               0.88889    100      0.78402   0.56577\n  0.3   9         0.6               0.88889    150      0.78736   0.57268\n  0.3   9         0.6               0.88889    200      0.79069   0.57928\n  0.3   9         0.6               0.88889    250      0.78747   0.57340\n  0.3   9         0.6               0.88889    300      0.78736   0.57321\n  0.3   9         0.6               0.88889    350      0.78057   0.55985\n  0.3   9         0.6               0.88889    400      0.78736   0.57371\n  0.3   9         0.6               0.88889    450      0.78402   0.56687\n  0.3   9         0.6               0.88889    500      0.78402   0.56687\n  0.3   9         0.6               0.94444     50      0.79736   0.59160\n  0.3   9         0.6               0.94444    100      0.79402   0.58445\n  0.3   9         0.6               0.94444    150      0.80069   0.59833\n  0.3   9         0.6               0.94444    200      0.80069   0.59833\n  0.3   9         0.6               0.94444    250      0.80069   0.59833\n  0.3   9         0.6               0.94444    300      0.79736   0.59141\n  0.3   9         0.6               0.94444    350      0.79402   0.58487\n  0.3   9         0.6               0.94444    400      0.79736   0.59141\n  0.3   9         0.6               0.94444    450      0.79402   0.58487\n  0.3   9         0.6               0.94444    500      0.79069   0.57838\n  0.3   9         0.6               1.00000     50      0.78379   0.56513\n  0.3   9         0.6               1.00000    100      0.79069   0.57759\n  0.3   9         0.6               1.00000    150      0.78057   0.55710\n  0.3   9         0.6               1.00000    200      0.78724   0.57110\n  0.3   9         0.6               1.00000    250      0.78724   0.57110\n  0.3   9         0.6               1.00000    300      0.78057   0.55783\n  0.3   9         0.6               1.00000    350      0.78057   0.55765\n  0.3   9         0.6               1.00000    400      0.78391   0.56450\n  0.3   9         0.6               1.00000    450      0.78391   0.56401\n  0.3   9         0.6               1.00000    500      0.78057   0.55765\n  0.3   9         0.8               0.50000     50      0.79391   0.58435\n  0.3   9         0.8               0.50000    100      0.77690   0.55157\n  0.3   9         0.8               0.50000    150      0.78368   0.56462\n  0.3   9         0.8               0.50000    200      0.77701   0.55026\n  0.3   9         0.8               0.50000    250      0.78713   0.57134\n  0.3   9         0.8               0.50000    300      0.77379   0.54496\n  0.3   9         0.8               0.50000    350      0.77034   0.53807\n  0.3   9         0.8               0.50000    400      0.77713   0.55183\n  0.3   9         0.8               0.50000    450      0.77034   0.53771\n  0.3   9         0.8               0.50000    500      0.76379   0.52522\n  0.3   9         0.8               0.55556     50      0.79080   0.58016\n  0.3   9         0.8               0.55556    100      0.77736   0.55242\n  0.3   9         0.8               0.55556    150      0.78069   0.55890\n  0.3   9         0.8               0.55556    200      0.77747   0.55109\n  0.3   9         0.8               0.55556    250      0.77402   0.54490\n  0.3   9         0.8               0.55556    300      0.78402   0.56520\n  0.3   9         0.8               0.55556    350      0.77402   0.54478\n  0.3   9         0.8               0.55556    400      0.76736   0.53181\n  0.3   9         0.8               0.55556    450      0.77402   0.54489\n  0.3   9         0.8               0.55556    500      0.76069   0.51830\n  0.3   9         0.8               0.61111     50      0.81092   0.61858\n  0.3   9         0.8               0.61111    100      0.79092   0.57861\n  0.3   9         0.8               0.61111    150      0.79092   0.57831\n  0.3   9         0.8               0.61111    200      0.79747   0.59115\n  0.3   9         0.8               0.61111    250      0.79425   0.58540\n  0.3   9         0.8               0.61111    300      0.79425   0.58560\n  0.3   9         0.8               0.61111    350      0.80092   0.59868\n  0.3   9         0.8               0.61111    400      0.80425   0.60553\n  0.3   9         0.8               0.61111    450      0.80092   0.59868\n  0.3   9         0.8               0.61111    500      0.79759   0.59171\n  0.3   9         0.8               0.66667     50      0.80414   0.60584\n  0.3   9         0.8               0.66667    100      0.79736   0.59210\n  0.3   9         0.8               0.66667    150      0.80080   0.59858\n  0.3   9         0.8               0.66667    200      0.80414   0.60554\n  0.3   9         0.8               0.66667    250      0.79069   0.57823\n  0.3   9         0.8               0.66667    300      0.79747   0.59178\n  0.3   9         0.8               0.66667    350      0.79414   0.58494\n  0.3   9         0.8               0.66667    400      0.79747   0.59191\n  0.3   9         0.8               0.66667    450      0.79069   0.57841\n  0.3   9         0.8               0.66667    500      0.79747   0.59191\n  0.3   9         0.8               0.72222     50      0.79391   0.58601\n  0.3   9         0.8               0.72222    100      0.79724   0.59382\n  0.3   9         0.8               0.72222    150      0.79724   0.59330\n  0.3   9         0.8               0.72222    200      0.79724   0.59330\n  0.3   9         0.8               0.72222    250      0.79391   0.58644\n  0.3   9         0.8               0.72222    300      0.79402   0.58619\n  0.3   9         0.8               0.72222    350      0.79402   0.58667\n  0.3   9         0.8               0.72222    400      0.79402   0.58667\n  0.3   9         0.8               0.72222    450      0.79402   0.58667\n  0.3   9         0.8               0.72222    500      0.79402   0.58667\n  0.3   9         0.8               0.77778     50      0.80080   0.59862\n  0.3   9         0.8               0.77778    100      0.79080   0.57797\n  0.3   9         0.8               0.77778    150      0.79080   0.57797\n  0.3   9         0.8               0.77778    200      0.78402   0.56518\n  0.3   9         0.8               0.77778    250      0.78736   0.57154\n  0.3   9         0.8               0.77778    300      0.78069   0.55803\n  0.3   9         0.8               0.77778    350      0.78402   0.56494\n  0.3   9         0.8               0.77778    400      0.78402   0.56505\n  0.3   9         0.8               0.77778    450      0.78057   0.55783\n  0.3   9         0.8               0.77778    500      0.78414   0.56498\n  0.3   9         0.8               0.83333     50      0.80782   0.61259\n  0.3   9         0.8               0.83333    100      0.79092   0.57980\n  0.3   9         0.8               0.83333    150      0.78414   0.56690\n  0.3   9         0.8               0.83333    200      0.79080   0.57999\n  0.3   9         0.8               0.83333    250      0.78069   0.55958\n  0.3   9         0.8               0.83333    300      0.78736   0.57279\n  0.3   9         0.8               0.83333    350      0.78402   0.56613\n  0.3   9         0.8               0.83333    400      0.78069   0.55958\n  0.3   9         0.8               0.83333    450      0.78069   0.55958\n  0.3   9         0.8               0.83333    500      0.78069   0.55958\n  0.3   9         0.8               0.88889     50      0.80080   0.59814\n  0.3   9         0.8               0.88889    100      0.79402   0.58553\n  0.3   9         0.8               0.88889    150      0.79402   0.58546\n  0.3   9         0.8               0.88889    200      0.78736   0.57117\n  0.3   9         0.8               0.88889    250      0.79736   0.59201\n  0.3   9         0.8               0.88889    300      0.79402   0.58558\n  0.3   9         0.8               0.88889    350      0.79402   0.58504\n  0.3   9         0.8               0.88889    400      0.78379   0.56556\n  0.3   9         0.8               0.88889    450      0.78391   0.56503\n  0.3   9         0.8               0.88889    500      0.79057   0.57837\n  0.3   9         0.8               0.94444     50      0.80092   0.60010\n  0.3   9         0.8               0.94444    100      0.78425   0.56533\n  0.3   9         0.8               0.94444    150      0.78747   0.57247\n  0.3   9         0.8               0.94444    200      0.78747   0.57247\n  0.3   9         0.8               0.94444    250      0.79414   0.58617\n  0.3   9         0.8               0.94444    300      0.79080   0.57956\n  0.3   9         0.8               0.94444    350      0.79080   0.57956\n  0.3   9         0.8               0.94444    400      0.79425   0.58677\n  0.3   9         0.8               0.94444    450      0.79092   0.58011\n  0.3   9         0.8               0.94444    500      0.79092   0.58011\n  0.3   9         0.8               1.00000     50      0.80080   0.59907\n  0.3   9         0.8               1.00000    100      0.79425   0.58526\n  0.3   9         0.8               1.00000    150      0.79425   0.58586\n  0.3   9         0.8               1.00000    200      0.79425   0.58617\n  0.3   9         0.8               1.00000    250      0.79092   0.57926\n  0.3   9         0.8               1.00000    300      0.79080   0.57905\n  0.3   9         0.8               1.00000    350      0.79080   0.57905\n  0.3   9         0.8               1.00000    400      0.79080   0.57905\n  0.3   9         0.8               1.00000    450      0.79080   0.57905\n  0.3   9         0.8               1.00000    500      0.79080   0.57905\n  0.3  10         0.6               0.50000     50      0.81115   0.61969\n  0.3  10         0.6               0.50000    100      0.79747   0.59284\n  0.3  10         0.6               0.50000    150      0.78736   0.57325\n  0.3  10         0.6               0.50000    200      0.80437   0.60640\n  0.3  10         0.6               0.50000    250      0.79759   0.59277\n  0.3  10         0.6               0.50000    300      0.80425   0.60659\n  0.3  10         0.6               0.50000    350      0.79759   0.59326\n  0.3  10         0.6               0.50000    400      0.79437   0.58597\n  0.3  10         0.6               0.50000    450      0.78747   0.57360\n  0.3  10         0.6               0.50000    500      0.78080   0.55997\n  0.3  10         0.6               0.55556     50      0.81414   0.62608\n  0.3  10         0.6               0.55556    100      0.79391   0.58586\n  0.3  10         0.6               0.55556    150      0.79046   0.57795\n  0.3  10         0.6               0.55556    200      0.79391   0.58569\n  0.3  10         0.6               0.55556    250      0.78379   0.56577\n  0.3  10         0.6               0.55556    300      0.79057   0.57873\n  0.3  10         0.6               0.55556    350      0.79057   0.57915\n  0.3  10         0.6               0.55556    400      0.78391   0.56509\n  0.3  10         0.6               0.55556    450      0.79069   0.57838\n  0.3  10         0.6               0.55556    500      0.79402   0.58541\n  0.3  10         0.6               0.61111     50      0.81080   0.62052\n  0.3  10         0.6               0.61111    100      0.78391   0.56609\n  0.3  10         0.6               0.61111    150      0.79080   0.57868\n  0.3  10         0.6               0.61111    200      0.78391   0.56645\n  0.3  10         0.6               0.61111    250      0.78046   0.56040\n  0.3  10         0.6               0.61111    300      0.78713   0.57392\n  0.3  10         0.6               0.61111    350      0.78046   0.55979\n  0.3  10         0.6               0.61111    400      0.78057   0.55941\n  0.3  10         0.6               0.61111    450      0.78724   0.57323\n  0.3  10         0.6               0.61111    500      0.78402   0.56576\n  0.3  10         0.6               0.66667     50      0.79069   0.57786\n  0.3  10         0.6               0.66667    100      0.79759   0.59152\n  0.3  10         0.6               0.66667    150      0.79057   0.57852\n  0.3  10         0.6               0.66667    200      0.80080   0.59827\n  0.3  10         0.6               0.66667    250      0.81103   0.61897\n  0.3  10         0.6               0.66667    300      0.80770   0.61249\n  0.3  10         0.6               0.66667    350      0.79770   0.59207\n  0.3  10         0.6               0.66667    400      0.79747   0.59219\n  0.3  10         0.6               0.66667    450      0.79092   0.57797\n  0.3  10         0.6               0.66667    500      0.79080   0.57859\n  0.3  10         0.6               0.72222     50      0.80402   0.60419\n  0.3  10         0.6               0.72222    100      0.80402   0.60443\n  0.3  10         0.6               0.72222    150      0.80069   0.59790\n  0.3  10         0.6               0.72222    200      0.80069   0.59851\n  0.3  10         0.6               0.72222    250      0.79391   0.58565\n  0.3  10         0.6               0.72222    300      0.80069   0.59928\n  0.3  10         0.6               0.72222    350      0.80425   0.60574\n  0.3  10         0.6               0.72222    400      0.80080   0.59930\n  0.3  10         0.6               0.72222    450      0.79736   0.59208\n  0.3  10         0.6               0.72222    500      0.79057   0.57793\n  0.3  10         0.6               0.77778     50      0.79057   0.57937\n  0.3  10         0.6               0.77778    100      0.79057   0.57961\n  0.3  10         0.6               0.77778    150      0.78379   0.56524\n  0.3  10         0.6               0.77778    200      0.79057   0.57824\n  0.3  10         0.6               0.77778    250      0.79057   0.57825\n  0.3  10         0.6               0.77778    300      0.78724   0.57208\n  0.3  10         0.6               0.77778    350      0.78724   0.57208\n  0.3  10         0.6               0.77778    400      0.78379   0.56492\n  0.3  10         0.6               0.77778    450      0.78724   0.57206\n  0.3  10         0.6               0.77778    500      0.78724   0.57188\n  0.3  10         0.6               0.83333     50      0.79414   0.58577\n  0.3  10         0.6               0.83333    100      0.79069   0.57837\n  0.3  10         0.6               0.83333    150      0.79402   0.58582\n  0.3  10         0.6               0.83333    200      0.79080   0.57916\n  0.3  10         0.6               0.83333    250      0.79747   0.59304\n  0.3  10         0.6               0.83333    300      0.79747   0.59304\n  0.3  10         0.6               0.83333    350      0.79057   0.57939\n  0.3  10         0.6               0.83333    400      0.79057   0.57939\n  0.3  10         0.6               0.83333    450      0.79069   0.57945\n  0.3  10         0.6               0.83333    500      0.79391   0.58636\n  0.3  10         0.6               0.88889     50      0.80437   0.60674\n  0.3  10         0.6               0.88889    100      0.79080   0.58003\n  0.3  10         0.6               0.88889    150      0.79080   0.58009\n  0.3  10         0.6               0.88889    200      0.79414   0.58572\n  0.3  10         0.6               0.88889    250      0.79069   0.57936\n  0.3  10         0.6               0.88889    300      0.78724   0.57226\n  0.3  10         0.6               0.88889    350      0.78724   0.57226\n  0.3  10         0.6               0.88889    400      0.79069   0.57861\n  0.3  10         0.6               0.88889    450      0.79069   0.57861\n  0.3  10         0.6               0.88889    500      0.79069   0.57861\n  0.3  10         0.6               0.94444     50      0.79747   0.59184\n  0.3  10         0.6               0.94444    100      0.78391   0.56480\n  0.3  10         0.6               0.94444    150      0.79069   0.57964\n  0.3  10         0.6               0.94444    200      0.78379   0.56581\n  0.3  10         0.6               0.94444    250      0.78391   0.56612\n  0.3  10         0.6               0.94444    300      0.78724   0.57303\n  0.3  10         0.6               0.94444    350      0.78724   0.57303\n  0.3  10         0.6               0.94444    400      0.78057   0.55946\n  0.3  10         0.6               0.94444    450      0.78057   0.55988\n  0.3  10         0.6               0.94444    500      0.78057   0.55988\n  0.3  10         0.6               1.00000     50      0.80437   0.60680\n  0.3  10         0.6               1.00000    100      0.79103   0.57946\n  0.3  10         0.6               1.00000    150      0.79103   0.57946\n  0.3  10         0.6               1.00000    200      0.78069   0.55928\n  0.3  10         0.6               1.00000    250      0.78402   0.56679\n  0.3  10         0.6               1.00000    300      0.78069   0.55988\n  0.3  10         0.6               1.00000    350      0.78069   0.56016\n  0.3  10         0.6               1.00000    400      0.78069   0.56016\n  0.3  10         0.6               1.00000    450      0.78402   0.56652\n  0.3  10         0.6               1.00000    500      0.78402   0.56584\n  0.3  10         0.8               0.50000     50      0.78724   0.57201\n  0.3  10         0.8               0.50000    100      0.78402   0.56384\n  0.3  10         0.8               0.50000    150      0.77724   0.55045\n  0.3  10         0.8               0.50000    200      0.77724   0.55003\n  0.3  10         0.8               0.50000    250      0.77724   0.55009\n  0.3  10         0.8               0.50000    300      0.77724   0.55099\n  0.3  10         0.8               0.50000    350      0.77379   0.54455\n  0.3  10         0.8               0.50000    400      0.77391   0.54462\n  0.3  10         0.8               0.50000    450      0.76713   0.53081\n  0.3  10         0.8               0.50000    500      0.76046   0.51807\n  0.3  10         0.8               0.55556     50      0.79736   0.59249\n  0.3  10         0.8               0.55556    100      0.79736   0.59207\n  0.3  10         0.8               0.55556    150      0.79736   0.59321\n  0.3  10         0.8               0.55556    200      0.79402   0.58600\n  0.3  10         0.8               0.55556    250      0.78057   0.56002\n  0.3  10         0.8               0.55556    300      0.78402   0.56597\n  0.3  10         0.8               0.55556    350      0.78069   0.55954\n  0.3  10         0.8               0.55556    400      0.77391   0.54608\n  0.3  10         0.8               0.55556    450      0.77724   0.55251\n  0.3  10         0.8               0.55556    500      0.78057   0.55899\n  0.3  10         0.8               0.61111     50      0.79034   0.57981\n  0.3  10         0.8               0.61111    100      0.80414   0.60590\n  0.3  10         0.8               0.61111    150      0.79747   0.59271\n  0.3  10         0.8               0.61111    200      0.80080   0.59992\n  0.3  10         0.8               0.61111    250      0.79414   0.58671\n  0.3  10         0.8               0.61111    300      0.78724   0.57292\n  0.3  10         0.8               0.61111    350      0.78391   0.56637\n  0.3  10         0.8               0.61111    400      0.77391   0.54590\n  0.3  10         0.8               0.61111    450      0.77724   0.55286\n  0.3  10         0.8               0.61111    500      0.78057   0.56012\n  0.3  10         0.8               0.66667     50      0.78069   0.55886\n  0.3  10         0.8               0.66667    100      0.79391   0.58609\n  0.3  10         0.8               0.66667    150      0.79402   0.58610\n  0.3  10         0.8               0.66667    200      0.78747   0.57350\n  0.3  10         0.8               0.66667    250      0.78724   0.57264\n  0.3  10         0.8               0.66667    300      0.79080   0.58118\n  0.3  10         0.8               0.66667    350      0.80092   0.60073\n  0.3  10         0.8               0.66667    400      0.79080   0.58070\n  0.3  10         0.8               0.66667    450      0.78747   0.57367\n  0.3  10         0.8               0.66667    500      0.78414   0.56659\n  0.3  10         0.8               0.72222     50      0.79747   0.59090\n  0.3  10         0.8               0.72222    100      0.79069   0.57932\n  0.3  10         0.8               0.72222    150      0.80057   0.59894\n  0.3  10         0.8               0.72222    200      0.80747   0.61328\n  0.3  10         0.8               0.72222    250      0.80425   0.60710\n  0.3  10         0.8               0.72222    300      0.81080   0.62040\n  0.3  10         0.8               0.72222    350      0.80747   0.61337\n  0.3  10         0.8               0.72222    400      0.80080   0.59998\n  0.3  10         0.8               0.72222    450      0.79402   0.58607\n  0.3  10         0.8               0.72222    500      0.79069   0.57897\n  0.3  10         0.8               0.77778     50      0.79759   0.59134\n  0.3  10         0.8               0.77778    100      0.78402   0.56571\n  0.3  10         0.8               0.77778    150      0.79069   0.57958\n  0.3  10         0.8               0.77778    200      0.78724   0.57315\n  0.3  10         0.8               0.77778    250      0.79069   0.58035\n  0.3  10         0.8               0.77778    300      0.78736   0.57404\n  0.3  10         0.8               0.77778    350      0.79069   0.58095\n  0.3  10         0.8               0.77778    400      0.78736   0.57434\n  0.3  10         0.8               0.77778    450      0.78402   0.56738\n  0.3  10         0.8               0.77778    500      0.78402   0.56768\n  0.3  10         0.8               0.83333     50      0.81724   0.63301\n  0.3  10         0.8               0.83333    100      0.82414   0.64647\n  0.3  10         0.8               0.83333    150      0.81747   0.63259\n  0.3  10         0.8               0.83333    200      0.81747   0.63344\n  0.3  10         0.8               0.83333    250      0.81414   0.62641\n  0.3  10         0.8               0.83333    300      0.81080   0.61992\n  0.3  10         0.8               0.83333    350      0.81080   0.61974\n  0.3  10         0.8               0.83333    400      0.80414   0.60677\n  0.3  10         0.8               0.83333    450      0.80080   0.60034\n  0.3  10         0.8               0.83333    500      0.80414   0.60677\n  0.3  10         0.8               0.88889     50      0.80080   0.59974\n  0.3  10         0.8               0.88889    100      0.79736   0.59289\n  0.3  10         0.8               0.88889    150      0.80069   0.59992\n  0.3  10         0.8               0.88889    200      0.79736   0.59300\n  0.3  10         0.8               0.88889    250      0.79736   0.59300\n  0.3  10         0.8               0.88889    300      0.79402   0.58622\n  0.3  10         0.8               0.88889    350      0.79402   0.58622\n  0.3  10         0.8               0.88889    400      0.78736   0.57283\n  0.3  10         0.8               0.88889    450      0.78736   0.57283\n  0.3  10         0.8               0.88889    500      0.78736   0.57283\n  0.3  10         0.8               0.94444     50      0.79092   0.57980\n  0.3  10         0.8               0.94444    100      0.77747   0.55318\n  0.3  10         0.8               0.94444    150      0.78414   0.56629\n  0.3  10         0.8               0.94444    200      0.78402   0.56619\n  0.3  10         0.8               0.94444    250      0.78069   0.55917\n  0.3  10         0.8               0.94444    300      0.78402   0.56572\n  0.3  10         0.8               0.94444    350      0.78069   0.55917\n  0.3  10         0.8               0.94444    400      0.78069   0.55905\n  0.3  10         0.8               0.94444    450      0.77402   0.54619\n  0.3  10         0.8               0.94444    500      0.77402   0.54619\n  0.3  10         0.8               1.00000     50      0.79759   0.59349\n  0.3  10         0.8               1.00000    100      0.78425   0.56618\n  0.3  10         0.8               1.00000    150      0.78759   0.57267\n  0.3  10         0.8               1.00000    200      0.79092   0.57987\n  0.3  10         0.8               1.00000    250      0.79103   0.58009\n  0.3  10         0.8               1.00000    300      0.79103   0.58009\n  0.3  10         0.8               1.00000    350      0.79437   0.58640\n  0.3  10         0.8               1.00000    400      0.79782   0.59362\n  0.3  10         0.8               1.00000    450      0.79782   0.59362\n  0.3  10         0.8               1.00000    500      0.79782   0.59362\n  0.4   1         0.6               0.50000     50      0.82414   0.64567\n  0.4   1         0.6               0.50000    100      0.82414   0.64483\n  0.4   1         0.6               0.50000    150      0.81414   0.62441\n  0.4   1         0.6               0.50000    200      0.81425   0.62530\n  0.4   1         0.6               0.50000    250      0.80080   0.59841\n  0.4   1         0.6               0.50000    300      0.80747   0.61193\n  0.4   1         0.6               0.50000    350      0.78701   0.57170\n  0.4   1         0.6               0.50000    400      0.80747   0.61151\n  0.4   1         0.6               0.50000    450      0.80046   0.59770\n  0.4   1         0.6               0.50000    500      0.78713   0.57147\n  0.4   1         0.6               0.55556     50      0.83437   0.66630\n  0.4   1         0.6               0.55556    100      0.81770   0.63480\n  0.4   1         0.6               0.55556    150      0.80425   0.60600\n  0.4   1         0.6               0.55556    200      0.80092   0.59907\n  0.4   1         0.6               0.55556    250      0.81057   0.61970\n  0.4   1         0.6               0.55556    300      0.79069   0.57821\n  0.4   1         0.6               0.55556    350      0.78402   0.56515\n  0.4   1         0.6               0.55556    400      0.78379   0.56428\n  0.4   1         0.6               0.55556    450      0.79402   0.58516\n  0.4   1         0.6               0.55556    500      0.78057   0.55886\n  0.4   1         0.6               0.61111     50      0.82437   0.64575\n  0.4   1         0.6               0.61111    100      0.80402   0.60612\n  0.4   1         0.6               0.61111    150      0.79402   0.58527\n  0.4   1         0.6               0.61111    200      0.79736   0.59155\n  0.4   1         0.6               0.61111    250      0.80057   0.59921\n  0.4   1         0.6               0.61111    300      0.79414   0.58575\n  0.4   1         0.6               0.61111    350      0.80057   0.59914\n  0.4   1         0.6               0.61111    400      0.78379   0.56589\n  0.4   1         0.6               0.61111    450      0.78368   0.56592\n  0.4   1         0.6               0.61111    500      0.78368   0.56526\n  0.4   1         0.6               0.66667     50      0.83805   0.67417\n  0.4   1         0.6               0.66667    100      0.81080   0.62026\n  0.4   1         0.6               0.66667    150      0.82103   0.63922\n  0.4   1         0.6               0.66667    200      0.80046   0.59946\n  0.4   1         0.6               0.66667    250      0.81747   0.63248\n  0.4   1         0.6               0.66667    300      0.80724   0.61212\n  0.4   1         0.6               0.66667    350      0.79724   0.59250\n  0.4   1         0.6               0.66667    400      0.81092   0.61889\n  0.4   1         0.6               0.66667    450      0.79402   0.58534\n  0.4   1         0.6               0.66667    500      0.79069   0.57819\n  0.4   1         0.6               0.72222     50      0.81747   0.63303\n  0.4   1         0.6               0.72222    100      0.81736   0.63248\n  0.4   1         0.6               0.72222    150      0.81414   0.62577\n  0.4   1         0.6               0.72222    200      0.81069   0.61813\n  0.4   1         0.6               0.72222    250      0.82425   0.64519\n  0.4   1         0.6               0.72222    300      0.80414   0.60405\n  0.4   1         0.6               0.72222    350      0.80425   0.60464\n  0.4   1         0.6               0.72222    400      0.79391   0.58421\n  0.4   1         0.6               0.72222    450      0.79736   0.59164\n  0.4   1         0.6               0.72222    500      0.80414   0.60496\n  0.4   1         0.6               0.77778     50      0.84793   0.69324\n  0.4   1         0.6               0.77778    100      0.84126   0.67917\n  0.4   1         0.6               0.77778    150      0.82103   0.63759\n  0.4   1         0.6               0.77778    200      0.80747   0.61174\n  0.4   1         0.6               0.77778    250      0.81069   0.61775\n  0.4   1         0.6               0.77778    300      0.79713   0.59122\n  0.4   1         0.6               0.77778    350      0.80736   0.61191\n  0.4   1         0.6               0.77778    400      0.80391   0.60508\n  0.4   1         0.6               0.77778    450      0.79379   0.58473\n  0.4   1         0.6               0.77778    500      0.79402   0.58454\n  0.4   1         0.6               0.83333     50      0.83759   0.67417\n  0.4   1         0.6               0.83333    100      0.82069   0.63819\n  0.4   1         0.6               0.83333    150      0.81736   0.63103\n  0.4   1         0.6               0.83333    200      0.81080   0.61771\n  0.4   1         0.6               0.83333    250      0.80402   0.60436\n  0.4   1         0.6               0.83333    300      0.82402   0.64600\n  0.4   1         0.6               0.83333    350      0.80713   0.61218\n  0.4   1         0.6               0.83333    400      0.81747   0.63144\n  0.4   1         0.6               0.83333    450      0.82425   0.64506\n  0.4   1         0.6               0.83333    500      0.81759   0.63136\n  0.4   1         0.6               0.88889     50      0.83448   0.66722\n  0.4   1         0.6               0.88889    100      0.83425   0.66643\n  0.4   1         0.6               0.88889    150      0.83414   0.66562\n  0.4   1         0.6               0.88889    200      0.83092   0.65881\n  0.4   1         0.6               0.88889    250      0.82414   0.64555\n  0.4   1         0.6               0.88889    300      0.82080   0.63853\n  0.4   1         0.6               0.88889    350      0.81069   0.61781\n  0.4   1         0.6               0.88889    400      0.81069   0.61818\n  0.4   1         0.6               0.88889    450      0.81069   0.61793\n  0.4   1         0.6               0.88889    500      0.81069   0.61793\n  0.4   1         0.6               0.94444     50      0.83425   0.66634\n  0.4   1         0.6               0.94444    100      0.82759   0.65327\n  0.4   1         0.6               0.94444    150      0.82080   0.63872\n  0.4   1         0.6               0.94444    200      0.81402   0.62461\n  0.4   1         0.6               0.94444    250      0.82080   0.63829\n  0.4   1         0.6               0.94444    300      0.81402   0.62460\n  0.4   1         0.6               0.94444    350      0.81736   0.63151\n  0.4   1         0.6               0.94444    400      0.82080   0.63872\n  0.4   1         0.6               0.94444    450      0.81736   0.63151\n  0.4   1         0.6               0.94444    500      0.81402   0.62466\n  0.4   1         0.6               1.00000     50      0.82759   0.65204\n  0.4   1         0.6               1.00000    100      0.83103   0.65969\n  0.4   1         0.6               1.00000    150      0.83448   0.66671\n  0.4   1         0.6               1.00000    200      0.82425   0.64567\n  0.4   1         0.6               1.00000    250      0.81747   0.63186\n  0.4   1         0.6               1.00000    300      0.81747   0.63186\n  0.4   1         0.6               1.00000    350      0.81080   0.61792\n  0.4   1         0.6               1.00000    400      0.81080   0.61792\n  0.4   1         0.6               1.00000    450      0.80402   0.60368\n  0.4   1         0.6               1.00000    500      0.80402   0.60368\n  0.4   1         0.8               0.50000     50      0.81425   0.62569\n  0.4   1         0.8               0.50000    100      0.79724   0.59170\n  0.4   1         0.8               0.50000    150      0.80402   0.60605\n  0.4   1         0.8               0.50000    200      0.79724   0.59119\n  0.4   1         0.8               0.50000    250      0.81747   0.63181\n  0.4   1         0.8               0.50000    300      0.81080   0.61866\n  0.4   1         0.8               0.50000    350      0.81759   0.63203\n  0.4   1         0.8               0.50000    400      0.81080   0.61968\n  0.4   1         0.8               0.50000    450      0.80425   0.60531\n  0.4   1         0.8               0.50000    500      0.78724   0.57248\n  0.4   1         0.8               0.55556     50      0.80747   0.61155\n  0.4   1         0.8               0.55556    100      0.79402   0.58468\n  0.4   1         0.8               0.55556    150      0.78713   0.57055\n  0.4   1         0.8               0.55556    200      0.79391   0.58412\n  0.4   1         0.8               0.55556    250      0.79414   0.58513\n  0.4   1         0.8               0.55556    300      0.80069   0.59800\n  0.4   1         0.8               0.55556    350      0.79402   0.58466\n  0.4   1         0.8               0.55556    400      0.79080   0.57810\n  0.4   1         0.8               0.55556    450      0.78057   0.55770\n  0.4   1         0.8               0.55556    500      0.79402   0.58517\n  0.4   1         0.8               0.61111     50      0.83425   0.66502\n  0.4   1         0.8               0.61111    100      0.82770   0.65240\n  0.4   1         0.8               0.61111    150      0.80736   0.61313\n  0.4   1         0.8               0.61111    200      0.79736   0.59163\n  0.4   1         0.8               0.61111    250      0.80379   0.60516\n  0.4   1         0.8               0.61111    300      0.79391   0.58505\n  0.4   1         0.8               0.61111    350      0.79736   0.59266\n  0.4   1         0.8               0.61111    400      0.81759   0.63246\n  0.4   1         0.8               0.61111    450      0.79747   0.59119\n  0.4   1         0.8               0.61111    500      0.79402   0.58467\n  0.4   1         0.8               0.66667     50      0.81782   0.63361\n  0.4   1         0.8               0.66667    100      0.81770   0.63320\n  0.4   1         0.8               0.66667    150      0.81759   0.63210\n  0.4   1         0.8               0.66667    200      0.81402   0.62545\n  0.4   1         0.8               0.66667    250      0.81736   0.63203\n  0.4   1         0.8               0.66667    300      0.81402   0.62665\n  0.4   1         0.8               0.66667    350      0.81414   0.62471\n  0.4   1         0.8               0.66667    400      0.80414   0.60526\n  0.4   1         0.8               0.66667    450      0.80414   0.60477\n  0.4   1         0.8               0.66667    500      0.80069   0.59894\n  0.4   1         0.8               0.72222     50      0.84448   0.68767\n  0.4   1         0.8               0.72222    100      0.83448   0.66593\n  0.4   1         0.8               0.72222    150      0.83126   0.66040\n  0.4   1         0.8               0.72222    200      0.81425   0.62611\n  0.4   1         0.8               0.72222    250      0.82770   0.65326\n  0.4   1         0.8               0.72222    300      0.82425   0.64592\n  0.4   1         0.8               0.72222    350      0.81414   0.62606\n  0.4   1         0.8               0.72222    400      0.81092   0.61877\n  0.4   1         0.8               0.72222    450      0.81080   0.61919\n  0.4   1         0.8               0.72222    500      0.80747   0.61246\n  0.4   1         0.8               0.77778     50      0.82069   0.64081\n  0.4   1         0.8               0.77778    100      0.82080   0.63871\n  0.4   1         0.8               0.77778    150      0.82080   0.64059\n  0.4   1         0.8               0.77778    200      0.80391   0.60416\n  0.4   1         0.8               0.77778    250      0.81080   0.61834\n  0.4   1         0.8               0.77778    300      0.81414   0.62403\n  0.4   1         0.8               0.77778    350      0.80080   0.59871\n  0.4   1         0.8               0.77778    400      0.79391   0.58353\n  0.4   1         0.8               0.77778    450      0.79414   0.58416\n  0.4   1         0.8               0.77778    500      0.79402   0.58464\n  0.4   1         0.8               0.83333     50      0.84437   0.68761\n  0.4   1         0.8               0.83333    100      0.82747   0.65138\n  0.4   1         0.8               0.83333    150      0.83437   0.66541\n  0.4   1         0.8               0.83333    200      0.82092   0.63839\n  0.4   1         0.8               0.83333    250      0.81069   0.61770\n  0.4   1         0.8               0.83333    300      0.80724   0.61157\n  0.4   1         0.8               0.83333    350      0.81057   0.61775\n  0.4   1         0.8               0.83333    400      0.80057   0.59769\n  0.4   1         0.8               0.83333    450      0.79724   0.59107\n  0.4   1         0.8               0.83333    500      0.80402   0.60567\n  0.4   1         0.8               0.88889     50      0.83425   0.66659\n  0.4   1         0.8               0.88889    100      0.81057   0.61916\n  0.4   1         0.8               0.88889    150      0.82080   0.63873\n  0.4   1         0.8               0.88889    200      0.80736   0.61145\n  0.4   1         0.8               0.88889    250      0.80046   0.59867\n  0.4   1         0.8               0.88889    300      0.80379   0.60515\n  0.4   1         0.8               0.88889    350      0.81391   0.62537\n  0.4   1         0.8               0.88889    400      0.80736   0.61188\n  0.4   1         0.8               0.88889    450      0.81069   0.61930\n  0.4   1         0.8               0.88889    500      0.80747   0.61204\n  0.4   1         0.8               0.94444     50      0.82770   0.65242\n  0.4   1         0.8               0.94444    100      0.83782   0.67271\n  0.4   1         0.8               0.94444    150      0.81069   0.61879\n  0.4   1         0.8               0.94444    200      0.80057   0.59765\n  0.4   1         0.8               0.94444    250      0.81414   0.62417\n  0.4   1         0.8               0.94444    300      0.81414   0.62416\n  0.4   1         0.8               0.94444    350      0.81402   0.62454\n  0.4   1         0.8               0.94444    400      0.81080   0.61761\n  0.4   1         0.8               0.94444    450      0.81747   0.63118\n  0.4   1         0.8               0.94444    500      0.81759   0.63160\n  0.4   1         0.8               1.00000     50      0.83103   0.65981\n  0.4   1         0.8               1.00000    100      0.82437   0.64606\n  0.4   1         0.8               1.00000    150      0.81770   0.63222\n  0.4   1         0.8               1.00000    200      0.81770   0.63270\n  0.4   1         0.8               1.00000    250      0.80747   0.61131\n  0.4   1         0.8               1.00000    300      0.80747   0.61113\n  0.4   1         0.8               1.00000    350      0.80747   0.61125\n  0.4   1         0.8               1.00000    400      0.81080   0.61792\n  0.4   1         0.8               1.00000    450      0.81080   0.61792\n  0.4   1         0.8               1.00000    500      0.80736   0.61023\n  0.4   2         0.6               0.50000     50      0.79747   0.59342\n  0.4   2         0.6               0.50000    100      0.78379   0.56485\n  0.4   2         0.6               0.50000    150      0.75667   0.51344\n  0.4   2         0.6               0.50000    200      0.75701   0.51156\n  0.4   2         0.6               0.50000    250      0.76379   0.52580\n  0.4   2         0.6               0.50000    300      0.76034   0.51871\n  0.4   2         0.6               0.50000    350      0.76034   0.51919\n  0.4   2         0.6               0.50000    400      0.76713   0.53208\n  0.4   2         0.6               0.50000    450      0.76391   0.52536\n  0.4   2         0.6               0.50000    500      0.76046   0.51813\n  0.4   2         0.6               0.55556     50      0.79759   0.59263\n  0.4   2         0.6               0.55556    100      0.78046   0.55848\n  0.4   2         0.6               0.55556    150      0.78057   0.55827\n  0.4   2         0.6               0.55556    200      0.78046   0.55891\n  0.4   2         0.6               0.55556    250      0.78391   0.56581\n  0.4   2         0.6               0.55556    300      0.78069   0.55916\n  0.4   2         0.6               0.55556    350      0.76713   0.53247\n  0.4   2         0.6               0.55556    400      0.77379   0.54586\n  0.4   2         0.6               0.55556    450      0.78080   0.55796\n  0.4   2         0.6               0.55556    500      0.77736   0.55229\n  0.4   2         0.6               0.61111     50      0.79747   0.59244\n  0.4   2         0.6               0.61111    100      0.78713   0.57121\n  0.4   2         0.6               0.61111    150      0.79701   0.59170\n  0.4   2         0.6               0.61111    200      0.78034   0.55819\n  0.4   2         0.6               0.61111    250      0.77701   0.55189\n  0.4   2         0.6               0.61111    300      0.77034   0.53905\n  0.4   2         0.6               0.61111    350      0.76713   0.53173\n  0.4   2         0.6               0.61111    400      0.76713   0.53308\n  0.4   2         0.6               0.61111    450      0.76713   0.53338\n  0.4   2         0.6               0.61111    500      0.76391   0.52608\n  0.4   2         0.6               0.66667     50      0.79759   0.59315\n  0.4   2         0.6               0.66667    100      0.78046   0.55747\n  0.4   2         0.6               0.66667    150      0.77046   0.53693\n  0.4   2         0.6               0.66667    200      0.77057   0.53801\n  0.4   2         0.6               0.66667    250      0.77057   0.53758\n  0.4   2         0.6               0.66667    300      0.77046   0.53818\n  0.4   2         0.6               0.66667    350      0.76034   0.51877\n  0.4   2         0.6               0.66667    400      0.76713   0.53206\n  0.4   2         0.6               0.66667    450      0.77402   0.54449\n  0.4   2         0.6               0.66667    500      0.77724   0.55151\n  0.4   2         0.6               0.72222     50      0.80402   0.60682\n  0.4   2         0.6               0.72222    100      0.79057   0.57812\n  0.4   2         0.6               0.72222    150      0.79414   0.58496\n  0.4   2         0.6               0.72222    200      0.79402   0.58546\n  0.4   2         0.6               0.72222    250      0.79057   0.57922\n  0.4   2         0.6               0.72222    300      0.78391   0.56555\n  0.4   2         0.6               0.72222    350      0.79057   0.57807\n  0.4   2         0.6               0.72222    400      0.78402   0.56505\n  0.4   2         0.6               0.72222    450      0.78391   0.56480\n  0.4   2         0.6               0.72222    500      0.78391   0.56557\n  0.4   2         0.6               0.77778     50      0.82437   0.64598\n  0.4   2         0.6               0.77778    100      0.78069   0.56029\n  0.4   2         0.6               0.77778    150      0.77713   0.55274\n  0.4   2         0.6               0.77778    200      0.77736   0.55284\n  0.4   2         0.6               0.77778    250      0.78057   0.55915\n  0.4   2         0.6               0.77778    300      0.78080   0.55917\n  0.4   2         0.6               0.77778    350      0.79080   0.58023\n  0.4   2         0.6               0.77778    400      0.78747   0.57338\n  0.4   2         0.6               0.77778    450      0.78402   0.56685\n  0.4   2         0.6               0.77778    500      0.78747   0.57332\n  0.4   2         0.6               0.83333     50      0.81414   0.62643\n  0.4   2         0.6               0.83333    100      0.77713   0.55093\n  0.4   2         0.6               0.83333    150      0.79080   0.57923\n  0.4   2         0.6               0.83333    200      0.77057   0.53795\n  0.4   2         0.6               0.83333    250      0.77402   0.54558\n  0.4   2         0.6               0.83333    300      0.77724   0.55117\n  0.4   2         0.6               0.83333    350      0.77057   0.53915\n  0.4   2         0.6               0.83333    400      0.77402   0.54570\n  0.4   2         0.6               0.83333    450      0.77402   0.54570\n  0.4   2         0.6               0.83333    500      0.77402   0.54559\n  0.4   2         0.6               0.88889     50      0.80034   0.60017\n  0.4   2         0.6               0.88889    100      0.77356   0.54548\n  0.4   2         0.6               0.88889    150      0.78034   0.55845\n  0.4   2         0.6               0.88889    200      0.78046   0.55874\n  0.4   2         0.6               0.88889    250      0.77368   0.54534\n  0.4   2         0.6               0.88889    300      0.77023   0.53950\n  0.4   2         0.6               0.88889    350      0.78391   0.56641\n  0.4   2         0.6               0.88889    400      0.78391   0.56653\n  0.4   2         0.6               0.88889    450      0.77391   0.54659\n  0.4   2         0.6               0.88889    500      0.78057   0.56016\n  0.4   2         0.6               0.94444     50      0.78414   0.56501\n  0.4   2         0.6               0.94444    100      0.78046   0.55897\n  0.4   2         0.6               0.94444    150      0.76713   0.53239\n  0.4   2         0.6               0.94444    200      0.77379   0.54504\n  0.4   2         0.6               0.94444    250      0.77368   0.54567\n  0.4   2         0.6               0.94444    300      0.78046   0.55948\n  0.4   2         0.6               0.94444    350      0.77736   0.55273\n  0.4   2         0.6               0.94444    400      0.77402   0.54624\n  0.4   2         0.6               0.94444    450      0.77736   0.55327\n  0.4   2         0.6               0.94444    500      0.77069   0.53993\n  0.4   2         0.6               1.00000     50      0.81414   0.62640\n  0.4   2         0.6               1.00000    100      0.79701   0.59215\n  0.4   2         0.6               1.00000    150      0.79034   0.57906\n  0.4   2         0.6               1.00000    200      0.78379   0.56522\n  0.4   2         0.6               1.00000    250      0.79391   0.58571\n  0.4   2         0.6               1.00000    300      0.80080   0.59966\n  0.4   2         0.6               1.00000    350      0.78402   0.56595\n  0.4   2         0.6               1.00000    400      0.78069   0.55904\n  0.4   2         0.6               1.00000    450      0.78069   0.55904\n  0.4   2         0.6               1.00000    500      0.77724   0.55207\n  0.4   2         0.8               0.50000     50      0.79724   0.59293\n  0.4   2         0.8               0.50000    100      0.78069   0.55875\n  0.4   2         0.8               0.50000    150      0.77747   0.55206\n  0.4   2         0.8               0.50000    200      0.78747   0.57138\n  0.4   2         0.8               0.50000    250      0.77391   0.54613\n  0.4   2         0.8               0.50000    300      0.77414   0.54516\n  0.4   2         0.8               0.50000    350      0.77069   0.53912\n  0.4   2         0.8               0.50000    400      0.76747   0.53201\n  0.4   2         0.8               0.50000    450      0.76724   0.53165\n  0.4   2         0.8               0.50000    500      0.76057   0.51951\n  0.4   2         0.8               0.55556     50      0.80724   0.61195\n  0.4   2         0.8               0.55556    100      0.79414   0.58699\n  0.4   2         0.8               0.55556    150      0.79747   0.59367\n  0.4   2         0.8               0.55556    200      0.79057   0.58010\n  0.4   2         0.8               0.55556    250      0.80069   0.59977\n  0.4   2         0.8               0.55556    300      0.78736   0.57293\n  0.4   2         0.8               0.55556    350      0.79724   0.59354\n  0.4   2         0.8               0.55556    400      0.79057   0.57904\n  0.4   2         0.8               0.55556    450      0.79069   0.57873\n  0.4   2         0.8               0.55556    500      0.79057   0.57868\n  0.4   2         0.8               0.61111     50      0.80402   0.60564\n  0.4   2         0.8               0.61111    100      0.78046   0.55899\n  0.4   2         0.8               0.61111    150      0.78057   0.55892\n  0.4   2         0.8               0.61111    200      0.77713   0.55123\n  0.4   2         0.8               0.61111    250      0.78724   0.57244\n  0.4   2         0.8               0.61111    300      0.77379   0.54439\n  0.4   2         0.8               0.61111    350      0.78379   0.56588\n  0.4   2         0.8               0.61111    400      0.77057   0.53887\n  0.4   2         0.8               0.61111    450      0.77057   0.53825\n  0.4   2         0.8               0.61111    500      0.76713   0.53182\n  0.4   2         0.8               0.66667     50      0.79379   0.58611\n  0.4   2         0.8               0.66667    100      0.80046   0.59948\n  0.4   2         0.8               0.66667    150      0.77368   0.54699\n  0.4   2         0.8               0.66667    200      0.78034   0.56002\n  0.4   2         0.8               0.66667    250      0.77023   0.53919\n  0.4   2         0.8               0.66667    300      0.76345   0.52657\n  0.4   2         0.8               0.66667    350      0.75678   0.51306\n  0.4   2         0.8               0.66667    400      0.75011   0.49924\n  0.4   2         0.8               0.66667    450      0.76701   0.53256\n  0.4   2         0.8               0.66667    500      0.76701   0.53256\n  0.4   2         0.8               0.72222     50      0.80782   0.61125\n  0.4   2         0.8               0.72222    100      0.79759   0.59186\n  0.4   2         0.8               0.72222    150      0.78414   0.56595\n  0.4   2         0.8               0.72222    200      0.77724   0.55091\n  0.4   2         0.8               0.72222    250      0.77724   0.55097\n  0.4   2         0.8               0.72222    300      0.78414   0.56420\n  0.4   2         0.8               0.72222    350      0.78402   0.56479\n  0.4   2         0.8               0.72222    400      0.78402   0.56516\n  0.4   2         0.8               0.72222    450      0.78402   0.56564\n  0.4   2         0.8               0.72222    500      0.79069   0.57867\n  0.4   2         0.8               0.77778     50      0.79713   0.59294\n  0.4   2         0.8               0.77778    100      0.79057   0.57949\n  0.4   2         0.8               0.77778    150      0.78057   0.56050\n  0.4   2         0.8               0.77778    200      0.78391   0.56606\n  0.4   2         0.8               0.77778    250      0.78046   0.55923\n  0.4   2         0.8               0.77778    300      0.78057   0.55951\n  0.4   2         0.8               0.77778    350      0.78713   0.57224\n  0.4   2         0.8               0.77778    400      0.78724   0.57290\n  0.4   2         0.8               0.77778    450      0.79402   0.58588\n  0.4   2         0.8               0.77778    500      0.79069   0.57933\n  0.4   2         0.8               0.83333     50      0.80034   0.59989\n  0.4   2         0.8               0.83333    100      0.78356   0.56536\n  0.4   2         0.8               0.83333    150      0.78368   0.56632\n  0.4   2         0.8               0.83333    200      0.79391   0.58733\n  0.4   2         0.8               0.83333    250      0.79057   0.58113\n  0.4   2         0.8               0.83333    300      0.79724   0.59410\n  0.4   2         0.8               0.83333    350      0.80057   0.60005\n  0.4   2         0.8               0.83333    400      0.79391   0.58630\n  0.4   2         0.8               0.83333    450      0.78724   0.57350\n  0.4   2         0.8               0.83333    500      0.78391   0.56672\n  0.4   2         0.8               0.88889     50      0.81414   0.62551\n  0.4   2         0.8               0.88889    100      0.79046   0.57911\n  0.4   2         0.8               0.88889    150      0.79069   0.57916\n  0.4   2         0.8               0.88889    200      0.78724   0.57213\n  0.4   2         0.8               0.88889    250      0.78057   0.55819\n  0.4   2         0.8               0.88889    300      0.78391   0.56534\n  0.4   2         0.8               0.88889    350      0.77724   0.55183\n  0.4   2         0.8               0.88889    400      0.77391   0.54486\n  0.4   2         0.8               0.88889    450      0.77724   0.55183\n  0.4   2         0.8               0.88889    500      0.77724   0.55183\n  0.4   2         0.8               0.94444     50      0.79368   0.58436\n  0.4   2         0.8               0.94444    100      0.79368   0.58426\n  0.4   2         0.8               0.94444    150      0.78368   0.56458\n  0.4   2         0.8               0.94444    200      0.76701   0.53143\n  0.4   2         0.8               0.94444    250      0.77379   0.54553\n  0.4   2         0.8               0.94444    300      0.77046   0.53953\n  0.4   2         0.8               0.94444    350      0.77713   0.55262\n  0.4   2         0.8               0.94444    400      0.77391   0.54600\n  0.4   2         0.8               0.94444    450      0.77057   0.53915\n  0.4   2         0.8               0.94444    500      0.77057   0.53969\n  0.4   2         0.8               1.00000     50      0.79057   0.57819\n  0.4   2         0.8               1.00000    100      0.77713   0.55225\n  0.4   2         0.8               1.00000    150      0.77713   0.55099\n  0.4   2         0.8               1.00000    200      0.77713   0.55184\n  0.4   2         0.8               1.00000    250      0.77391   0.54487\n  0.4   2         0.8               1.00000    300      0.77736   0.55170\n  0.4   2         0.8               1.00000    350      0.77057   0.53789\n  0.4   2         0.8               1.00000    400      0.77391   0.54474\n  0.4   2         0.8               1.00000    450      0.76379   0.52506\n  0.4   2         0.8               1.00000    500      0.76713   0.53191\n  0.4   3         0.6               0.50000     50      0.80402   0.60454\n  0.4   3         0.6               0.50000    100      0.78736   0.57078\n  0.4   3         0.6               0.50000    150      0.78402   0.56513\n  0.4   3         0.6               0.50000    200      0.78747   0.57198\n  0.4   3         0.6               0.50000    250      0.78069   0.55841\n  0.4   3         0.6               0.50000    300      0.77724   0.55258\n  0.4   3         0.6               0.50000    350      0.77379   0.54502\n  0.4   3         0.6               0.50000    400      0.77046   0.53848\n  0.4   3         0.6               0.50000    450      0.77379   0.54557\n  0.4   3         0.6               0.50000    500      0.76379   0.52521\n  0.4   3         0.6               0.55556     50      0.79724   0.59415\n  0.4   3         0.6               0.55556    100      0.79736   0.59398\n  0.4   3         0.6               0.55556    150      0.79724   0.59287\n  0.4   3         0.6               0.55556    200      0.79057   0.58123\n  0.4   3         0.6               0.55556    250      0.79391   0.58808\n  0.4   3         0.6               0.55556    300      0.80069   0.60063\n  0.4   3         0.6               0.55556    350      0.79391   0.58773\n  0.4   3         0.6               0.55556    400      0.78713   0.57486\n  0.4   3         0.6               0.55556    450      0.79034   0.58107\n  0.4   3         0.6               0.55556    500      0.79379   0.58733\n  0.4   3         0.6               0.61111     50      0.80759   0.61317\n  0.4   3         0.6               0.61111    100      0.78747   0.57326\n  0.4   3         0.6               0.61111    150      0.77379   0.54614\n  0.4   3         0.6               0.61111    200      0.78402   0.56552\n  0.4   3         0.6               0.61111    250      0.78414   0.56589\n  0.4   3         0.6               0.61111    300      0.79080   0.57911\n  0.4   3         0.6               0.61111    350      0.77402   0.54623\n  0.4   3         0.6               0.61111    400      0.77057   0.53981\n  0.4   3         0.6               0.61111    450      0.78080   0.55970\n  0.4   3         0.6               0.61111    500      0.78069   0.55975\n  0.4   3         0.6               0.66667     50      0.80402   0.60547\n  0.4   3         0.6               0.66667    100      0.80080   0.59830\n  0.4   3         0.6               0.66667    150      0.79736   0.59304\n  0.4   3         0.6               0.66667    200      0.79759   0.59255\n  0.4   3         0.6               0.66667    250      0.79747   0.59188\n  0.4   3         0.6               0.66667    300      0.79414   0.58551\n  0.4   3         0.6               0.66667    350      0.79747   0.59182\n  0.4   3         0.6               0.66667    400      0.79747   0.59170\n  0.4   3         0.6               0.66667    450      0.79747   0.59170\n  0.4   3         0.6               0.66667    500      0.79080   0.57849\n  0.4   3         0.6               0.72222     50      0.80747   0.61329\n  0.4   3         0.6               0.72222    100      0.80046   0.59813\n  0.4   3         0.6               0.72222    150      0.79379   0.58438\n  0.4   3         0.6               0.72222    200      0.79379   0.58570\n  0.4   3         0.6               0.72222    250      0.78023   0.55821\n  0.4   3         0.6               0.72222    300      0.79724   0.59371\n  0.4   3         0.6               0.72222    350      0.79391   0.58758\n  0.4   3         0.6               0.72222    400      0.78391   0.56747\n  0.4   3         0.6               0.72222    450      0.78724   0.57371\n  0.4   3         0.6               0.72222    500      0.78713   0.57363\n  0.4   3         0.6               0.77778     50      0.80057   0.59882\n  0.4   3         0.6               0.77778    100      0.78713   0.57146\n  0.4   3         0.6               0.77778    150      0.78736   0.57224\n  0.4   3         0.6               0.77778    200      0.78069   0.56002\n  0.4   3         0.6               0.77778    250      0.78402   0.56627\n  0.4   3         0.6               0.77778    300      0.77736   0.55354\n  0.4   3         0.6               0.77778    350      0.77402   0.54699\n  0.4   3         0.6               0.77778    400      0.78069   0.56015\n  0.4   3         0.6               0.77778    450      0.77736   0.55388\n  0.4   3         0.6               0.77778    500      0.77402   0.54699\n  0.4   3         0.6               0.83333     50      0.79069   0.57766\n  0.4   3         0.6               0.83333    100      0.78724   0.57189\n  0.4   3         0.6               0.83333    150      0.79046   0.57865\n  0.4   3         0.6               0.83333    200      0.79057   0.57838\n  0.4   3         0.6               0.83333    250      0.79069   0.57862\n  0.4   3         0.6               0.83333    300      0.79069   0.57862\n  0.4   3         0.6               0.83333    350      0.79069   0.57862\n  0.4   3         0.6               0.83333    400      0.79402   0.58517\n  0.4   3         0.6               0.83333    450      0.79069   0.57862\n  0.4   3         0.6               0.83333    500      0.78057   0.55860\n  0.4   3         0.6               0.88889     50      0.82092   0.63886\n  0.4   3         0.6               0.88889    100      0.80080   0.59899\n  0.4   3         0.6               0.88889    150      0.79747   0.59232\n  0.4   3         0.6               0.88889    200      0.78057   0.55873\n  0.4   3         0.6               0.88889    250      0.78402   0.56516\n  0.4   3         0.6               0.88889    300      0.78057   0.55873\n  0.4   3         0.6               0.88889    350      0.78391   0.56564\n  0.4   3         0.6               0.88889    400      0.78057   0.55861\n  0.4   3         0.6               0.88889    450      0.78391   0.56510\n  0.4   3         0.6               0.88889    500      0.78391   0.56564\n  0.4   3         0.6               0.94444     50      0.78023   0.55643\n  0.4   3         0.6               0.94444    100      0.78023   0.55797\n  0.4   3         0.6               0.94444    150      0.78368   0.56572\n  0.4   3         0.6               0.94444    200      0.78379   0.56522\n  0.4   3         0.6               0.94444    250      0.77701   0.55184\n  0.4   3         0.6               0.94444    300      0.77368   0.54542\n  0.4   3         0.6               0.94444    350      0.77701   0.55227\n  0.4   3         0.6               0.94444    400      0.77701   0.55227\n  0.4   3         0.6               0.94444    450      0.77368   0.54542\n  0.4   3         0.6               0.94444    500      0.77368   0.54542\n  0.4   3         0.6               1.00000     50      0.78747   0.57129\n  0.4   3         0.6               1.00000    100      0.77414   0.54499\n  0.4   3         0.6               1.00000    150      0.77069   0.53885\n  0.4   3         0.6               1.00000    200      0.77414   0.54530\n  0.4   3         0.6               1.00000    250      0.78080   0.55923\n  0.4   3         0.6               1.00000    300      0.78080   0.55917\n  0.4   3         0.6               1.00000    350      0.78414   0.56578\n  0.4   3         0.6               1.00000    400      0.78069   0.55924\n  0.4   3         0.6               1.00000    450      0.78069   0.55972\n  0.4   3         0.6               1.00000    500      0.78069   0.55972\n  0.4   3         0.8               0.50000     50      0.80414   0.60527\n  0.4   3         0.8               0.50000    100      0.79069   0.57810\n  0.4   3         0.8               0.50000    150      0.78069   0.55815\n  0.4   3         0.8               0.50000    200      0.78391   0.56444\n  0.4   3         0.8               0.50000    250      0.78414   0.56520\n  0.4   3         0.8               0.50000    300      0.78069   0.55801\n  0.4   3         0.8               0.50000    350      0.78747   0.57265\n  0.4   3         0.8               0.50000    400      0.78414   0.56539\n  0.4   3         0.8               0.50000    450      0.78425   0.56568\n  0.4   3         0.8               0.50000    500      0.78080   0.55819\n  0.4   3         0.8               0.55556     50      0.79759   0.59248\n  0.4   3         0.8               0.55556    100      0.77391   0.54570\n  0.4   3         0.8               0.55556    150      0.79069   0.57954\n  0.4   3         0.8               0.55556    200      0.78747   0.57184\n  0.4   3         0.8               0.55556    250      0.78402   0.56505\n  0.4   3         0.8               0.55556    300      0.78402   0.56584\n  0.4   3         0.8               0.55556    350      0.77724   0.55147\n  0.4   3         0.8               0.55556    400      0.79080   0.57881\n  0.4   3         0.8               0.55556    450      0.78069   0.55856\n  0.4   3         0.8               0.55556    500      0.78747   0.57256\n  0.4   3         0.8               0.61111     50      0.79379   0.58511\n  0.4   3         0.8               0.61111    100      0.79414   0.58571\n  0.4   3         0.8               0.61111    150      0.78724   0.57202\n  0.4   3         0.8               0.61111    200      0.77391   0.54540\n  0.4   3         0.8               0.61111    250      0.76724   0.53273\n  0.4   3         0.8               0.61111    300      0.78069   0.55951\n  0.4   3         0.8               0.61111    350      0.78069   0.55927\n  0.4   3         0.8               0.61111    400      0.77736   0.55230\n  0.4   3         0.8               0.61111    450      0.78080   0.55902\n  0.4   3         0.8               0.61111    500      0.77747   0.55211\n  0.4   3         0.8               0.66667     50      0.80425   0.60579\n  0.4   3         0.8               0.66667    100      0.78747   0.57280\n  0.4   3         0.8               0.66667    150      0.78069   0.55909\n  0.4   3         0.8               0.66667    200      0.78391   0.56606\n  0.4   3         0.8               0.66667    250      0.77391   0.54618\n  0.4   3         0.8               0.66667    300      0.77391   0.54647\n  0.4   3         0.8               0.66667    350      0.77724   0.55284\n  0.4   3         0.8               0.66667    400      0.77724   0.55284\n  0.4   3         0.8               0.66667    450      0.78069   0.56006\n  0.4   3         0.8               0.66667    500      0.78057   0.55939\n  0.4   3         0.8               0.72222     50      0.79425   0.58505\n  0.4   3         0.8               0.72222    100      0.78747   0.57267\n  0.4   3         0.8               0.72222    150      0.78759   0.57171\n  0.4   3         0.8               0.72222    200      0.79092   0.57983\n  0.4   3         0.8               0.72222    250      0.79437   0.58642\n  0.4   3         0.8               0.72222    300      0.79092   0.57989\n  0.4   3         0.8               0.72222    350      0.79092   0.57989\n  0.4   3         0.8               0.72222    400      0.78759   0.57293\n  0.4   3         0.8               0.72222    450      0.78414   0.56690\n  0.4   3         0.8               0.72222    500      0.79092   0.57989\n  0.4   3         0.8               0.77778     50      0.78747   0.57214\n  0.4   3         0.8               0.77778    100      0.79069   0.57965\n  0.4   3         0.8               0.77778    150      0.77736   0.55268\n  0.4   3         0.8               0.77778    200      0.78080   0.55899\n  0.4   3         0.8               0.77778    250      0.78759   0.57312\n  0.4   3         0.8               0.77778    300      0.77747   0.55286\n  0.4   3         0.8               0.77778    350      0.77747   0.55286\n  0.4   3         0.8               0.77778    400      0.77402   0.54576\n  0.4   3         0.8               0.77778    450      0.77402   0.54576\n  0.4   3         0.8               0.77778    500      0.77069   0.53891\n  0.4   3         0.8               0.83333     50      0.80402   0.60625\n  0.4   3         0.8               0.83333    100      0.81425   0.62576\n  0.4   3         0.8               0.83333    150      0.79736   0.59244\n  0.4   3         0.8               0.83333    200      0.81425   0.62576\n  0.4   3         0.8               0.83333    250      0.81425   0.62645\n  0.4   3         0.8               0.83333    300      0.80736   0.61196\n  0.4   3         0.8               0.83333    350      0.80402   0.60541\n  0.4   3         0.8               0.83333    400      0.80736   0.61169\n  0.4   3         0.8               0.83333    450      0.79724   0.59247\n  0.4   3         0.8               0.83333    500      0.79724   0.59247\n  0.4   3         0.8               0.88889     50      0.79414   0.58456\n  0.4   3         0.8               0.88889    100      0.78425   0.56536\n  0.4   3         0.8               0.88889    150      0.78425   0.56494\n  0.4   3         0.8               0.88889    200      0.78092   0.55991\n  0.4   3         0.8               0.88889    250      0.78425   0.56627\n  0.4   3         0.8               0.88889    300      0.78425   0.56675\n  0.4   3         0.8               0.88889    350      0.78092   0.55991\n  0.4   3         0.8               0.88889    400      0.79092   0.57930\n  0.4   3         0.8               0.88889    450      0.78425   0.56627\n  0.4   3         0.8               0.88889    500      0.78425   0.56651\n  0.4   3         0.8               0.94444     50      0.80425   0.60542\n  0.4   3         0.8               0.94444    100      0.78402   0.56438\n  0.4   3         0.8               0.94444    150      0.78414   0.56470\n  0.4   3         0.8               0.94444    200      0.78747   0.57125\n  0.4   3         0.8               0.94444    250      0.78747   0.57173\n  0.4   3         0.8               0.94444    300      0.78747   0.57185\n  0.4   3         0.8               0.94444    350      0.78080   0.55803\n  0.4   3         0.8               0.94444    400      0.77747   0.55119\n  0.4   3         0.8               0.94444    450      0.77747   0.55119\n  0.4   3         0.8               0.94444    500      0.77747   0.55119\n  0.4   3         0.8               1.00000     50      0.78092   0.55853\n  0.4   3         0.8               1.00000    100      0.77782   0.55246\n  0.4   3         0.8               1.00000    150      0.78115   0.56031\n  0.4   3         0.8               1.00000    200      0.77770   0.55306\n  0.4   3         0.8               1.00000    250      0.78092   0.55921\n  0.4   3         0.8               1.00000    300      0.78092   0.55921\n  0.4   3         0.8               1.00000    350      0.78092   0.55921\n  0.4   3         0.8               1.00000    400      0.78092   0.55921\n  0.4   3         0.8               1.00000    450      0.78092   0.55921\n  0.4   3         0.8               1.00000    500      0.77759   0.55224\n  0.4   4         0.6               0.50000     50      0.79092   0.58104\n  0.4   4         0.6               0.50000    100      0.78069   0.55854\n  0.4   4         0.6               0.50000    150      0.78057   0.55885\n  0.4   4         0.6               0.50000    200      0.77736   0.55210\n  0.4   4         0.6               0.50000    250      0.77724   0.55169\n  0.4   4         0.6               0.50000    300      0.78747   0.57283\n  0.4   4         0.6               0.50000    350      0.77736   0.55352\n  0.4   4         0.6               0.50000    400      0.77724   0.55328\n  0.4   4         0.6               0.50000    450      0.77402   0.54649\n  0.4   4         0.6               0.50000    500      0.77402   0.54649\n  0.4   4         0.6               0.55556     50      0.79402   0.58686\n  0.4   4         0.6               0.55556    100      0.78724   0.57286\n  0.4   4         0.6               0.55556    150      0.78057   0.55890\n  0.4   4         0.6               0.55556    200      0.77724   0.55211\n  0.4   4         0.6               0.55556    250      0.77724   0.55218\n  0.4   4         0.6               0.55556    300      0.77391   0.54527\n  0.4   4         0.6               0.55556    350      0.77391   0.54527\n  0.4   4         0.6               0.55556    400      0.78057   0.55878\n  0.4   4         0.6               0.55556    450      0.77057   0.53805\n  0.4   4         0.6               0.55556    500      0.77724   0.55187\n  0.4   4         0.6               0.61111     50      0.80713   0.61272\n  0.4   4         0.6               0.61111    100      0.80057   0.59951\n  0.4   4         0.6               0.61111    150      0.80402   0.60752\n  0.4   4         0.6               0.61111    200      0.79747   0.59350\n  0.4   4         0.6               0.61111    250      0.79391   0.58617\n  0.4   4         0.6               0.61111    300      0.78379   0.56610\n  0.4   4         0.6               0.61111    350      0.78724   0.57343\n  0.4   4         0.6               0.61111    400      0.78713   0.57247\n  0.4   4         0.6               0.61111    450      0.78368   0.56638\n  0.4   4         0.6               0.61111    500      0.78379   0.56568\n  0.4   4         0.6               0.66667     50      0.79080   0.57831\n  0.4   4         0.6               0.66667    100      0.79080   0.57775\n  0.4   4         0.6               0.66667    150      0.78770   0.57204\n  0.4   4         0.6               0.66667    200      0.78080   0.55871\n  0.4   4         0.6               0.66667    250      0.79759   0.59169\n  0.4   4         0.6               0.66667    300      0.78759   0.57236\n  0.4   4         0.6               0.66667    350      0.78759   0.57194\n  0.4   4         0.6               0.66667    400      0.79092   0.57830\n  0.4   4         0.6               0.66667    450      0.78425   0.56509\n  0.4   4         0.6               0.66667    500      0.78425   0.56509\n  0.4   4         0.6               0.72222     50      0.79057   0.58013\n  0.4   4         0.6               0.72222    100      0.80402   0.60658\n  0.4   4         0.6               0.72222    150      0.79069   0.57913\n  0.4   4         0.6               0.72222    200      0.78391   0.56490\n  0.4   4         0.6               0.72222    250      0.78057   0.55842\n  0.4   4         0.6               0.72222    300      0.78391   0.56490\n  0.4   4         0.6               0.72222    350      0.78057   0.55879\n  0.4   4         0.6               0.72222    400      0.77736   0.55129\n  0.4   4         0.6               0.72222    450      0.78069   0.55850\n  0.4   4         0.6               0.72222    500      0.77391   0.54552\n  0.4   4         0.6               0.77778     50      0.79736   0.59259\n  0.4   4         0.6               0.77778    100      0.80080   0.59849\n  0.4   4         0.6               0.77778    150      0.79425   0.58469\n  0.4   4         0.6               0.77778    200      0.79747   0.59159\n  0.4   4         0.6               0.77778    250      0.80103   0.59850\n  0.4   4         0.6               0.77778    300      0.80103   0.59890\n  0.4   4         0.6               0.77778    350      0.80092   0.59944\n  0.4   4         0.6               0.77778    400      0.80092   0.59944\n  0.4   4         0.6               0.77778    450      0.79425   0.58605\n  0.4   4         0.6               0.77778    500      0.79759   0.59260\n  0.4   4         0.6               0.83333     50      0.79736   0.59310\n  0.4   4         0.6               0.83333    100      0.79057   0.58020\n  0.4   4         0.6               0.83333    150      0.79046   0.58053\n  0.4   4         0.6               0.83333    200      0.78724   0.57305\n  0.4   4         0.6               0.83333    250      0.78391   0.56663\n  0.4   4         0.6               0.83333    300      0.78391   0.56663\n  0.4   4         0.6               0.83333    350      0.78391   0.56663\n  0.4   4         0.6               0.83333    400      0.78724   0.57305\n  0.4   4         0.6               0.83333    450      0.78391   0.56651\n  0.4   4         0.6               0.83333    500      0.78391   0.56711\n  0.4   4         0.6               0.88889     50      0.78379   0.56566\n  0.4   4         0.6               0.88889    100      0.78724   0.57189\n  0.4   4         0.6               0.88889    150      0.79414   0.58518\n  0.4   4         0.6               0.88889    200      0.79402   0.58535\n  0.4   4         0.6               0.88889    250      0.79069   0.57874\n  0.4   4         0.6               0.88889    300      0.79069   0.57874\n  0.4   4         0.6               0.88889    350      0.79069   0.57874\n  0.4   4         0.6               0.88889    400      0.79069   0.57874\n  0.4   4         0.6               0.88889    450      0.78724   0.57185\n  0.4   4         0.6               0.88889    500      0.78736   0.57165\n  0.4   4         0.6               0.94444     50      0.78057   0.55802\n  0.4   4         0.6               0.94444    100      0.78736   0.57175\n  0.4   4         0.6               0.94444    150      0.79080   0.57885\n  0.4   4         0.6               0.94444    200      0.78736   0.57244\n  0.4   4         0.6               0.94444    250      0.78402   0.56565\n  0.4   4         0.6               0.94444    300      0.79069   0.57934\n  0.4   4         0.6               0.94444    350      0.78736   0.57268\n  0.4   4         0.6               0.94444    400      0.79069   0.57934\n  0.4   4         0.6               0.94444    450      0.78736   0.57268\n  0.4   4         0.6               0.94444    500      0.78402   0.56619\n  0.4   4         0.6               1.00000     50      0.80747   0.61355\n  0.4   4         0.6               1.00000    100      0.80414   0.60686\n  0.4   4         0.6               1.00000    150      0.79747   0.59363\n  0.4   4         0.6               1.00000    200      0.78747   0.57334\n  0.4   4         0.6               1.00000    250      0.78747   0.57263\n  0.4   4         0.6               1.00000    300      0.79425   0.58651\n  0.4   4         0.6               1.00000    350      0.78759   0.57294\n  0.4   4         0.6               1.00000    400      0.78759   0.57294\n  0.4   4         0.6               1.00000    450      0.78414   0.56651\n  0.4   4         0.6               1.00000    500      0.78414   0.56651\n  0.4   4         0.8               0.50000     50      0.79057   0.57855\n  0.4   4         0.8               0.50000    100      0.79414   0.58510\n  0.4   4         0.8               0.50000    150      0.79080   0.58036\n  0.4   4         0.8               0.50000    200      0.79414   0.58691\n  0.4   4         0.8               0.50000    250      0.78724   0.57296\n  0.4   4         0.8               0.50000    300      0.79414   0.58631\n  0.4   4         0.8               0.50000    350      0.79414   0.58522\n  0.4   4         0.8               0.50000    400      0.78736   0.57212\n  0.4   4         0.8               0.50000    450      0.78724   0.57235\n  0.4   4         0.8               0.50000    500      0.78402   0.56517\n  0.4   4         0.8               0.55556     50      0.79425   0.58461\n  0.4   4         0.8               0.55556    100      0.79069   0.57927\n  0.4   4         0.8               0.55556    150      0.80759   0.61344\n  0.4   4         0.8               0.55556    200      0.79759   0.59306\n  0.4   4         0.8               0.55556    250      0.80080   0.59976\n  0.4   4         0.8               0.55556    300      0.79092   0.58003\n  0.4   4         0.8               0.55556    350      0.79092   0.57984\n  0.4   4         0.8               0.55556    400      0.79092   0.58015\n  0.4   4         0.8               0.55556    450      0.79425   0.58645\n  0.4   4         0.8               0.55556    500      0.77747   0.55345\n  0.4   4         0.8               0.61111     50      0.78057   0.55804\n  0.4   4         0.8               0.61111    100      0.77034   0.53923\n  0.4   4         0.8               0.61111    150      0.77057   0.53946\n  0.4   4         0.8               0.61111    200      0.76724   0.53213\n  0.4   4         0.8               0.61111    250      0.77736   0.55286\n  0.4   4         0.8               0.61111    300      0.77391   0.54565\n  0.4   4         0.8               0.61111    350      0.77736   0.55316\n  0.4   4         0.8               0.61111    400      0.76724   0.53269\n  0.4   4         0.8               0.61111    450      0.77069   0.53952\n  0.4   4         0.8               0.61111    500      0.76046   0.51930\n  0.4   4         0.8               0.66667     50      0.80425   0.60597\n  0.4   4         0.8               0.66667    100      0.80414   0.60437\n  0.4   4         0.8               0.66667    150      0.79414   0.58502\n  0.4   4         0.8               0.66667    200      0.78414   0.56503\n  0.4   4         0.8               0.66667    250      0.79080   0.57860\n  0.4   4         0.8               0.66667    300      0.79759   0.59265\n  0.4   4         0.8               0.66667    350      0.79414   0.58520\n  0.4   4         0.8               0.66667    400      0.78747   0.57254\n  0.4   4         0.8               0.66667    450      0.79080   0.57878\n  0.4   4         0.8               0.66667    500      0.78080   0.55896\n  0.4   4         0.8               0.72222     50      0.79046   0.57877\n  0.4   4         0.8               0.72222    100      0.78391   0.56624\n  0.4   4         0.8               0.72222    150      0.79747   0.59275\n  0.4   4         0.8               0.72222    200      0.79069   0.57983\n  0.4   4         0.8               0.72222    250      0.79747   0.59275\n  0.4   4         0.8               0.72222    300      0.79080   0.57918\n  0.4   4         0.8               0.72222    350      0.78747   0.57263\n  0.4   4         0.8               0.72222    400      0.78747   0.57269\n  0.4   4         0.8               0.72222    450      0.78080   0.55912\n  0.4   4         0.8               0.72222    500      0.79092   0.57928\n  0.4   4         0.8               0.77778     50      0.78770   0.57297\n  0.4   4         0.8               0.77778    100      0.79103   0.57886\n  0.4   4         0.8               0.77778    150      0.79759   0.59233\n  0.4   4         0.8               0.77778    200      0.78425   0.56506\n  0.4   4         0.8               0.77778    250      0.79092   0.57858\n  0.4   4         0.8               0.77778    300      0.78425   0.56573\n  0.4   4         0.8               0.77778    350      0.78092   0.55918\n  0.4   4         0.8               0.77778    400      0.78092   0.55918\n  0.4   4         0.8               0.77778    450      0.76736   0.53321\n  0.4   4         0.8               0.77778    500      0.77080   0.53965\n  0.4   4         0.8               0.83333     50      0.79402   0.58541\n  0.4   4         0.8               0.83333    100      0.80414   0.60583\n  0.4   4         0.8               0.83333    150      0.79747   0.59286\n  0.4   4         0.8               0.83333    200      0.79414   0.58602\n  0.4   4         0.8               0.83333    250      0.79402   0.58564\n  0.4   4         0.8               0.83333    300      0.78736   0.57262\n  0.4   4         0.8               0.83333    350      0.79069   0.57922\n  0.4   4         0.8               0.83333    400      0.79069   0.57964\n  0.4   4         0.8               0.83333    450      0.78736   0.57262\n  0.4   4         0.8               0.83333    500      0.78402   0.56595\n  0.4   4         0.8               0.88889     50      0.78402   0.56548\n  0.4   4         0.8               0.88889    100      0.77368   0.54536\n  0.4   4         0.8               0.88889    150      0.76713   0.53109\n  0.4   4         0.8               0.88889    200      0.76391   0.52468\n  0.4   4         0.8               0.88889    250      0.76057   0.51783\n  0.4   4         0.8               0.88889    300      0.76402   0.52505\n  0.4   4         0.8               0.88889    350      0.76736   0.53159\n  0.4   4         0.8               0.88889    400      0.76402   0.52505\n  0.4   4         0.8               0.88889    450      0.76736   0.53159\n  0.4   4         0.8               0.88889    500      0.76736   0.53159\n  0.4   4         0.8               0.94444     50      0.81103   0.62038\n  0.4   4         0.8               0.94444    100      0.79414   0.58592\n  0.4   4         0.8               0.94444    150      0.80414   0.60616\n  0.4   4         0.8               0.94444    200      0.80080   0.59932\n  0.4   4         0.8               0.94444    250      0.80080   0.59932\n  0.4   4         0.8               0.94444    300      0.80080   0.59932\n  0.4   4         0.8               0.94444    350      0.79414   0.58628\n  0.4   4         0.8               0.94444    400      0.79414   0.58628\n  0.4   4         0.8               0.94444    450      0.79414   0.58628\n  0.4   4         0.8               0.94444    500      0.79414   0.58628\n  0.4   4         0.8               1.00000     50      0.79414   0.58437\n  0.4   4         0.8               1.00000    100      0.78080   0.55752\n  0.4   4         0.8               1.00000    150      0.78092   0.55834\n  0.4   4         0.8               1.00000    200      0.78080   0.55872\n  0.4   4         0.8               1.00000    250      0.79080   0.57890\n  0.4   4         0.8               1.00000    300      0.78391   0.56456\n  0.4   4         0.8               1.00000    350      0.78391   0.56456\n  0.4   4         0.8               1.00000    400      0.78736   0.57178\n  0.4   4         0.8               1.00000    450      0.78736   0.57178\n  0.4   4         0.8               1.00000    500      0.78736   0.57178\n  0.4   5         0.6               0.50000     50      0.78402   0.56585\n  0.4   5         0.6               0.50000    100      0.79713   0.59254\n  0.4   5         0.6               0.50000    150      0.78713   0.57256\n  0.4   5         0.6               0.50000    200      0.78391   0.56509\n  0.4   5         0.6               0.50000    250      0.79046   0.57917\n  0.4   5         0.6               0.50000    300      0.78379   0.56536\n  0.4   5         0.6               0.50000    350      0.78046   0.55899\n  0.4   5         0.6               0.50000    400      0.77713   0.55233\n  0.4   5         0.6               0.50000    450      0.78046   0.55857\n  0.4   5         0.6               0.50000    500      0.77713   0.55112\n  0.4   5         0.6               0.55556     50      0.78092   0.55778\n  0.4   5         0.6               0.55556    100      0.77046   0.53735\n  0.4   5         0.6               0.55556    150      0.76379   0.52439\n  0.4   5         0.6               0.55556    200      0.76713   0.53197\n  0.4   5         0.6               0.55556    250      0.76701   0.53088\n  0.4   5         0.6               0.55556    300      0.76713   0.53155\n  0.4   5         0.6               0.55556    350      0.77046   0.53821\n  0.4   5         0.6               0.55556    400      0.77391   0.54553\n  0.4   5         0.6               0.55556    450      0.76713   0.53173\n  0.4   5         0.6               0.55556    500      0.77057   0.53905\n  0.4   5         0.6               0.61111     50      0.80770   0.61230\n  0.4   5         0.6               0.61111    100      0.79747   0.59199\n  0.4   5         0.6               0.61111    150      0.78414   0.56537\n  0.4   5         0.6               0.61111    200      0.78402   0.56455\n  0.4   5         0.6               0.61111    250      0.78069   0.55813\n  0.4   5         0.6               0.61111    300      0.78069   0.55868\n  0.4   5         0.6               0.61111    350      0.78069   0.55819\n  0.4   5         0.6               0.61111    400      0.77736   0.55176\n  0.4   5         0.6               0.61111    450      0.78069   0.55813\n  0.4   5         0.6               0.61111    500      0.78069   0.55819\n  0.4   5         0.6               0.66667     50      0.80092   0.59873\n  0.4   5         0.6               0.66667    100      0.79402   0.58610\n  0.4   5         0.6               0.66667    150      0.79747   0.59336\n  0.4   5         0.6               0.66667    200      0.81092   0.61963\n  0.4   5         0.6               0.66667    250      0.80092   0.59999\n  0.4   5         0.6               0.66667    300      0.80759   0.61320\n  0.4   5         0.6               0.66667    350      0.80414   0.60676\n  0.4   5         0.6               0.66667    400      0.80414   0.60634\n  0.4   5         0.6               0.66667    450      0.80080   0.59950\n  0.4   5         0.6               0.66667    500      0.80092   0.59933\n  0.4   5         0.6               0.72222     50      0.79736   0.59192\n  0.4   5         0.6               0.72222    100      0.79437   0.58637\n  0.4   5         0.6               0.72222    150      0.79425   0.58728\n  0.4   5         0.6               0.72222    200      0.79069   0.58048\n  0.4   5         0.6               0.72222    250      0.79425   0.58770\n  0.4   5         0.6               0.72222    300      0.79736   0.59368\n  0.4   5         0.6               0.72222    350      0.78736   0.57423\n  0.4   5         0.6               0.72222    400      0.78747   0.57499\n  0.4   5         0.6               0.72222    450      0.79414   0.58820\n  0.4   5         0.6               0.72222    500      0.79069   0.58108\n  0.4   5         0.6               0.77778     50      0.78713   0.57155\n  0.4   5         0.6               0.77778    100      0.78034   0.55857\n  0.4   5         0.6               0.77778    150      0.77368   0.54591\n  0.4   5         0.6               0.77778    200      0.78724   0.57189\n  0.4   5         0.6               0.77778    250      0.78057   0.55868\n  0.4   5         0.6               0.77778    300      0.77713   0.55224\n  0.4   5         0.6               0.77778    350      0.77713   0.55234\n  0.4   5         0.6               0.77778    400      0.78046   0.55891\n  0.4   5         0.6               0.77778    450      0.77724   0.55225\n  0.4   5         0.6               0.77778    500      0.77713   0.55219\n  0.4   5         0.6               0.83333     50      0.79402   0.58543\n  0.4   5         0.6               0.83333    100      0.78736   0.57240\n  0.4   5         0.6               0.83333    150      0.79069   0.57949\n  0.4   5         0.6               0.83333    200      0.79414   0.58528\n  0.4   5         0.6               0.83333    250      0.79080   0.57874\n  0.4   5         0.6               0.83333    300      0.78747   0.57207\n  0.4   5         0.6               0.83333    350      0.79080   0.57898\n  0.4   5         0.6               0.83333    400      0.79080   0.57898\n  0.4   5         0.6               0.83333    450      0.79080   0.57898\n  0.4   5         0.6               0.83333    500      0.79414   0.58546\n  0.4   5         0.6               0.88889     50      0.80736   0.61152\n  0.4   5         0.6               0.88889    100      0.79080   0.57836\n  0.4   5         0.6               0.88889    150      0.77724   0.55129\n  0.4   5         0.6               0.88889    200      0.78057   0.55795\n  0.4   5         0.6               0.88889    250      0.78402   0.56439\n  0.4   5         0.6               0.88889    300      0.77724   0.55141\n  0.4   5         0.6               0.88889    350      0.78069   0.55861\n  0.4   5         0.6               0.88889    400      0.78402   0.56546\n  0.4   5         0.6               0.88889    450      0.78069   0.55861\n  0.4   5         0.6               0.88889    500      0.78069   0.55861\n  0.4   5         0.6               0.94444     50      0.78402   0.56606\n  0.4   5         0.6               0.94444    100      0.79425   0.58658\n  0.4   5         0.6               0.94444    150      0.79080   0.57923\n  0.4   5         0.6               0.94444    200      0.78747   0.57274\n  0.4   5         0.6               0.94444    250      0.78402   0.56631\n  0.4   5         0.6               0.94444    300      0.78747   0.57342\n  0.4   5         0.6               0.94444    350      0.78402   0.56631\n  0.4   5         0.6               0.94444    400      0.78402   0.56631\n  0.4   5         0.6               0.94444    450      0.78402   0.56631\n  0.4   5         0.6               0.94444    500      0.78747   0.57284\n  0.4   5         0.6               1.00000     50      0.79747   0.59452\n  0.4   5         0.6               1.00000    100      0.79069   0.58036\n  0.4   5         0.6               1.00000    150      0.79402   0.58643\n  0.4   5         0.6               1.00000    200      0.79736   0.59280\n  0.4   5         0.6               1.00000    250      0.79736   0.59280\n  0.4   5         0.6               1.00000    300      0.79402   0.58613\n  0.4   5         0.6               1.00000    350      0.79402   0.58613\n  0.4   5         0.6               1.00000    400      0.79402   0.58613\n  0.4   5         0.6               1.00000    450      0.79402   0.58613\n  0.4   5         0.6               1.00000    500      0.79402   0.58613\n  0.4   5         0.8               0.50000     50      0.80126   0.59761\n  0.4   5         0.8               0.50000    100      0.81471   0.62605\n  0.4   5         0.8               0.50000    150      0.80425   0.60649\n  0.4   5         0.8               0.50000    200      0.79747   0.59225\n  0.4   5         0.8               0.50000    250      0.79414   0.58534\n  0.4   5         0.8               0.50000    300      0.78759   0.57110\n  0.4   5         0.8               0.50000    350      0.79437   0.58481\n  0.4   5         0.8               0.50000    400      0.79759   0.59095\n  0.4   5         0.8               0.50000    450      0.79092   0.57833\n  0.4   5         0.8               0.50000    500      0.78425   0.56457\n  0.4   5         0.8               0.55556     50      0.79080   0.57802\n  0.4   5         0.8               0.55556    100      0.80437   0.60651\n  0.4   5         0.8               0.55556    150      0.79414   0.58646\n  0.4   5         0.8               0.55556    200      0.78759   0.57284\n  0.4   5         0.8               0.55556    250      0.79770   0.59382\n  0.4   5         0.8               0.55556    300      0.79092   0.58083\n  0.4   5         0.8               0.55556    350      0.79425   0.58720\n  0.4   5         0.8               0.55556    400      0.79092   0.58071\n  0.4   5         0.8               0.55556    450      0.78759   0.57368\n  0.4   5         0.8               0.55556    500      0.79092   0.58078\n  0.4   5         0.8               0.61111     50      0.81759   0.63292\n  0.4   5         0.8               0.61111    100      0.80402   0.60547\n  0.4   5         0.8               0.61111    150      0.80747   0.61200\n  0.4   5         0.8               0.61111    200      0.80080   0.59843\n  0.4   5         0.8               0.61111    250      0.79747   0.59182\n  0.4   5         0.8               0.61111    300      0.79747   0.59170\n  0.4   5         0.8               0.61111    350      0.79759   0.59122\n  0.4   5         0.8               0.61111    400      0.79425   0.58474\n  0.4   5         0.8               0.61111    450      0.79759   0.59122\n  0.4   5         0.8               0.61111    500      0.79759   0.59122\n  0.4   5         0.8               0.66667     50      0.80057   0.59848\n  0.4   5         0.8               0.66667    100      0.78713   0.57096\n  0.4   5         0.8               0.66667    150      0.78046   0.55690\n  0.4   5         0.8               0.66667    200      0.78713   0.57076\n  0.4   5         0.8               0.66667    250      0.78391   0.56454\n  0.4   5         0.8               0.66667    300      0.78402   0.56526\n  0.4   5         0.8               0.66667    350      0.77379   0.54430\n  0.4   5         0.8               0.66667    400      0.77724   0.55163\n  0.4   5         0.8               0.66667    450      0.77736   0.55193\n  0.4   5         0.8               0.66667    500      0.77736   0.55193\n  0.4   5         0.8               0.72222     50      0.80437   0.60464\n  0.4   5         0.8               0.72222    100      0.79092   0.57814\n  0.4   5         0.8               0.72222    150      0.79747   0.59325\n  0.4   5         0.8               0.72222    200      0.77724   0.55237\n  0.4   5         0.8               0.72222    250      0.79069   0.57979\n  0.4   5         0.8               0.72222    300      0.78736   0.57348\n  0.4   5         0.8               0.72222    350      0.79092   0.58039\n  0.4   5         0.8               0.72222    400      0.78747   0.57284\n  0.4   5         0.8               0.72222    450      0.78747   0.57337\n  0.4   5         0.8               0.72222    500      0.78414   0.56629\n  0.4   5         0.8               0.77778     50      0.79759   0.59217\n  0.4   5         0.8               0.77778    100      0.80080   0.59971\n  0.4   5         0.8               0.77778    150      0.79747   0.59304\n  0.4   5         0.8               0.77778    200      0.79080   0.57929\n  0.4   5         0.8               0.77778    250      0.78414   0.56571\n  0.4   5         0.8               0.77778    300      0.79414   0.58656\n  0.4   5         0.8               0.77778    350      0.78759   0.57275\n  0.4   5         0.8               0.77778    400      0.78759   0.57275\n  0.4   5         0.8               0.77778    450      0.78414   0.56608\n  0.4   5         0.8               0.77778    500      0.79425   0.58602\n  0.4   5         0.8               0.83333     50      0.80414   0.60693\n  0.4   5         0.8               0.83333    100      0.78736   0.57345\n  0.4   5         0.8               0.83333    150      0.79759   0.59359\n  0.4   5         0.8               0.83333    200      0.80080   0.60084\n  0.4   5         0.8               0.83333    250      0.80770   0.61380\n  0.4   5         0.8               0.83333    300      0.80437   0.60690\n  0.4   5         0.8               0.83333    350      0.80425   0.60727\n  0.4   5         0.8               0.83333    400      0.80092   0.60037\n  0.4   5         0.8               0.83333    450      0.80092   0.60037\n  0.4   5         0.8               0.83333    500      0.79759   0.59388\n  0.4   5         0.8               0.88889     50      0.79747   0.59186\n  0.4   5         0.8               0.88889    100      0.79747   0.59228\n  0.4   5         0.8               0.88889    150      0.79057   0.57801\n  0.4   5         0.8               0.88889    200      0.80069   0.59805\n  0.4   5         0.8               0.88889    250      0.79046   0.57861\n  0.4   5         0.8               0.88889    300      0.79379   0.58509\n  0.4   5         0.8               0.88889    350      0.79379   0.58546\n  0.4   5         0.8               0.88889    400      0.79046   0.57861\n  0.4   5         0.8               0.88889    450      0.79379   0.58509\n  0.4   5         0.8               0.88889    500      0.78713   0.57201\n  0.4   5         0.8               0.94444     50      0.79425   0.58574\n  0.4   5         0.8               0.94444    100      0.79103   0.57893\n  0.4   5         0.8               0.94444    150      0.79770   0.59286\n  0.4   5         0.8               0.94444    200      0.79092   0.57947\n  0.4   5         0.8               0.94444    250      0.79425   0.58596\n  0.4   5         0.8               0.94444    300      0.79759   0.59323\n  0.4   5         0.8               0.94444    350      0.79425   0.58626\n  0.4   5         0.8               0.94444    400      0.79425   0.58626\n  0.4   5         0.8               0.94444    450      0.79425   0.58626\n  0.4   5         0.8               0.94444    500      0.79425   0.58626\n  0.4   5         0.8               1.00000     50      0.79092   0.57912\n  0.4   5         0.8               1.00000    100      0.79425   0.58602\n  0.4   5         0.8               1.00000    150      0.79414   0.58599\n  0.4   5         0.8               1.00000    200      0.80092   0.59930\n  0.4   5         0.8               1.00000    250      0.79425   0.58609\n  0.4   5         0.8               1.00000    300      0.79414   0.58635\n  0.4   5         0.8               1.00000    350      0.79414   0.58635\n  0.4   5         0.8               1.00000    400      0.79414   0.58635\n  0.4   5         0.8               1.00000    450      0.79759   0.59356\n  0.4   5         0.8               1.00000    500      0.79759   0.59356\n  0.4   6         0.6               0.50000     50      0.79402   0.58272\n  0.4   6         0.6               0.50000    100      0.77368   0.54325\n  0.4   6         0.6               0.50000    150      0.79069   0.57871\n  0.4   6         0.6               0.50000    200      0.77713   0.55054\n  0.4   6         0.6               0.50000    250      0.78046   0.55781\n  0.4   6         0.6               0.50000    300      0.78391   0.56425\n  0.4   6         0.6               0.50000    350      0.77368   0.54512\n  0.4   6         0.6               0.50000    400      0.77368   0.54498\n  0.4   6         0.6               0.50000    450      0.77701   0.55171\n  0.4   6         0.6               0.50000    500      0.77034   0.53789\n  0.4   6         0.6               0.55556     50      0.79747   0.59151\n  0.4   6         0.6               0.55556    100      0.79080   0.58041\n  0.4   6         0.6               0.55556    150      0.79736   0.59340\n  0.4   6         0.6               0.55556    200      0.78391   0.56675\n  0.4   6         0.6               0.55556    250      0.78379   0.56674\n  0.4   6         0.6               0.55556    300      0.77379   0.54661\n  0.4   6         0.6               0.55556    350      0.77724   0.55248\n  0.4   6         0.6               0.55556    400      0.78391   0.56582\n  0.4   6         0.6               0.55556    450      0.77391   0.54612\n  0.4   6         0.6               0.55556    500      0.78057   0.55939\n  0.4   6         0.6               0.61111     50      0.79402   0.58592\n  0.4   6         0.6               0.61111    100      0.78391   0.56491\n  0.4   6         0.6               0.61111    150      0.79414   0.58521\n  0.4   6         0.6               0.61111    200      0.80092   0.59975\n  0.4   6         0.6               0.61111    250      0.79069   0.57898\n  0.4   6         0.6               0.61111    300      0.77736   0.55231\n  0.4   6         0.6               0.61111    350      0.79414   0.58595\n  0.4   6         0.6               0.61111    400      0.77736   0.55178\n  0.4   6         0.6               0.61111    450      0.78069   0.55880\n  0.4   6         0.6               0.61111    500      0.78069   0.55880\n  0.4   6         0.6               0.66667     50      0.81092   0.61887\n  0.4   6         0.6               0.66667    100      0.80080   0.59889\n  0.4   6         0.6               0.66667    150      0.79747   0.59295\n  0.4   6         0.6               0.66667    200      0.79080   0.57940\n  0.4   6         0.6               0.66667    250      0.78069   0.55856\n  0.4   6         0.6               0.66667    300      0.79080   0.57928\n  0.4   6         0.6               0.66667    350      0.78747   0.57267\n  0.4   6         0.6               0.66667    400      0.79414   0.58619\n  0.4   6         0.6               0.66667    450      0.79092   0.57966\n  0.4   6         0.6               0.66667    500      0.78736   0.57315\n  0.4   6         0.6               0.72222     50      0.81747   0.63327\n  0.4   6         0.6               0.72222    100      0.81080   0.61873\n  0.4   6         0.6               0.72222    150      0.81080   0.61873\n  0.4   6         0.6               0.72222    200      0.81759   0.63202\n  0.4   6         0.6               0.72222    250      0.81425   0.62640\n  0.4   6         0.6               0.72222    300      0.81080   0.61873\n  0.4   6         0.6               0.72222    350      0.81425   0.62646\n  0.4   6         0.6               0.72222    400      0.81080   0.61873\n  0.4   6         0.6               0.72222    450      0.80747   0.61218\n  0.4   6         0.6               0.72222    500      0.80747   0.61218\n  0.4   6         0.6               0.77778     50      0.76690   0.52956\n  0.4   6         0.6               0.77778    100      0.77701   0.55179\n  0.4   6         0.6               0.77778    150      0.77701   0.55179\n  0.4   6         0.6               0.77778    200      0.77023   0.53756\n  0.4   6         0.6               0.77778    250      0.77023   0.53762\n  0.4   6         0.6               0.77778    300      0.76690   0.53101\n  0.4   6         0.6               0.77778    350      0.76356   0.52441\n  0.4   6         0.6               0.77778    400      0.76034   0.51784\n  0.4   6         0.6               0.77778    450      0.76023   0.51804\n  0.4   6         0.6               0.77778    500      0.76023   0.51786\n  0.4   6         0.6               0.83333     50      0.78724   0.57171\n  0.4   6         0.6               0.83333    100      0.78736   0.57148\n  0.4   6         0.6               0.83333    150      0.79414   0.58500\n  0.4   6         0.6               0.83333    200      0.78736   0.57221\n  0.4   6         0.6               0.83333    250      0.79080   0.57924\n  0.4   6         0.6               0.83333    300      0.78069   0.55881\n  0.4   6         0.6               0.83333    350      0.78080   0.55918\n  0.4   6         0.6               0.83333    400      0.78092   0.55941\n  0.4   6         0.6               0.83333    450      0.78080   0.55903\n  0.4   6         0.6               0.83333    500      0.78092   0.55941\n  0.4   6         0.6               0.88889     50      0.78092   0.55836\n  0.4   6         0.6               0.88889    100      0.78092   0.55909\n  0.4   6         0.6               0.88889    150      0.77747   0.55266\n  0.4   6         0.6               0.88889    200      0.78747   0.57241\n  0.4   6         0.6               0.88889    250      0.78747   0.57241\n  0.4   6         0.6               0.88889    300      0.78402   0.56606\n  0.4   6         0.6               0.88889    350      0.78414   0.56566\n  0.4   6         0.6               0.88889    400      0.78069   0.55922\n  0.4   6         0.6               0.88889    450      0.78069   0.55922\n  0.4   6         0.6               0.88889    500      0.78402   0.56607\n  0.4   6         0.6               0.94444     50      0.80747   0.61188\n  0.4   6         0.6               0.94444    100      0.79425   0.58558\n  0.4   6         0.6               0.94444    150      0.78724   0.57178\n  0.4   6         0.6               0.94444    200      0.78391   0.56523\n  0.4   6         0.6               0.94444    250      0.79057   0.57862\n  0.4   6         0.6               0.94444    300      0.78391   0.56541\n  0.4   6         0.6               0.94444    350      0.78057   0.55856\n  0.4   6         0.6               0.94444    400      0.78391   0.56541\n  0.4   6         0.6               0.94444    450      0.77724   0.55202\n  0.4   6         0.6               0.94444    500      0.77391   0.54553\n  0.4   6         0.6               1.00000     50      0.81770   0.63326\n  0.4   6         0.6               1.00000    100      0.80414   0.60650\n  0.4   6         0.6               1.00000    150      0.79747   0.59323\n  0.4   6         0.6               1.00000    200      0.79747   0.59323\n  0.4   6         0.6               1.00000    250      0.79747   0.59323\n  0.4   6         0.6               1.00000    300      0.79414   0.58657\n  0.4   6         0.6               1.00000    350      0.79747   0.59348\n  0.4   6         0.6               1.00000    400      0.79747   0.59348\n  0.4   6         0.6               1.00000    450      0.79747   0.59348\n  0.4   6         0.6               1.00000    500      0.79747   0.59348\n  0.4   6         0.8               0.50000     50      0.78414   0.56600\n  0.4   6         0.8               0.50000    100      0.79402   0.58613\n  0.4   6         0.8               0.50000    150      0.78736   0.57200\n  0.4   6         0.8               0.50000    200      0.78402   0.56539\n  0.4   6         0.8               0.50000    250      0.79069   0.57897\n  0.4   6         0.8               0.50000    300      0.79069   0.57890\n  0.4   6         0.8               0.50000    350      0.78057   0.55949\n  0.4   6         0.8               0.50000    400      0.78736   0.57278\n  0.4   6         0.8               0.50000    450      0.78747   0.57225\n  0.4   6         0.8               0.50000    500      0.79092   0.57891\n  0.4   6         0.8               0.55556     50      0.82770   0.65342\n  0.4   6         0.8               0.55556    100      0.81425   0.62634\n  0.4   6         0.8               0.55556    150      0.82126   0.63884\n  0.4   6         0.8               0.55556    200      0.81437   0.62644\n  0.4   6         0.8               0.55556    250      0.80759   0.61277\n  0.4   6         0.8               0.55556    300      0.81437   0.62596\n  0.4   6         0.8               0.55556    350      0.80759   0.61289\n  0.4   6         0.8               0.55556    400      0.81103   0.61918\n  0.4   6         0.8               0.55556    450      0.81103   0.61948\n  0.4   6         0.8               0.55556    500      0.81437   0.62602\n  0.4   6         0.8               0.61111     50      0.79759   0.59307\n  0.4   6         0.8               0.61111    100      0.79759   0.59154\n  0.4   6         0.8               0.61111    150      0.78425   0.56451\n  0.4   6         0.8               0.61111    200      0.78080   0.55808\n  0.4   6         0.8               0.61111    250      0.78414   0.56591\n  0.4   6         0.8               0.61111    300      0.78092   0.55950\n  0.4   6         0.8               0.61111    350      0.78092   0.55896\n  0.4   6         0.8               0.61111    400      0.77759   0.55260\n  0.4   6         0.8               0.61111    450      0.78414   0.56584\n  0.4   6         0.8               0.61111    500      0.77736   0.55249\n  0.4   6         0.8               0.66667     50      0.79034   0.57870\n  0.4   6         0.8               0.66667    100      0.78379   0.56540\n  0.4   6         0.8               0.66667    150      0.79736   0.59251\n  0.4   6         0.8               0.66667    200      0.79402   0.58597\n  0.4   6         0.8               0.66667    250      0.79391   0.58491\n  0.4   6         0.8               0.66667    300      0.80069   0.59912\n  0.4   6         0.8               0.66667    350      0.79057   0.57932\n  0.4   6         0.8               0.66667    400      0.79414   0.58647\n  0.4   6         0.8               0.66667    450      0.80069   0.59882\n  0.4   6         0.8               0.66667    500      0.79414   0.58562\n  0.4   6         0.8               0.72222     50      0.79425   0.58633\n  0.4   6         0.8               0.72222    100      0.78414   0.56665\n  0.4   6         0.8               0.72222    150      0.78747   0.57353\n  0.4   6         0.8               0.72222    200      0.78747   0.57349\n  0.4   6         0.8               0.72222    250      0.78414   0.56640\n  0.4   6         0.8               0.72222    300      0.78747   0.57295\n  0.4   6         0.8               0.72222    350      0.79425   0.58701\n  0.4   6         0.8               0.72222    400      0.79080   0.58057\n  0.4   6         0.8               0.72222    450      0.78414   0.56689\n  0.4   6         0.8               0.72222    500      0.78080   0.56052\n  0.4   6         0.8               0.77778     50      0.80069   0.59917\n  0.4   6         0.8               0.77778    100      0.79402   0.58697\n  0.4   6         0.8               0.77778    150      0.79402   0.58708\n  0.4   6         0.8               0.77778    200      0.79069   0.58017\n  0.4   6         0.8               0.77778    250      0.79736   0.59357\n  0.4   6         0.8               0.77778    300      0.79402   0.58697\n  0.4   6         0.8               0.77778    350      0.79069   0.58030\n  0.4   6         0.8               0.77778    400      0.79069   0.58065\n  0.4   6         0.8               0.77778    450      0.78402   0.56750\n  0.4   6         0.8               0.77778    500      0.78069   0.56053\n  0.4   6         0.8               0.83333     50      0.79414   0.58606\n  0.4   6         0.8               0.83333    100      0.77736   0.55184\n  0.4   6         0.8               0.83333    150      0.78402   0.56571\n  0.4   6         0.8               0.83333    200      0.77724   0.55123\n  0.4   6         0.8               0.83333    250      0.78057   0.55814\n  0.4   6         0.8               0.83333    300      0.78069   0.55884\n  0.4   6         0.8               0.83333    350      0.78402   0.56539\n  0.4   6         0.8               0.83333    400      0.79414   0.58606\n  0.4   6         0.8               0.83333    450      0.78402   0.56602\n  0.4   6         0.8               0.83333    500      0.79057   0.57898\n  0.4   6         0.8               0.88889     50      0.79747   0.59084\n  0.4   6         0.8               0.88889    100      0.78057   0.55820\n  0.4   6         0.8               0.88889    150      0.78724   0.57135\n  0.4   6         0.8               0.88889    200      0.78391   0.56474\n  0.4   6         0.8               0.88889    250      0.78713   0.57182\n  0.4   6         0.8               0.88889    300      0.79402   0.58547\n  0.4   6         0.8               0.88889    350      0.79391   0.58605\n  0.4   6         0.8               0.88889    400      0.79057   0.57945\n  0.4   6         0.8               0.88889    450      0.79057   0.57945\n  0.4   6         0.8               0.88889    500      0.79057   0.57951\n  0.4   6         0.8               0.94444     50      0.81770   0.63257\n  0.4   6         0.8               0.94444    100      0.80092   0.59855\n  0.4   6         0.8               0.94444    150      0.80759   0.61207\n  0.4   6         0.8               0.94444    200      0.80425   0.60546\n  0.4   6         0.8               0.94444    250      0.79747   0.59218\n  0.4   6         0.8               0.94444    300      0.79747   0.59218\n  0.4   6         0.8               0.94444    350      0.79414   0.58576\n  0.4   6         0.8               0.94444    400      0.79080   0.57873\n  0.4   6         0.8               0.94444    450      0.78747   0.57218\n  0.4   6         0.8               0.94444    500      0.79414   0.58582\n  0.4   6         0.8               1.00000     50      0.78069   0.55947\n  0.4   6         0.8               1.00000    100      0.78759   0.57241\n  0.4   6         0.8               1.00000    150      0.78770   0.57263\n  0.4   6         0.8               1.00000    200      0.78770   0.57263\n  0.4   6         0.8               1.00000    250      0.78770   0.57263\n  0.4   6         0.8               1.00000    300      0.78425   0.56551\n  0.4   6         0.8               1.00000    350      0.78759   0.57242\n  0.4   6         0.8               1.00000    400      0.78759   0.57242\n  0.4   6         0.8               1.00000    450      0.78770   0.57287\n  0.4   6         0.8               1.00000    500      0.78770   0.57287\n  0.4   7         0.6               0.50000     50      0.81770   0.63418\n  0.4   7         0.6               0.50000    100      0.79770   0.59262\n  0.4   7         0.6               0.50000    150      0.80092   0.59836\n  0.4   7         0.6               0.50000    200      0.81115   0.61899\n  0.4   7         0.6               0.50000    250      0.80092   0.59871\n  0.4   7         0.6               0.50000    300      0.79425   0.58578\n  0.4   7         0.6               0.50000    350      0.79437   0.58537\n  0.4   7         0.6               0.50000    400      0.78414   0.56490\n  0.4   7         0.6               0.50000    450      0.78080   0.55781\n  0.4   7         0.6               0.50000    500      0.79080   0.57853\n  0.4   7         0.6               0.55556     50      0.80437   0.60553\n  0.4   7         0.6               0.55556    100      0.78414   0.56446\n  0.4   7         0.6               0.55556    150      0.80747   0.61252\n  0.4   7         0.6               0.55556    200      0.79402   0.58519\n  0.4   7         0.6               0.55556    250      0.80414   0.60572\n  0.4   7         0.6               0.55556    300      0.78724   0.57216\n  0.4   7         0.6               0.55556    350      0.76713   0.53174\n  0.4   7         0.6               0.55556    400      0.77713   0.55186\n  0.4   7         0.6               0.55556    450      0.77034   0.53882\n  0.4   7         0.6               0.55556    500      0.76701   0.53221\n  0.4   7         0.6               0.61111     50      0.81460   0.62590\n  0.4   7         0.6               0.61111    100      0.80770   0.61326\n  0.4   7         0.6               0.61111    150      0.79770   0.59332\n  0.4   7         0.6               0.61111    200      0.79414   0.58597\n  0.4   7         0.6               0.61111    250      0.79425   0.58541\n  0.4   7         0.6               0.61111    300      0.79425   0.58511\n  0.4   7         0.6               0.61111    350      0.79080   0.57838\n  0.4   7         0.6               0.61111    400      0.79080   0.57831\n  0.4   7         0.6               0.61111    450      0.79080   0.57814\n  0.4   7         0.6               0.61111    500      0.78414   0.56499\n  0.4   7         0.6               0.66667     50      0.79402   0.58530\n  0.4   7         0.6               0.66667    100      0.80057   0.59800\n  0.4   7         0.6               0.66667    150      0.80069   0.59843\n  0.4   7         0.6               0.66667    200      0.80080   0.59832\n  0.4   7         0.6               0.66667    250      0.79414   0.58487\n  0.4   7         0.6               0.66667    300      0.79080   0.57845\n  0.4   7         0.6               0.66667    350      0.79080   0.57845\n  0.4   7         0.6               0.66667    400      0.79069   0.57861\n  0.4   7         0.6               0.66667    450      0.79069   0.57855\n  0.4   7         0.6               0.66667    500      0.78736   0.57201\n  0.4   7         0.6               0.72222     50      0.78402   0.56524\n  0.4   7         0.6               0.72222    100      0.76391   0.52426\n  0.4   7         0.6               0.72222    150      0.78057   0.55795\n  0.4   7         0.6               0.72222    200      0.77379   0.54549\n  0.4   7         0.6               0.72222    250      0.77368   0.54581\n  0.4   7         0.6               0.72222    300      0.78736   0.57136\n  0.4   7         0.6               0.72222    350      0.77724   0.55135\n  0.4   7         0.6               0.72222    400      0.77046   0.53852\n  0.4   7         0.6               0.72222    450      0.77391   0.54483\n  0.4   7         0.6               0.72222    500      0.77057   0.53826\n  0.4   7         0.6               0.77778     50      0.80414   0.60526\n  0.4   7         0.6               0.77778    100      0.80092   0.59938\n  0.4   7         0.6               0.77778    150      0.80092   0.59878\n  0.4   7         0.6               0.77778    200      0.80747   0.61282\n  0.4   7         0.6               0.77778    250      0.80080   0.59912\n  0.4   7         0.6               0.77778    300      0.80414   0.60621\n  0.4   7         0.6               0.77778    350      0.80770   0.61260\n  0.4   7         0.6               0.77778    400      0.80437   0.60551\n  0.4   7         0.6               0.77778    450      0.80092   0.59975\n  0.4   7         0.6               0.77778    500      0.79759   0.59266\n  0.4   7         0.6               0.83333     50      0.80782   0.61277\n  0.4   7         0.6               0.83333    100      0.79747   0.59286\n  0.4   7         0.6               0.83333    150      0.78402   0.56589\n  0.4   7         0.6               0.83333    200      0.78057   0.55900\n  0.4   7         0.6               0.83333    250      0.78414   0.56603\n  0.4   7         0.6               0.83333    300      0.79425   0.58652\n  0.4   7         0.6               0.83333    350      0.79425   0.58652\n  0.4   7         0.6               0.83333    400      0.78425   0.56652\n  0.4   7         0.6               0.83333    450      0.78425   0.56634\n  0.4   7         0.6               0.83333    500      0.78425   0.56694\n  0.4   7         0.6               0.88889     50      0.80391   0.60390\n  0.4   7         0.6               0.88889    100      0.80391   0.60546\n  0.4   7         0.6               0.88889    150      0.80057   0.59862\n  0.4   7         0.6               0.88889    200      0.80057   0.59862\n  0.4   7         0.6               0.88889    250      0.80069   0.59872\n  0.4   7         0.6               0.88889    300      0.79402   0.58503\n  0.4   7         0.6               0.88889    350      0.79402   0.58503\n  0.4   7         0.6               0.88889    400      0.79402   0.58503\n  0.4   7         0.6               0.88889    450      0.79747   0.59165\n  0.4   7         0.6               0.88889    500      0.79402   0.58503\n  0.4   7         0.6               0.94444     50      0.79402   0.58613\n  0.4   7         0.6               0.94444    100      0.79736   0.59250\n  0.4   7         0.6               0.94444    150      0.79391   0.58584\n  0.4   7         0.6               0.94444    200      0.78379   0.56579\n  0.4   7         0.6               0.94444    250      0.78713   0.57281\n  0.4   7         0.6               0.94444    300      0.78379   0.56596\n  0.4   7         0.6               0.94444    350      0.79057   0.57965\n  0.4   7         0.6               0.94444    400      0.78391   0.56607\n  0.4   7         0.6               0.94444    450      0.79057   0.57965\n  0.4   7         0.6               0.94444    500      0.79057   0.57965\n  0.4   7         0.6               1.00000     50      0.81080   0.62094\n  0.4   7         0.6               1.00000    100      0.80770   0.61396\n  0.4   7         0.6               1.00000    150      0.80425   0.60702\n  0.4   7         0.6               1.00000    200      0.79425   0.58732\n  0.4   7         0.6               1.00000    250      0.79759   0.59416\n  0.4   7         0.6               1.00000    300      0.79080   0.58081\n  0.4   7         0.6               1.00000    350      0.79425   0.58792\n  0.4   7         0.6               1.00000    400      0.79425   0.58792\n  0.4   7         0.6               1.00000    450      0.79425   0.58792\n  0.4   7         0.6               1.00000    500      0.79092   0.58083\n  0.4   7         0.8               0.50000     50      0.79736   0.59175\n  0.4   7         0.8               0.50000    100      0.80759   0.61236\n  0.4   7         0.8               0.50000    150      0.80759   0.61299\n  0.4   7         0.8               0.50000    200      0.80759   0.61352\n  0.4   7         0.8               0.50000    250      0.79770   0.59185\n  0.4   7         0.8               0.50000    300      0.79092   0.57893\n  0.4   7         0.8               0.50000    350      0.79736   0.59217\n  0.4   7         0.8               0.50000    400      0.79080   0.57921\n  0.4   7         0.8               0.50000    450      0.78391   0.56490\n  0.4   7         0.8               0.50000    500      0.78747   0.57195\n  0.4   7         0.8               0.55556     50      0.79747   0.59192\n  0.4   7         0.8               0.55556    100      0.80414   0.60476\n  0.4   7         0.8               0.55556    150      0.79724   0.59127\n  0.4   7         0.8               0.55556    200      0.80736   0.61189\n  0.4   7         0.8               0.55556    250      0.78736   0.57153\n  0.4   7         0.8               0.55556    300      0.78736   0.57123\n  0.4   7         0.8               0.55556    350      0.78724   0.57185\n  0.4   7         0.8               0.55556    400      0.78046   0.55875\n  0.4   7         0.8               0.55556    450      0.77379   0.54536\n  0.4   7         0.8               0.55556    500      0.77057   0.53780\n  0.4   7         0.8               0.61111     50      0.80425   0.60711\n  0.4   7         0.8               0.61111    100      0.80425   0.60692\n  0.4   7         0.8               0.61111    150      0.80437   0.60682\n  0.4   7         0.8               0.61111    200      0.79759   0.59205\n  0.4   7         0.8               0.61111    250      0.79759   0.59253\n  0.4   7         0.8               0.61111    300      0.79759   0.59332\n  0.4   7         0.8               0.61111    350      0.79425   0.58604\n  0.4   7         0.8               0.61111    400      0.79103   0.58058\n  0.4   7         0.8               0.61111    450      0.79092   0.58034\n  0.4   7         0.8               0.61111    500      0.78425   0.56694\n  0.4   7         0.8               0.66667     50      0.78747   0.57199\n  0.4   7         0.8               0.66667    100      0.78069   0.55777\n  0.4   7         0.8               0.66667    150      0.78069   0.55849\n  0.4   7         0.8               0.66667    200      0.78069   0.55837\n  0.4   7         0.8               0.66667    250      0.77736   0.55194\n  0.4   7         0.8               0.66667    300      0.78402   0.56582\n  0.4   7         0.8               0.66667    350      0.78069   0.55939\n  0.4   7         0.8               0.66667    400      0.77069   0.53879\n  0.4   7         0.8               0.66667    450      0.77747   0.55198\n  0.4   7         0.8               0.66667    500      0.78080   0.55883\n  0.4   7         0.8               0.72222     50      0.79092   0.57865\n  0.4   7         0.8               0.72222    100      0.79770   0.59295\n  0.4   7         0.8               0.72222    150      0.79092   0.57982\n  0.4   7         0.8               0.72222    200      0.78770   0.57288\n  0.4   7         0.8               0.72222    250      0.78080   0.55973\n  0.4   7         0.8               0.72222    300      0.78080   0.55982\n  0.4   7         0.8               0.72222    350      0.78759   0.57322\n  0.4   7         0.8               0.72222    400      0.78759   0.57322\n  0.4   7         0.8               0.72222    450      0.78069   0.55957\n  0.4   7         0.8               0.72222    500      0.78080   0.55979\n  0.4   7         0.8               0.77778     50      0.78425   0.56557\n  0.4   7         0.8               0.77778    100      0.78414   0.56514\n  0.4   7         0.8               0.77778    150      0.78759   0.57189\n  0.4   7         0.8               0.77778    200      0.78080   0.55960\n  0.4   7         0.8               0.77778    250      0.78080   0.55960\n  0.4   7         0.8               0.77778    300      0.78759   0.57287\n  0.4   7         0.8               0.77778    350      0.78425   0.56555\n  0.4   7         0.8               0.77778    400      0.78414   0.56609\n  0.4   7         0.8               0.77778    450      0.78092   0.55947\n  0.4   7         0.8               0.77778    500      0.78092   0.56012\n  0.4   7         0.8               0.83333     50      0.79414   0.58594\n  0.4   7         0.8               0.83333    100      0.79414   0.58571\n  0.4   7         0.8               0.83333    150      0.79414   0.58575\n  0.4   7         0.8               0.83333    200      0.79402   0.58651\n  0.4   7         0.8               0.83333    250      0.79402   0.58639\n  0.4   7         0.8               0.83333    300      0.79747   0.59360\n  0.4   7         0.8               0.83333    350      0.78747   0.57424\n  0.4   7         0.8               0.83333    400      0.78747   0.57419\n  0.4   7         0.8               0.83333    450      0.78402   0.56687\n  0.4   7         0.8               0.83333    500      0.78069   0.56038\n  0.4   7         0.8               0.88889     50      0.79747   0.59414\n  0.4   7         0.8               0.88889    100      0.80759   0.61329\n  0.4   7         0.8               0.88889    150      0.81092   0.62038\n  0.4   7         0.8               0.88889    200      0.80402   0.60595\n  0.4   7         0.8               0.88889    250      0.80080   0.60022\n  0.4   7         0.8               0.88889    300      0.79057   0.57993\n  0.4   7         0.8               0.88889    350      0.79402   0.58577\n  0.4   7         0.8               0.88889    400      0.79391   0.58643\n  0.4   7         0.8               0.88889    450      0.79736   0.59405\n  0.4   7         0.8               0.88889    500      0.79402   0.58726\n  0.4   7         0.8               0.94444     50      0.79770   0.59274\n  0.4   7         0.8               0.94444    100      0.79092   0.57871\n  0.4   7         0.8               0.94444    150      0.79080   0.57931\n  0.4   7         0.8               0.94444    200      0.79080   0.57931\n  0.4   7         0.8               0.94444    250      0.79414   0.58628\n  0.4   7         0.8               0.94444    300      0.79414   0.58659\n  0.4   7         0.8               0.94444    350      0.78414   0.56628\n  0.4   7         0.8               0.94444    400      0.79080   0.58016\n  0.4   7         0.8               0.94444    450      0.78759   0.57356\n  0.4   7         0.8               0.94444    500      0.78759   0.57356\n  0.4   7         0.8               1.00000     50      0.80103   0.59987\n  0.4   7         0.8               1.00000    100      0.78759   0.57298\n  0.4   7         0.8               1.00000    150      0.78759   0.57298\n  0.4   7         0.8               1.00000    200      0.79092   0.57994\n  0.4   7         0.8               1.00000    250      0.78759   0.57298\n  0.4   7         0.8               1.00000    300      0.78414   0.56586\n  0.4   7         0.8               1.00000    350      0.78414   0.56586\n  0.4   7         0.8               1.00000    400      0.79080   0.57973\n  0.4   7         0.8               1.00000    450      0.79080   0.57973\n  0.4   7         0.8               1.00000    500      0.79425   0.58695\n  0.4   8         0.6               0.50000     50      0.81425   0.62488\n  0.4   8         0.6               0.50000    100      0.80414   0.60416\n  0.4   8         0.6               0.50000    150      0.80092   0.60057\n  0.4   8         0.6               0.50000    200      0.80092   0.60024\n  0.4   8         0.6               0.50000    250      0.79759   0.59363\n  0.4   8         0.6               0.50000    300      0.79080   0.57917\n  0.4   8         0.6               0.50000    350      0.79069   0.57845\n  0.4   8         0.6               0.50000    400      0.78391   0.56547\n  0.4   8         0.6               0.50000    450      0.78069   0.55904\n  0.4   8         0.6               0.50000    500      0.79425   0.58618\n  0.4   8         0.6               0.55556     50      0.79057   0.57866\n  0.4   8         0.6               0.55556    100      0.76701   0.53105\n  0.4   8         0.6               0.55556    150      0.77701   0.55202\n  0.4   8         0.6               0.55556    200      0.76034   0.51802\n  0.4   8         0.6               0.55556    250      0.77046   0.53888\n  0.4   8         0.6               0.55556    300      0.76379   0.52506\n  0.4   8         0.6               0.55556    350      0.77057   0.53893\n  0.4   8         0.6               0.55556    400      0.77057   0.53862\n  0.4   8         0.6               0.55556    450      0.77057   0.53862\n  0.4   8         0.6               0.55556    500      0.77057   0.53893\n  0.4   8         0.6               0.61111     50      0.79115   0.57977\n  0.4   8         0.6               0.61111    100      0.80437   0.60643\n  0.4   8         0.6               0.61111    150      0.79103   0.57870\n  0.4   8         0.6               0.61111    200      0.79759   0.59252\n  0.4   8         0.6               0.61111    250      0.79437   0.58598\n  0.4   8         0.6               0.61111    300      0.79437   0.58598\n  0.4   8         0.6               0.61111    350      0.79092   0.57955\n  0.4   8         0.6               0.61111    400      0.78759   0.57306\n  0.4   8         0.6               0.61111    450      0.78770   0.57306\n  0.4   8         0.6               0.61111    500      0.79103   0.58027\n  0.4   8         0.6               0.66667     50      0.82115   0.63847\n  0.4   8         0.6               0.66667    100      0.81057   0.61735\n  0.4   8         0.6               0.66667    150      0.79713   0.59086\n  0.4   8         0.6               0.66667    200      0.81069   0.61808\n  0.4   8         0.6               0.66667    250      0.80069   0.59778\n  0.4   8         0.6               0.66667    300      0.80069   0.59741\n  0.4   8         0.6               0.66667    350      0.79069   0.57856\n  0.4   8         0.6               0.66667    400      0.79057   0.57874\n  0.4   8         0.6               0.66667    450      0.79414   0.58577\n  0.4   8         0.6               0.66667    500      0.79080   0.57893\n  0.4   8         0.6               0.72222     50      0.80402   0.60408\n  0.4   8         0.6               0.72222    100      0.79736   0.59117\n  0.4   8         0.6               0.72222    150      0.79724   0.59170\n  0.4   8         0.6               0.72222    200      0.80069   0.59814\n  0.4   8         0.6               0.72222    250      0.79391   0.58485\n  0.4   8         0.6               0.72222    300      0.78724   0.57152\n  0.4   8         0.6               0.72222    350      0.79747   0.59202\n  0.4   8         0.6               0.72222    400      0.78724   0.57152\n  0.4   8         0.6               0.72222    450      0.78724   0.57188\n  0.4   8         0.6               0.72222    500      0.79069   0.57832\n  0.4   8         0.6               0.77778     50      0.80759   0.61229\n  0.4   8         0.6               0.77778    100      0.80747   0.61227\n  0.4   8         0.6               0.77778    150      0.80724   0.61324\n  0.4   8         0.6               0.77778    200      0.79736   0.59292\n  0.4   8         0.6               0.77778    250      0.79402   0.58576\n  0.4   8         0.6               0.77778    300      0.79069   0.57928\n  0.4   8         0.6               0.77778    350      0.79057   0.57976\n  0.4   8         0.6               0.77778    400      0.79747   0.59322\n  0.4   8         0.6               0.77778    450      0.79747   0.59357\n  0.4   8         0.6               0.77778    500      0.78747   0.57411\n  0.4   8         0.6               0.83333     50      0.80080   0.60069\n  0.4   8         0.6               0.83333    100      0.79414   0.58670\n  0.4   8         0.6               0.83333    150      0.79092   0.58016\n  0.4   8         0.6               0.83333    200      0.80092   0.60089\n  0.4   8         0.6               0.83333    250      0.79759   0.59392\n  0.4   8         0.6               0.83333    300      0.79092   0.58022\n  0.4   8         0.6               0.83333    350      0.78759   0.57320\n  0.4   8         0.6               0.83333    400      0.78425   0.56618\n  0.4   8         0.6               0.83333    450      0.78414   0.56582\n  0.4   8         0.6               0.83333    500      0.78080   0.55897\n  0.4   8         0.6               0.88889     50      0.80759   0.61094\n  0.4   8         0.6               0.88889    100      0.79402   0.58456\n  0.4   8         0.6               0.88889    150      0.79747   0.59134\n  0.4   8         0.6               0.88889    200      0.79069   0.57842\n  0.4   8         0.6               0.88889    250      0.79069   0.57842\n  0.4   8         0.6               0.88889    300      0.79069   0.57842\n  0.4   8         0.6               0.88889    350      0.78736   0.57175\n  0.4   8         0.6               0.88889    400      0.78736   0.57175\n  0.4   8         0.6               0.88889    450      0.79069   0.57824\n  0.4   8         0.6               0.88889    500      0.79069   0.57781\n  0.4   8         0.6               0.94444     50      0.80793   0.61406\n  0.4   8         0.6               0.94444    100      0.79437   0.58718\n  0.4   8         0.6               0.94444    150      0.79770   0.59430\n  0.4   8         0.6               0.94444    200      0.79437   0.58799\n  0.4   8         0.6               0.94444    250      0.79103   0.58097\n  0.4   8         0.6               0.94444    300      0.78759   0.57385\n  0.4   8         0.6               0.94444    350      0.78759   0.57385\n  0.4   8         0.6               0.94444    400      0.78759   0.57385\n  0.4   8         0.6               0.94444    450      0.78770   0.57430\n  0.4   8         0.6               0.94444    500      0.78437   0.56781\n  0.4   8         0.6               1.00000     50      0.79080   0.57907\n  0.4   8         0.6               1.00000    100      0.79069   0.57901\n  0.4   8         0.6               1.00000    150      0.77747   0.55308\n  0.4   8         0.6               1.00000    200      0.78080   0.55974\n  0.4   8         0.6               1.00000    250      0.77069   0.53878\n  0.4   8         0.6               1.00000    300      0.76724   0.53147\n  0.4   8         0.6               1.00000    350      0.76724   0.53147\n  0.4   8         0.6               1.00000    400      0.77402   0.54577\n  0.4   8         0.6               1.00000    450      0.77402   0.54577\n  0.4   8         0.6               1.00000    500      0.77402   0.54577\n  0.4   8         0.8               0.50000     50      0.78402   0.56549\n  0.4   8         0.8               0.50000    100      0.77747   0.55272\n  0.4   8         0.8               0.50000    150      0.77425   0.54603\n  0.4   8         0.8               0.50000    200      0.79080   0.58028\n  0.4   8         0.8               0.50000    250      0.78414   0.56666\n  0.4   8         0.8               0.50000    300      0.78069   0.56004\n  0.4   8         0.8               0.50000    350      0.77414   0.54677\n  0.4   8         0.8               0.50000    400      0.77080   0.53943\n  0.4   8         0.8               0.50000    450      0.77080   0.54011\n  0.4   8         0.8               0.50000    500      0.77414   0.54671\n  0.4   8         0.8               0.55556     50      0.78437   0.56648\n  0.4   8         0.8               0.55556    100      0.77080   0.54022\n  0.4   8         0.8               0.55556    150      0.77736   0.55352\n  0.4   8         0.8               0.55556    200      0.77736   0.55424\n  0.4   8         0.8               0.55556    250      0.78402   0.56710\n  0.4   8         0.8               0.55556    300      0.77736   0.55301\n  0.4   8         0.8               0.55556    350      0.78069   0.55932\n  0.4   8         0.8               0.55556    400      0.77379   0.54701\n  0.4   8         0.8               0.55556    450      0.76701   0.53279\n  0.4   8         0.8               0.55556    500      0.77046   0.54001\n  0.4   8         0.8               0.61111     50      0.81425   0.62615\n  0.4   8         0.8               0.61111    100      0.81770   0.63216\n  0.4   8         0.8               0.61111    150      0.81759   0.63264\n  0.4   8         0.8               0.61111    200      0.80747   0.61272\n  0.4   8         0.8               0.61111    250      0.80069   0.59842\n  0.4   8         0.8               0.61111    300      0.81425   0.62541\n  0.4   8         0.8               0.61111    350      0.80747   0.61271\n  0.4   8         0.8               0.61111    400      0.81103   0.61895\n  0.4   8         0.8               0.61111    450      0.80759   0.61270\n  0.4   8         0.8               0.61111    500      0.80103   0.59926\n  0.4   8         0.8               0.66667     50      0.80414   0.60578\n  0.4   8         0.8               0.66667    100      0.79391   0.58534\n  0.4   8         0.8               0.66667    150      0.79747   0.59251\n  0.4   8         0.8               0.66667    200      0.79414   0.58662\n  0.4   8         0.8               0.66667    250      0.79747   0.59305\n  0.4   8         0.8               0.66667    300      0.79747   0.59305\n  0.4   8         0.8               0.66667    350      0.79747   0.59305\n  0.4   8         0.8               0.66667    400      0.79080   0.58002\n  0.4   8         0.8               0.66667    450      0.79414   0.58644\n  0.4   8         0.8               0.66667    500      0.79080   0.58002\n  0.4   8         0.8               0.72222     50      0.80425   0.60636\n  0.4   8         0.8               0.72222    100      0.79736   0.59135\n  0.4   8         0.8               0.72222    150      0.79057   0.57825\n  0.4   8         0.8               0.72222    200      0.79057   0.57776\n  0.4   8         0.8               0.72222    250      0.79402   0.58509\n  0.4   8         0.8               0.72222    300      0.78391   0.56498\n  0.4   8         0.8               0.72222    350      0.79069   0.57771\n  0.4   8         0.8               0.72222    400      0.78391   0.56539\n  0.4   8         0.8               0.72222    450      0.78736   0.57189\n  0.4   8         0.8               0.72222    500      0.78736   0.57146\n  0.4   8         0.8               0.77778     50      0.80057   0.59836\n  0.4   8         0.8               0.77778    100      0.79724   0.59272\n  0.4   8         0.8               0.77778    150      0.80046   0.59900\n  0.4   8         0.8               0.77778    200      0.79724   0.59174\n  0.4   8         0.8               0.77778    250      0.79736   0.59218\n  0.4   8         0.8               0.77778    300      0.79736   0.59279\n  0.4   8         0.8               0.77778    350      0.80069   0.59970\n  0.4   8         0.8               0.77778    400      0.79736   0.59267\n  0.4   8         0.8               0.77778    450      0.79402   0.58588\n  0.4   8         0.8               0.77778    500      0.79069   0.57921\n  0.4   8         0.8               0.83333     50      0.79414   0.58647\n  0.4   8         0.8               0.83333    100      0.79414   0.58587\n  0.4   8         0.8               0.83333    150      0.79747   0.59206\n  0.4   8         0.8               0.83333    200      0.79414   0.58570\n  0.4   8         0.8               0.83333    250      0.79402   0.58563\n  0.4   8         0.8               0.83333    300      0.79747   0.59206\n  0.4   8         0.8               0.83333    350      0.79069   0.57926\n  0.4   8         0.8               0.83333    400      0.79747   0.59206\n  0.4   8         0.8               0.83333    450      0.79080   0.57988\n  0.4   8         0.8               0.83333    500      0.78747   0.57339\n  0.4   8         0.8               0.88889     50      0.79402   0.58475\n  0.4   8         0.8               0.88889    100      0.80103   0.59854\n  0.4   8         0.8               0.88889    150      0.79770   0.59218\n  0.4   8         0.8               0.88889    200      0.78425   0.56426\n  0.4   8         0.8               0.88889    250      0.78092   0.55790\n  0.4   8         0.8               0.88889    300      0.78759   0.57189\n  0.4   8         0.8               0.88889    350      0.79092   0.57850\n  0.4   8         0.8               0.88889    400      0.79092   0.57886\n  0.4   8         0.8               0.88889    450      0.79747   0.59243\n  0.4   8         0.8               0.88889    500      0.79414   0.58566\n  0.4   8         0.8               0.94444     50      0.78736   0.57252\n  0.4   8         0.8               0.94444    100      0.78057   0.55874\n  0.4   8         0.8               0.94444    150      0.77724   0.55225\n  0.4   8         0.8               0.94444    200      0.77724   0.55225\n  0.4   8         0.8               0.94444    250      0.77391   0.54588\n  0.4   8         0.8               0.94444    300      0.77724   0.55285\n  0.4   8         0.8               0.94444    350      0.78057   0.55922\n  0.4   8         0.8               0.94444    400      0.78057   0.55922\n  0.4   8         0.8               0.94444    450      0.78736   0.57346\n  0.4   8         0.8               0.94444    500      0.79069   0.57983\n  0.4   8         0.8               1.00000     50      0.78391   0.56599\n  0.4   8         0.8               1.00000    100      0.78080   0.55882\n  0.4   8         0.8               1.00000    150      0.78080   0.55911\n  0.4   8         0.8               1.00000    200      0.78747   0.57299\n  0.4   8         0.8               1.00000    250      0.79103   0.57918\n  0.4   8         0.8               1.00000    300      0.78770   0.57286\n  0.4   8         0.8               1.00000    350      0.79103   0.57946\n  0.4   8         0.8               1.00000    400      0.79103   0.57946\n  0.4   8         0.8               1.00000    450      0.79448   0.58668\n  0.4   8         0.8               1.00000    500      0.79103   0.58025\n  0.4   9         0.6               0.50000     50      0.79782   0.59251\n  0.4   9         0.6               0.50000    100      0.80793   0.61194\n  0.4   9         0.6               0.50000    150      0.80448   0.60663\n  0.4   9         0.6               0.50000    200      0.79126   0.57943\n  0.4   9         0.6               0.50000    250      0.79115   0.58033\n  0.4   9         0.6               0.50000    300      0.79793   0.59331\n  0.4   9         0.6               0.50000    350      0.78437   0.56624\n  0.4   9         0.6               0.50000    400      0.79103   0.57969\n  0.4   9         0.6               0.50000    450      0.79115   0.57996\n  0.4   9         0.6               0.50000    500      0.78103   0.55993\n  0.4   9         0.6               0.55556     50      0.80080   0.60063\n  0.4   9         0.6               0.55556    100      0.78069   0.56033\n  0.4   9         0.6               0.55556    150      0.79092   0.58130\n  0.4   9         0.6               0.55556    200      0.80448   0.60785\n  0.4   9         0.6               0.55556    250      0.79115   0.58160\n  0.4   9         0.6               0.55556    300      0.80448   0.60785\n  0.4   9         0.6               0.55556    350      0.79770   0.59507\n  0.4   9         0.6               0.55556    400      0.79425   0.58797\n  0.4   9         0.6               0.55556    450      0.78080   0.56066\n  0.4   9         0.6               0.55556    500      0.78414   0.56737\n  0.4   9         0.6               0.61111     50      0.80747   0.61298\n  0.4   9         0.6               0.61111    100      0.79402   0.58575\n  0.4   9         0.6               0.61111    150      0.78724   0.57291\n  0.4   9         0.6               0.61111    200      0.79080   0.57950\n  0.4   9         0.6               0.61111    250      0.78736   0.57229\n  0.4   9         0.6               0.61111    300      0.78724   0.57278\n  0.4   9         0.6               0.61111    350      0.78391   0.56545\n  0.4   9         0.6               0.61111    400      0.78402   0.56568\n  0.4   9         0.6               0.61111    450      0.78402   0.56537\n  0.4   9         0.6               0.61111    500      0.77736   0.55265\n  0.4   9         0.6               0.66667     50      0.79448   0.58467\n  0.4   9         0.6               0.66667    100      0.80092   0.59813\n  0.4   9         0.6               0.66667    150      0.81092   0.61855\n  0.4   9         0.6               0.66667    200      0.80092   0.59910\n  0.4   9         0.6               0.66667    250      0.80437   0.60544\n  0.4   9         0.6               0.66667    300      0.80092   0.59849\n  0.4   9         0.6               0.66667    350      0.78759   0.57194\n  0.4   9         0.6               0.66667    400      0.78425   0.56485\n  0.4   9         0.6               0.66667    450      0.79092   0.57855\n  0.4   9         0.6               0.66667    500      0.78759   0.57194\n  0.4   9         0.6               0.72222     50      0.80069   0.59902\n  0.4   9         0.6               0.72222    100      0.80069   0.59956\n  0.4   9         0.6               0.72222    150      0.79747   0.59283\n  0.4   9         0.6               0.72222    200      0.79080   0.57944\n  0.4   9         0.6               0.72222    250      0.79747   0.59283\n  0.4   9         0.6               0.72222    300      0.79057   0.57917\n  0.4   9         0.6               0.72222    350      0.78724   0.57280\n  0.4   9         0.6               0.72222    400      0.78736   0.57284\n  0.4   9         0.6               0.72222    450      0.78402   0.56599\n  0.4   9         0.6               0.72222    500      0.78057   0.55964\n  0.4   9         0.6               0.77778     50      0.79448   0.58748\n  0.4   9         0.6               0.77778    100      0.78747   0.57397\n  0.4   9         0.6               0.77778    150      0.79080   0.58027\n  0.4   9         0.6               0.77778    200      0.78402   0.56775\n  0.4   9         0.6               0.77778    250      0.78080   0.56046\n  0.4   9         0.6               0.77778    300      0.78080   0.56075\n  0.4   9         0.6               0.77778    350      0.78080   0.55954\n  0.4   9         0.6               0.77778    400      0.78425   0.56759\n  0.4   9         0.6               0.77778    450      0.79092   0.58116\n  0.4   9         0.6               0.77778    500      0.78080   0.55989\n  0.4   9         0.6               0.83333     50      0.79793   0.59409\n  0.4   9         0.6               0.83333    100      0.78414   0.56549\n  0.4   9         0.6               0.83333    150      0.78759   0.57254\n  0.4   9         0.6               0.83333    200      0.78414   0.56535\n  0.4   9         0.6               0.83333    250      0.78747   0.57244\n  0.4   9         0.6               0.83333    300      0.78747   0.57172\n  0.4   9         0.6               0.83333    350      0.78747   0.57172\n  0.4   9         0.6               0.83333    400      0.79425   0.58635\n  0.4   9         0.6               0.83333    450      0.79425   0.58635\n  0.4   9         0.6               0.83333    500      0.79092   0.57975\n  0.4   9         0.6               0.88889     50      0.83437   0.66694\n  0.4   9         0.6               0.88889    100      0.81126   0.61978\n  0.4   9         0.6               0.88889    150      0.81092   0.62063\n  0.4   9         0.6               0.88889    200      0.80747   0.61373\n  0.4   9         0.6               0.88889    250      0.80080   0.60028\n  0.4   9         0.6               0.88889    300      0.80080   0.60028\n  0.4   9         0.6               0.88889    350      0.80747   0.61379\n  0.4   9         0.6               0.88889    400      0.80414   0.60713\n  0.4   9         0.6               0.88889    450      0.79736   0.59411\n  0.4   9         0.6               0.88889    500      0.79736   0.59316\n  0.4   9         0.6               0.94444     50      0.80057   0.59818\n  0.4   9         0.6               0.94444    100      0.80402   0.60510\n  0.4   9         0.6               0.94444    150      0.80736   0.61213\n  0.4   9         0.6               0.94444    200      0.80391   0.60481\n  0.4   9         0.6               0.94444    250      0.80391   0.60481\n  0.4   9         0.6               0.94444    300      0.80736   0.61213\n  0.4   9         0.6               0.94444    350      0.81080   0.61935\n  0.4   9         0.6               0.94444    400      0.81080   0.61935\n  0.4   9         0.6               0.94444    450      0.81080   0.61935\n  0.4   9         0.6               0.94444    500      0.80747   0.61232\n  0.4   9         0.6               1.00000     50      0.81103   0.61872\n  0.4   9         0.6               1.00000    100      0.79747   0.59167\n  0.4   9         0.6               1.00000    150      0.79080   0.57864\n  0.4   9         0.6               1.00000    200      0.78402   0.56511\n  0.4   9         0.6               1.00000    250      0.78747   0.57256\n  0.4   9         0.6               1.00000    300      0.78747   0.57256\n  0.4   9         0.6               1.00000    350      0.79080   0.57916\n  0.4   9         0.6               1.00000    400      0.79414   0.58547\n  0.4   9         0.6               1.00000    450      0.79080   0.57916\n  0.4   9         0.6               1.00000    500      0.79080   0.57916\n  0.4   9         0.8               0.50000     50      0.79414   0.58613\n  0.4   9         0.8               0.50000    100      0.80425   0.60642\n  0.4   9         0.8               0.50000    150      0.81080   0.62021\n  0.4   9         0.8               0.50000    200      0.80402   0.60618\n  0.4   9         0.8               0.50000    250      0.80057   0.59984\n  0.4   9         0.8               0.50000    300      0.80747   0.61330\n  0.4   9         0.8               0.50000    350      0.80759   0.61271\n  0.4   9         0.8               0.50000    400      0.80414   0.60633\n  0.4   9         0.8               0.50000    450      0.80080   0.60015\n  0.4   9         0.8               0.50000    500      0.79747   0.59342\n  0.4   9         0.8               0.55556     50      0.79759   0.59343\n  0.4   9         0.8               0.55556    100      0.78402   0.56561\n  0.4   9         0.8               0.55556    150      0.78391   0.56665\n  0.4   9         0.8               0.55556    200      0.76701   0.53356\n  0.4   9         0.8               0.55556    250      0.77391   0.54618\n  0.4   9         0.8               0.55556    300      0.77736   0.55339\n  0.4   9         0.8               0.55556    350      0.77724   0.55338\n  0.4   9         0.8               0.55556    400      0.77379   0.54551\n  0.4   9         0.8               0.55556    450      0.77713   0.55242\n  0.4   9         0.8               0.55556    500      0.77391   0.54671\n  0.4   9         0.8               0.61111     50      0.79069   0.58017\n  0.4   9         0.8               0.61111    100      0.80414   0.60673\n  0.4   9         0.8               0.61111    150      0.80092   0.59985\n  0.4   9         0.8               0.61111    200      0.79402   0.58643\n  0.4   9         0.8               0.61111    250      0.79736   0.59327\n  0.4   9         0.8               0.61111    300      0.80080   0.59971\n  0.4   9         0.8               0.61111    350      0.79414   0.58650\n  0.4   9         0.8               0.61111    400      0.78402   0.56673\n  0.4   9         0.8               0.61111    450      0.78736   0.57279\n  0.4   9         0.8               0.61111    500      0.79080   0.57978\n  0.4   9         0.8               0.66667     50      0.80759   0.61282\n  0.4   9         0.8               0.66667    100      0.80092   0.60018\n  0.4   9         0.8               0.66667    150      0.79747   0.59392\n  0.4   9         0.8               0.66667    200      0.79747   0.59439\n  0.4   9         0.8               0.66667    250      0.79069   0.58039\n  0.4   9         0.8               0.66667    300      0.80080   0.60034\n  0.4   9         0.8               0.66667    350      0.79069   0.58038\n  0.4   9         0.8               0.66667    400      0.79069   0.58068\n  0.4   9         0.8               0.66667    450      0.78391   0.56676\n  0.4   9         0.8               0.66667    500      0.78713   0.57339\n  0.4   9         0.8               0.72222     50      0.78391   0.56655\n  0.4   9         0.8               0.72222    100      0.79402   0.58739\n  0.4   9         0.8               0.72222    150      0.78402   0.56650\n  0.4   9         0.8               0.72222    200      0.78402   0.56631\n  0.4   9         0.8               0.72222    250      0.77736   0.55316\n  0.4   9         0.8               0.72222    300      0.77724   0.55271\n  0.4   9         0.8               0.72222    350      0.78414   0.56557\n  0.4   9         0.8               0.72222    400      0.78747   0.57242\n  0.4   9         0.8               0.72222    450      0.79092   0.57917\n  0.4   9         0.8               0.72222    500      0.78759   0.57220\n  0.4   9         0.8               0.77778     50      0.78736   0.57015\n  0.4   9         0.8               0.77778    100      0.78747   0.57105\n  0.4   9         0.8               0.77778    150      0.77747   0.55190\n  0.4   9         0.8               0.77778    200      0.77057   0.53903\n  0.4   9         0.8               0.77778    250      0.77736   0.55262\n  0.4   9         0.8               0.77778    300      0.77736   0.55262\n  0.4   9         0.8               0.77778    350      0.78080   0.55906\n  0.4   9         0.8               0.77778    400      0.77736   0.55286\n  0.4   9         0.8               0.77778    450      0.77402   0.54631\n  0.4   9         0.8               0.77778    500      0.77402   0.54631\n  0.4   9         0.8               0.83333     50      0.79425   0.58557\n  0.4   9         0.8               0.83333    100      0.78414   0.56458\n  0.4   9         0.8               0.83333    150      0.78747   0.57218\n  0.4   9         0.8               0.83333    200      0.78092   0.55879\n  0.4   9         0.8               0.83333    250      0.77747   0.55236\n  0.4   9         0.8               0.83333    300      0.78080   0.55873\n  0.4   9         0.8               0.83333    350      0.78759   0.57216\n  0.4   9         0.8               0.83333    400      0.78759   0.57216\n  0.4   9         0.8               0.83333    450      0.78759   0.57216\n  0.4   9         0.8               0.83333    500      0.78069   0.55920\n  0.4   9         0.8               0.88889     50      0.79414   0.58601\n  0.4   9         0.8               0.88889    100      0.79736   0.59214\n  0.4   9         0.8               0.88889    150      0.79736   0.59256\n  0.4   9         0.8               0.88889    200      0.79391   0.58522\n  0.4   9         0.8               0.88889    250      0.79057   0.57867\n  0.4   9         0.8               0.88889    300      0.78724   0.57230\n  0.4   9         0.8               0.88889    350      0.78391   0.56546\n  0.4   9         0.8               0.88889    400      0.79057   0.57885\n  0.4   9         0.8               0.88889    450      0.78391   0.56546\n  0.4   9         0.8               0.88889    500      0.78391   0.56546\n  0.4   9         0.8               0.94444     50      0.80770   0.61173\n  0.4   9         0.8               0.94444    100      0.80437   0.60554\n  0.4   9         0.8               0.94444    150      0.80103   0.59923\n  0.4   9         0.8               0.94444    200      0.79080   0.57902\n  0.4   9         0.8               0.94444    250      0.79425   0.58613\n  0.4   9         0.8               0.94444    300      0.79425   0.58613\n  0.4   9         0.8               0.94444    350      0.79092   0.57929\n  0.4   9         0.8               0.94444    400      0.78747   0.57217\n  0.4   9         0.8               0.94444    450      0.79080   0.57945\n  0.4   9         0.8               0.94444    500      0.79759   0.59317\n  0.4   9         0.8               1.00000     50      0.80425   0.60623\n  0.4   9         0.8               1.00000    100      0.80080   0.59911\n  0.4   9         0.8               1.00000    150      0.79759   0.59236\n  0.4   9         0.8               1.00000    200      0.80437   0.60585\n  0.4   9         0.8               1.00000    250      0.80103   0.59948\n  0.4   9         0.8               1.00000    300      0.79759   0.59295\n  0.4   9         0.8               1.00000    350      0.80103   0.60016\n  0.4   9         0.8               1.00000    400      0.80103   0.60016\n  0.4   9         0.8               1.00000    450      0.80103   0.60016\n  0.4   9         0.8               1.00000    500      0.80103   0.60016\n  0.4  10         0.6               0.50000     50      0.81080   0.62003\n  0.4  10         0.6               0.50000    100      0.78402   0.56550\n  0.4  10         0.6               0.50000    150      0.78080   0.55785\n  0.4  10         0.6               0.50000    200      0.77391   0.54507\n  0.4  10         0.6               0.50000    250      0.79080   0.57864\n  0.4  10         0.6               0.50000    300      0.79425   0.58526\n  0.4  10         0.6               0.50000    350      0.79080   0.57858\n  0.4  10         0.6               0.50000    400      0.79080   0.57864\n  0.4  10         0.6               0.50000    450      0.79414   0.58512\n  0.4  10         0.6               0.50000    500      0.79425   0.58483\n  0.4  10         0.6               0.55556     50      0.78391   0.56340\n  0.4  10         0.6               0.55556    100      0.78046   0.55675\n  0.4  10         0.6               0.55556    150      0.78379   0.56336\n  0.4  10         0.6               0.55556    200      0.79391   0.58358\n  0.4  10         0.6               0.55556    250      0.78724   0.56964\n  0.4  10         0.6               0.55556    300      0.78057   0.55692\n  0.4  10         0.6               0.55556    350      0.78057   0.55692\n  0.4  10         0.6               0.55556    400      0.78379   0.56360\n  0.4  10         0.6               0.55556    450      0.77724   0.55049\n  0.4  10         0.6               0.55556    500      0.78046   0.55706\n  0.4  10         0.6               0.61111     50      0.81747   0.63482\n  0.4  10         0.6               0.61111    100      0.80425   0.60746\n  0.4  10         0.6               0.61111    150      0.79425   0.58770\n  0.4  10         0.6               0.61111    200      0.80103   0.60065\n  0.4  10         0.6               0.61111    250      0.79069   0.57987\n  0.4  10         0.6               0.61111    300      0.79747   0.59325\n  0.4  10         0.6               0.61111    350      0.79069   0.57987\n  0.4  10         0.6               0.61111    400      0.79069   0.57987\n  0.4  10         0.6               0.61111    450      0.79402   0.58653\n  0.4  10         0.6               0.61111    500      0.79069   0.57987\n  0.4  10         0.6               0.66667     50      0.78402   0.56583\n  0.4  10         0.6               0.66667    100      0.77736   0.55255\n  0.4  10         0.6               0.66667    150      0.78080   0.55932\n  0.4  10         0.6               0.66667    200      0.78402   0.56520\n  0.4  10         0.6               0.66667    250      0.78402   0.56520\n  0.4  10         0.6               0.66667    300      0.78402   0.56543\n  0.4  10         0.6               0.66667    350      0.79080   0.57908\n  0.4  10         0.6               0.66667    400      0.79414   0.58593\n  0.4  10         0.6               0.66667    450      0.79080   0.57956\n  0.4  10         0.6               0.66667    500      0.79414   0.58599\n  0.4  10         0.6               0.72222     50      0.79724   0.59350\n  0.4  10         0.6               0.72222    100      0.79069   0.58025\n  0.4  10         0.6               0.72222    150      0.79736   0.59336\n  0.4  10         0.6               0.72222    200      0.79736   0.59431\n  0.4  10         0.6               0.72222    250      0.80069   0.60051\n  0.4  10         0.6               0.72222    300      0.79736   0.59399\n  0.4  10         0.6               0.72222    350      0.79736   0.59384\n  0.4  10         0.6               0.72222    400      0.78724   0.57337\n  0.4  10         0.6               0.72222    450      0.79391   0.58647\n  0.4  10         0.6               0.72222    500      0.79057   0.57968\n  0.4  10         0.6               0.77778     50      0.78724   0.57291\n  0.4  10         0.6               0.77778    100      0.78724   0.57291\n  0.4  10         0.6               0.77778    150      0.78391   0.56607\n  0.4  10         0.6               0.77778    200      0.78379   0.56668\n  0.4  10         0.6               0.77778    250      0.77701   0.55268\n  0.4  10         0.6               0.77778    300      0.77368   0.54583\n  0.4  10         0.6               0.77778    350      0.78046   0.55988\n  0.4  10         0.6               0.77778    400      0.78046   0.55988\n  0.4  10         0.6               0.77778    450      0.78046   0.55988\n  0.4  10         0.6               0.77778    500      0.78046   0.55982\n  0.4  10         0.6               0.83333     50      0.82425   0.64721\n  0.4  10         0.6               0.83333    100      0.82437   0.64628\n  0.4  10         0.6               0.83333    150      0.81425   0.62648\n  0.4  10         0.6               0.83333    200      0.81448   0.62635\n  0.4  10         0.6               0.83333    250      0.81448   0.62635\n  0.4  10         0.6               0.83333    300      0.81103   0.61982\n  0.4  10         0.6               0.83333    350      0.81448   0.62635\n  0.4  10         0.6               0.83333    400      0.81448   0.62635\n  0.4  10         0.6               0.83333    450      0.81103   0.61982\n  0.4  10         0.6               0.83333    500      0.80770   0.61327\n  0.4  10         0.6               0.88889     50      0.80092   0.59975\n  0.4  10         0.6               0.88889    100      0.79092   0.58025\n  0.4  10         0.6               0.88889    150      0.78747   0.57310\n  0.4  10         0.6               0.88889    200      0.78414   0.56715\n  0.4  10         0.6               0.88889    250      0.79080   0.58013\n  0.4  10         0.6               0.88889    300      0.79092   0.58093\n  0.4  10         0.6               0.88889    350      0.79425   0.58724\n  0.4  10         0.6               0.88889    400      0.79080   0.58013\n  0.4  10         0.6               0.88889    450      0.79080   0.58013\n  0.4  10         0.6               0.88889    500      0.79080   0.58013\n  0.4  10         0.6               0.94444     50      0.79736   0.59244\n  0.4  10         0.6               0.94444    100      0.79069   0.57919\n  0.4  10         0.6               0.94444    150      0.79069   0.57973\n  0.4  10         0.6               0.94444    200      0.79080   0.57913\n  0.4  10         0.6               0.94444    250      0.79747   0.59289\n  0.4  10         0.6               0.94444    300      0.79414   0.58604\n  0.4  10         0.6               0.94444    350      0.79069   0.57869\n  0.4  10         0.6               0.94444    400      0.78736   0.57202\n  0.4  10         0.6               0.94444    450      0.78736   0.57202\n  0.4  10         0.6               0.94444    500      0.78747   0.57307\n  0.4  10         0.6               1.00000     50      0.79770   0.59287\n  0.4  10         0.6               1.00000    100      0.79425   0.58493\n  0.4  10         0.6               1.00000    150      0.78747   0.57203\n  0.4  10         0.6               1.00000    200      0.78414   0.56512\n  0.4  10         0.6               1.00000    250      0.78747   0.57166\n  0.4  10         0.6               1.00000    300      0.78759   0.57187\n  0.4  10         0.6               1.00000    350      0.78759   0.57187\n  0.4  10         0.6               1.00000    400      0.79092   0.57884\n  0.4  10         0.6               1.00000    450      0.79092   0.57865\n  0.4  10         0.6               1.00000    500      0.78425   0.56521\n  0.4  10         0.8               0.50000     50      0.78057   0.55764\n  0.4  10         0.8               0.50000    100      0.76724   0.52897\n  0.4  10         0.8               0.50000    150      0.77391   0.54285\n  0.4  10         0.8               0.50000    200      0.77379   0.54383\n  0.4  10         0.8               0.50000    250      0.77034   0.53676\n  0.4  10         0.8               0.50000    300      0.77034   0.53670\n  0.4  10         0.8               0.50000    350      0.77046   0.53607\n  0.4  10         0.8               0.50000    400      0.77034   0.53627\n  0.4  10         0.8               0.50000    450      0.77701   0.55028\n  0.4  10         0.8               0.50000    500      0.76701   0.52985\n  0.4  10         0.8               0.55556     50      0.79379   0.58555\n  0.4  10         0.8               0.55556    100      0.79379   0.58562\n  0.4  10         0.8               0.55556    150      0.80069   0.59908\n  0.4  10         0.8               0.55556    200      0.79057   0.57958\n  0.4  10         0.8               0.55556    250      0.79069   0.58021\n  0.4  10         0.8               0.55556    300      0.79414   0.58665\n  0.4  10         0.8               0.55556    350      0.78724   0.57290\n  0.4  10         0.8               0.55556    400      0.78724   0.57290\n  0.4  10         0.8               0.55556    450      0.79414   0.58577\n  0.4  10         0.8               0.55556    500      0.79080   0.57910\n  0.4  10         0.8               0.61111     50      0.80034   0.59807\n  0.4  10         0.8               0.61111    100      0.77690   0.55145\n  0.4  10         0.8               0.61111    150      0.77713   0.55245\n  0.4  10         0.8               0.61111    200      0.78379   0.56643\n  0.4  10         0.8               0.61111    250      0.78034   0.55917\n  0.4  10         0.8               0.61111    300      0.77690   0.55144\n  0.4  10         0.8               0.61111    350      0.78368   0.56541\n  0.4  10         0.8               0.61111    400      0.78034   0.55875\n  0.4  10         0.8               0.61111    450      0.77368   0.54528\n  0.4  10         0.8               0.61111    500      0.77713   0.55163\n  0.4  10         0.8               0.66667     50      0.80425   0.60499\n  0.4  10         0.8               0.66667    100      0.80092   0.59807\n  0.4  10         0.8               0.66667    150      0.79092   0.57845\n  0.4  10         0.8               0.66667    200      0.79759   0.59233\n  0.4  10         0.8               0.66667    250      0.80092   0.59893\n  0.4  10         0.8               0.66667    300      0.79770   0.59252\n  0.4  10         0.8               0.66667    350      0.78759   0.57256\n  0.4  10         0.8               0.66667    400      0.79437   0.58615\n  0.4  10         0.8               0.66667    450      0.79092   0.57893\n  0.4  10         0.8               0.66667    500      0.78759   0.57268\n  0.4  10         0.8               0.72222     50      0.79115   0.58026\n  0.4  10         0.8               0.72222    100      0.78414   0.56545\n  0.4  10         0.8               0.72222    150      0.77724   0.55214\n  0.4  10         0.8               0.72222    200      0.77747   0.55385\n  0.4  10         0.8               0.72222    250      0.77069   0.54059\n  0.4  10         0.8               0.72222    300      0.76736   0.53369\n  0.4  10         0.8               0.72222    350      0.77080   0.54004\n  0.4  10         0.8               0.72222    400      0.76747   0.53349\n  0.4  10         0.8               0.72222    450      0.76747   0.53349\n  0.4  10         0.8               0.72222    500      0.76747   0.53349\n  0.4  10         0.8               0.77778     50      0.80080   0.60005\n  0.4  10         0.8               0.77778    100      0.79425   0.58610\n  0.4  10         0.8               0.77778    150      0.79425   0.58579\n  0.4  10         0.8               0.77778    200      0.79414   0.58617\n  0.4  10         0.8               0.77778    250      0.79069   0.57905\n  0.4  10         0.8               0.77778    300      0.79069   0.57905\n  0.4  10         0.8               0.77778    350      0.78069   0.55971\n  0.4  10         0.8               0.77778    400      0.78402   0.56542\n  0.4  10         0.8               0.77778    450      0.77402   0.54596\n  0.4  10         0.8               0.77778    500      0.77402   0.54596\n  0.4  10         0.8               0.83333     50      0.79080   0.57939\n  0.4  10         0.8               0.83333    100      0.78391   0.56545\n  0.4  10         0.8               0.83333    150      0.78736   0.57332\n  0.4  10         0.8               0.83333    200      0.78402   0.56678\n  0.4  10         0.8               0.83333    250      0.78069   0.55987\n  0.4  10         0.8               0.83333    300      0.78069   0.55987\n  0.4  10         0.8               0.83333    350      0.78069   0.55987\n  0.4  10         0.8               0.83333    400      0.78736   0.57284\n  0.4  10         0.8               0.83333    450      0.78736   0.57284\n  0.4  10         0.8               0.83333    500      0.78402   0.56647\n  0.4  10         0.8               0.88889     50      0.81448   0.62672\n  0.4  10         0.8               0.88889    100      0.80782   0.61322\n  0.4  10         0.8               0.88889    150      0.79437   0.58658\n  0.4  10         0.8               0.88889    200      0.79770   0.59324\n  0.4  10         0.8               0.88889    250      0.80103   0.60009\n  0.4  10         0.8               0.88889    300      0.79770   0.59377\n  0.4  10         0.8               0.88889    350      0.79437   0.58640\n  0.4  10         0.8               0.88889    400      0.79782   0.59414\n  0.4  10         0.8               0.88889    450      0.79448   0.58783\n  0.4  10         0.8               0.88889    500      0.79448   0.58783\n  0.4  10         0.8               0.94444     50      0.79402   0.58559\n  0.4  10         0.8               0.94444    100      0.78736   0.57267\n  0.4  10         0.8               0.94444    150      0.79069   0.57922\n  0.4  10         0.8               0.94444    200      0.79402   0.58613\n  0.4  10         0.8               0.94444    250      0.79402   0.58613\n  0.4  10         0.8               0.94444    300      0.79402   0.58613\n  0.4  10         0.8               0.94444    350      0.79069   0.57947\n  0.4  10         0.8               0.94444    400      0.79069   0.57947\n  0.4  10         0.8               0.94444    450      0.79414   0.58580\n  0.4  10         0.8               0.94444    500      0.79414   0.58580\n  0.4  10         0.8               1.00000     50      0.80425   0.60666\n  0.4  10         0.8               1.00000    100      0.80092   0.60023\n  0.4  10         0.8               1.00000    150      0.80092   0.60051\n  0.4  10         0.8               1.00000    200      0.79759   0.59366\n  0.4  10         0.8               1.00000    250      0.79747   0.59340\n  0.4  10         0.8               1.00000    300      0.80080   0.60006\n  0.4  10         0.8               1.00000    350      0.80080   0.60006\n  0.4  10         0.8               1.00000    400      0.79747   0.59340\n  0.4  10         0.8               1.00000    450      0.79747   0.59340\n  0.4  10         0.8               1.00000    500      0.79747   0.59340\n\nTuning parameter 'gamma' was held constant at a value of 0\nTuning\n parameter 'min_child_weight' was held constant at a value of 1\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were nrounds = 50, max_depth = 1, eta\n = 0.3, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample\n = 0.72222.\n\n\nEn este caso, caret mostrará la precisión del modelo para cada combinación de hiperparámetros, ayudando a identificar el mejor conjunto de hiperparámetros.\n\nEste es un ejemplo básico para ajustar un modelo XGBoost con el paquete caret, que incluye la validación cruzada y la visualización de la importancia de las variables. Puedes experimentar con otros hiperparámetros y técnicas de validación según el conjunto de datos y el problema específico."
  },
  {
    "objectID": "sobreajuste_y_entrenamiento.html#aic-vs-validación-cruzada",
    "href": "sobreajuste_y_entrenamiento.html#aic-vs-validación-cruzada",
    "title": "Sobreajuste y entrenamiento de modelos",
    "section": "3.9 AIC vs validación cruzada",
    "text": "3.9 AIC vs validación cruzada\nLa elección entre el AIC (Criterio de Información de Akaike) (o selección de modelos en general) y la validación cruzada depende de varios factores relacionados con los datos, el objetivo del modelo y las limitaciones computacionales. Ambos métodos tienen ventajas y desventajas en el contexto de evitar el sobreajuste:\n\n3.9.1 Cuándo usar AIC\n\nModelos paramétricos bien especificados: AIC es ideal cuando se supone que el modelo sigue una distribución específica y esta está correctamente especificada.\nEvaluación relativa de modelos: AIC es útil para comparar varios modelos ajustados a un mismo conjunto de datos y elegir el modelo más parsimonioso (con el mejor equilibrio entre ajuste y complejidad).\nTamaños de muestra moderados a grandes: AIC funciona bien con datos suficientes para que las estimaciones de parámetros sean confiables.\nMenor costo computacional: Como AIC no requiere dividir ni reentrenar modelos, es más rápido y eficiente que la validación cruzada.\n\n\n\n3.9.2 Cuándo usar validación cruzada\n\nModelos no paramétricos o más complejos: La validación cruzada se adapta bien a métodos como árboles de decisión, Random Forest o XGBoost, donde las suposiciones paramétricas son menos claras.\nEvaluación de capacidad predictiva: Es preferible si el objetivo principal es maximizar la capacidad predictiva en datos futuros.\nTamaños de muestra pequeños: La validación cruzada utiliza el conjunto completo de datos de manera eficiente al dividirlo en diferentes subconjuntos.\nDatos con ruido o alta dimensionalidad: Es más robusta para medir el rendimiento real del modelo en presencia de ruido o muchos predictores.\n\n\n\n3.9.3 Comparación general\n\n\n\n\n\n\n\n\n\n\n\nCaracterística\nAIC\nValidacion_Cruzada\n\n\n\n\nObjetivo\nEquilibrar ajuste y complejidad del modelo.\nMaximizar capacidad predictiva.\n\n\nNaturaleza de los datos\nModelos paramétricos bien especificados.\nModelos complejos o no paramétricos.\n\n\nComputación\nRápida y eficiente.\nMás costosa computacionalmente.\n\n\nTamaño de muestra\nModerado a grande.\nPequeño a grande.\n\n\nMedida de evaluación\nBasada en la verosimilitud penalizada.\nBasada en error promedio en datos no usados.\n\n\nEjemplos de Modelos\nRegresiónes: lineal, generalizada, mixta, logística …\nRandom Forest, XGBoost, Support Vector Machines (SVM).\n\n\n\n\n\n\n\n\n3.9.4 Recomendación\n\nUsa AIC: Cuando necesitas comparar modelos paramétricos rápidamente en situaciones con suficiente información para confiar en la especificación del modelo.\nUsa validación cruzada: Cuando estás trabajando con métodos más complejos, datos con ruido o si la predicción futura precisa es tu principal objetivo.\n\nAmbos enfoques pueden ser complementarios. Por ejemplo, puedes usar AIC para seleccionar un modelo inicial y luego aplicar validación cruzada para confirmar su capacidad predictiva. Estas dos estrategias se eligieron para este tutorial porque abordan el sobreajuste desde perspectivas complementarias. El AIC, basado en principios de inferencia estadística, es ideal cuando los datos se ajustan bien a la estructura asumida por el modelo. Por otro lado, la validación cruzada es flexible y no asume una especificación estricta del modelo, siendo útil en problemas complejos y de aprendizaje estadístico.\nSin embargo existen otros métodos que permiten evitar el sobreajuste. Algunas de estas son: - Regularización: Métodos como LASSO y Ridge penalizan la complejidad del modelo para evitar ajustes excesivos. - Podado de modelos: En algoritmos como árboles de decisión, se eliminan ramas menos significativas. - Dropout: Usado en redes neuronales para prevenir dependencia excesiva en nodos específicos durante el entrenamiento."
  },
  {
    "objectID": "sobreajuste_y_entrenamiento.html#validación-cruzada-balanceada",
    "href": "sobreajuste_y_entrenamiento.html#validación-cruzada-balanceada",
    "title": "Sobreajuste y entrenamiento de modelos",
    "section": "3.6 Validación cruzada balanceada",
    "text": "3.6 Validación cruzada balanceada\nEn problemas de clasificación, la distribución de las clases puede ser desbalanceada. La validación cruzada balanceada garantiza que cada subconjunto tenga una proporción similar de clases, mejorando la estabilidad de las métricas.\nHay dos formas basicas en las que se puede balancear los datos durante las iteraciones de la validación cruzada. La primera (muestrear hacia arriba o “up-sampling”) es balancear forzando que todas las clases tengan el mismo numero de observaciones que la clase mas frecuente. tenga el mismo numero de observaciones. La segunda es balancear “hacia abajo” (down-sampling) forzando todas las clases a tener el mismo numero de observaciones que la clase menos frecuente.\nPara muestrear hacia abajo, simplemente definimos el argumento sampling = \"up\" en la función trainControl:\n\n\nCódigo\n# Configurar validación cruzada con 10 folds\ncontrol &lt;- caret::trainControl(\n  method = \"cv\",\n  number = 10,\n  classProbs = TRUE,\n  sampling = \"up\"\n)\n\n# Ajustar modelo de clasificación\nmtcars$am_f &lt;- ifelse(mtcars$am == 1, \"manual\", \"automatico\")\n\n\nset.seed(123)\nmodelo &lt;- caret::train(\n  am_f ~ wt + hp,\n  data = mtcars,\n  method = \"glm\",\n  family = binomial,\n  trControl = control\n)\n\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nCódigo\n# Resultados del modelo\nprint(modelo)\n\n\nGeneralized Linear Model \n\n32 samples\n 2 predictor\n 2 classes: 'automatico', 'manual' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 29, 29, 28, 28, 28, 29, ... \nAddtional sampling using up-sampling\n\nResampling results:\n\n  Accuracy  Kappa\n  0.95      0.9  \n\n\nPodemos ver las categorías no estaban perfectamente balanceada en los datos originales:\n\n\nCódigo\ntable(mtcars$am_f)\n\n\n\nautomatico     manual \n        19         13 \n\n\nSin embargo, la proporción de clases se mantiene constante durante el entrenamiento y para cada categoría el número de observaciones es igual al de la clase mas numerosa:\n\n\nCódigo\n# ver frecuencia de clases en el modelo final\ntable(modelo$finalModel$data$.outcome)\n\n\n\nautomatico     manual \n        19         19 \n\n\nPara muestrear hacia abajo, definimos el argumento sampling = \"down\":\n\n\nCódigo\n# Configurar validación cruzada con 10 folds\ncontrol &lt;- caret::trainControl(\n  method = \"cv\",\n  number = 10,\n  classProbs = TRUE,\n  sampling = \"down\"\n)\n\n\nset.seed(123)\nmodelo &lt;- caret::train(\n  am_f ~ wt + hp,\n  data = mtcars,\n  method = \"glm\",\n  family = binomial,\n  trControl = control\n)\n\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nCódigo\n# Resultados del modelo\nprint(modelo)\n\n\nGeneralized Linear Model \n\n32 samples\n 2 predictor\n 2 classes: 'automatico', 'manual' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 29, 29, 28, 28, 28, 29, ... \nAddtional sampling using down-sampling\n\nResampling results:\n\n  Accuracy  Kappa\n  0.925     0.85 \n\n\nPodemos ver nuevamente como la proporción de clases se mantiene constante durante en el entrenamieto, pero en este caso con el número de observaciones en la categoría menos numerosa:\n\n\nCódigo\n# ver frecuencia de clases en el modelo final\ntable(modelo$finalModel$data$.outcome)\n\n\n\nautomatico     manual \n        13         13 \n\n\nTambien se han desarrollado metodos hibridos que combinan “up-sampling” y “down-sampling”. El método híbrido “smote” puede ser utilizado de esta forma:\nPara muestrear hacia abajo, definimos el argumento sampling = \"down\":\n\n\nCódigo\n# Configurar validación cruzada con 10 folds\ncontrol &lt;- caret::trainControl(\n  method = \"cv\",\n  number = 10,\n  classProbs = TRUE,\n  sampling = \"smote\"\n)\n\n\nset.seed(123)\nmodelo &lt;- caret::train(\n  am_f ~ wt + hp,\n  data = mtcars,\n  method = \"glm\",\n  family = binomial,\n  trControl = control\n)\n\n\nLoading required package: recipes\n\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nAttaching package: 'recipes'\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nCódigo\n# Resultados del modelo\nprint(modelo)\n\n\nGeneralized Linear Model \n\n32 samples\n 2 predictor\n 2 classes: 'automatico', 'manual' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 29, 29, 28, 28, 28, 29, ... \nAddtional sampling using SMOTE\n\nResampling results:\n\n  Accuracy  Kappa\n  0.95      0.9"
  }
]